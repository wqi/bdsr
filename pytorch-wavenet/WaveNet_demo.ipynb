{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is notebook gives a quick overview of this WaveNet implementation, i.e. creating the model and the data set, training the model and generating samples from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from model_logging import *\n",
    "import torch\n",
    "from wavenet_model import *\n",
    "from audio_data import WavenetDataset\n",
    "from wavenet_training import *\n",
    "#from model_logging import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "This is an implementation of WaveNet as it was described in the original paper (https://arxiv.org/abs/1609.03499). Each layer looks like this:\n",
    "\n",
    "```\n",
    "            |----------------------------------------|      *residual*\n",
    "            |                                        |\n",
    "            |    |-- conv -- tanh --|                |\n",
    " -> dilate -|----|                  * ----|-- 1x1 -- + -->  *input*\n",
    "                 |-- conv -- sigm --|     |\n",
    "                                         1x1\n",
    "                                          |\n",
    " ---------------------------------------> + ------------->  *skip*\n",
    "```\n",
    "\n",
    "Each layer dilates the input by a factor of two. After each block the dilation is reset and start from one. You can define the number of layers in each block (``layers``) and the number of blocks (``blocks``). The blocks are followed by two 1x1 convolutions and a softmax output function.\n",
    "Because of the dilation operation, the independent output for multiple successive samples can be calculated efficiently. With ``output_length``, you can define the number these outputs. Empirically, it seems that a large number of skip channels is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use gpu\n"
     ]
    }
   ],
   "source": [
    "# initialize cuda option\n",
    "dtype = torch.FloatTensor # data type\n",
    "ltype = torch.LongTensor # label type\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "#use_cuda = False\n",
    "if use_cuda:\n",
    "    print('use gpu')\n",
    "    dtype = torch.cuda.FloatTensor\n",
    "    ltype = torch.cuda.LongTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:  WaveNetModel (\n",
      "  (filter_convs): ModuleList (\n",
      "    (0): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (1): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (2): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (3): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (4): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (5): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (6): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (7): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (8): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (9): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (10): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (11): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (12): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (13): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (14): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (15): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (16): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (17): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (18): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (19): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (20): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (21): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (22): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (23): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (24): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (25): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (26): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (27): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (28): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (29): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "  )\n",
      "  (gate_convs): ModuleList (\n",
      "    (0): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (1): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (2): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (3): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (4): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (5): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (6): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (7): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (8): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (9): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (10): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (11): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (12): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (13): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (14): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (15): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (16): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (17): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (18): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (19): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (20): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (21): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (22): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (23): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (24): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (25): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (26): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (27): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (28): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "    (29): Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "  )\n",
      "  (residual_convs): ModuleList (\n",
      "    (0): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "    (1): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "    (2): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "    (3): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "    (4): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "    (5): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "    (6): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "    (7): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "    (8): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "    (9): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "    (10): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "    (11): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "    (12): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "    (13): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "    (14): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "    (15): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "    (16): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "    (17): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "    (18): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "    (19): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "    (20): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "    (21): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "    (22): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "    (23): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "    (24): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "    (25): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "    (26): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "    (27): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "    (28): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "    (29): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "  )\n",
      "  (skip_convs): ModuleList (\n",
      "    (0): Conv1d(32, 1024, kernel_size=(1,), stride=(1,))\n",
      "    (1): Conv1d(32, 1024, kernel_size=(1,), stride=(1,))\n",
      "    (2): Conv1d(32, 1024, kernel_size=(1,), stride=(1,))\n",
      "    (3): Conv1d(32, 1024, kernel_size=(1,), stride=(1,))\n",
      "    (4): Conv1d(32, 1024, kernel_size=(1,), stride=(1,))\n",
      "    (5): Conv1d(32, 1024, kernel_size=(1,), stride=(1,))\n",
      "    (6): Conv1d(32, 1024, kernel_size=(1,), stride=(1,))\n",
      "    (7): Conv1d(32, 1024, kernel_size=(1,), stride=(1,))\n",
      "    (8): Conv1d(32, 1024, kernel_size=(1,), stride=(1,))\n",
      "    (9): Conv1d(32, 1024, kernel_size=(1,), stride=(1,))\n",
      "    (10): Conv1d(32, 1024, kernel_size=(1,), stride=(1,))\n",
      "    (11): Conv1d(32, 1024, kernel_size=(1,), stride=(1,))\n",
      "    (12): Conv1d(32, 1024, kernel_size=(1,), stride=(1,))\n",
      "    (13): Conv1d(32, 1024, kernel_size=(1,), stride=(1,))\n",
      "    (14): Conv1d(32, 1024, kernel_size=(1,), stride=(1,))\n",
      "    (15): Conv1d(32, 1024, kernel_size=(1,), stride=(1,))\n",
      "    (16): Conv1d(32, 1024, kernel_size=(1,), stride=(1,))\n",
      "    (17): Conv1d(32, 1024, kernel_size=(1,), stride=(1,))\n",
      "    (18): Conv1d(32, 1024, kernel_size=(1,), stride=(1,))\n",
      "    (19): Conv1d(32, 1024, kernel_size=(1,), stride=(1,))\n",
      "    (20): Conv1d(32, 1024, kernel_size=(1,), stride=(1,))\n",
      "    (21): Conv1d(32, 1024, kernel_size=(1,), stride=(1,))\n",
      "    (22): Conv1d(32, 1024, kernel_size=(1,), stride=(1,))\n",
      "    (23): Conv1d(32, 1024, kernel_size=(1,), stride=(1,))\n",
      "    (24): Conv1d(32, 1024, kernel_size=(1,), stride=(1,))\n",
      "    (25): Conv1d(32, 1024, kernel_size=(1,), stride=(1,))\n",
      "    (26): Conv1d(32, 1024, kernel_size=(1,), stride=(1,))\n",
      "    (27): Conv1d(32, 1024, kernel_size=(1,), stride=(1,))\n",
      "    (28): Conv1d(32, 1024, kernel_size=(1,), stride=(1,))\n",
      "    (29): Conv1d(32, 1024, kernel_size=(1,), stride=(1,))\n",
      "  )\n",
      "  (start_conv): Conv1d(256, 32, kernel_size=(1,), stride=(1,))\n",
      "  (end_conv_1): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))\n",
      "  (end_conv_2): Conv1d(512, 256, kernel_size=(1,), stride=(1,))\n",
      ")\n",
      "receptive field:  3070\n",
      "parameter count:  1834592\n"
     ]
    }
   ],
   "source": [
    "model = WaveNetModel(layers=10,\n",
    "                     blocks=3,\n",
    "                     dilation_channels=32,\n",
    "                     residual_channels=32,\n",
    "                     skip_channels=1024,\n",
    "                     end_channels=512, \n",
    "                     output_length=16,\n",
    "                     dtype=dtype, \n",
    "                     bias=True)\n",
    "# model = load_latest_model_from('snapshots', use_cuda=use_cuda)\n",
    "\n",
    "print('model: ', model.cuda())\n",
    "print('receptive field: ', model.receptive_field)\n",
    "print('parameter count: ', model.parameter_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Set\n",
    "To create the data set, you have to specify a path to a data set file. If this file already exists it will be used, if not it will be generated. If you want to generate the data set file (a ``.npz`` file), you have to specify the directory (``file_location``) in which all the audio files you want to use are located. The attribute ``target_length`` specifies the number of successive samples are used as a target and corresponds to the output length of the model. The ``item_length`` defines the number of samples in each item of the dataset and should always be ``model.receptive_field + model.output_length - 1``.\n",
    "\n",
    "```\n",
    "          |----receptive_field----|\n",
    "                                |--output_length--|\n",
    "example:  | | | | | | | | | | | | | | | | | | | | |\n",
    "target:                           | | | | | | | | | |  \n",
    "```\n",
    "To create a test set, you should define a ``test_stride``. Then each ``test_stride``th item will be assigned to the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one hot input\n",
      "the dataset has 598277 items\n"
     ]
    }
   ],
   "source": [
    "data = WavenetDataset(dataset_file='train_samples/bach_chaconne/dataset.npz',\n",
    "                      item_length=model.receptive_field + model.output_length - 1,\n",
    "                      target_length=model.output_length,\n",
    "                      file_location='train_samples/bach_chaconne',\n",
    "                      test_stride=500)\n",
    "print('the dataset has ' + str(len(data)) + ' items')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Logging\n",
    "This implementation supports logging with TensorBoard (you need to have TensorFlow installed). You can even generate audio samples from the current snapshot of the model during training. This will happen in a background thread on the cpu, so it will not interfere with the actual training but will be rather slow. If you don't have TensorFlow, you can use the standard logger that will print out to the console.\n",
    "The trainer uses Adam as default optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_log_samples(step):\n",
    "    sample_length=32000\n",
    "    gen_model = load_latest_model_from('snapshots', use_cuda=False)\n",
    "    print(\"start generating...\")\n",
    "    samples = generate_audio(gen_model,\n",
    "                             length=sample_length,\n",
    "                             temperatures=[0.5])\n",
    "    tf_samples = tf.convert_to_tensor(samples, dtype=tf.float32)\n",
    "    logger.audio_summary('temperature_0.5', tf_samples, step, sr=16000)\n",
    "\n",
    "    samples = generate_audio(gen_model,\n",
    "                             length=sample_length,\n",
    "                             temperatures=[1.])\n",
    "    tf_samples = tf.convert_to_tensor(samples, dtype=tf.float32)\n",
    "    logger.audio_summary('temperature_1.0', tf_samples, step, sr=16000)\n",
    "    print(\"audio clips generated\")\n",
    "\n",
    "\n",
    "#logger = TensorboardLogger(log_interval=200,\n",
    " #                          validation_interval=400,\n",
    "  #                         generate_interval=1000,\n",
    "   #                        generate_function=generate_and_log_samples,\n",
    "    #                       log_dir=\"logs/chaconne_model\")\n",
    "\n",
    "logger = Logger(log_interval=20,\n",
    "                 validation_interval=400,\n",
    "                 generate_interval=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training...\n",
      "epoch 0\n",
      "loss at step 20: 5.162780165672302\n",
      "loss at step 40: 5.089954590797424\n",
      "loss at step 60: 5.054309391975403\n",
      "loss at step 80: 4.960115432739258\n",
      "one training step does take approximately 0.5236522936820984 seconds)\n",
      "loss at step 100: 4.897372150421143\n",
      "loss at step 120: 4.790813255310058\n",
      "loss at step 140: 4.636266350746155\n",
      "loss at step 160: 4.484929537773132\n",
      "loss at step 180: 4.268103325366974\n",
      "loss at step 200: 4.15226628780365\n",
      "loss at step 220: 4.086933124065399\n",
      "loss at step 240: 4.068137288093567\n",
      "loss at step 260: 4.022809708118439\n",
      "loss at step 280: 4.026028096675873\n",
      "loss at step 300: 3.888850724697113\n",
      "loss at step 320: 3.8549909591674805\n",
      "loss at step 340: 3.834302806854248\n",
      "loss at step 360: 3.79020094871521\n",
      "loss at step 380: 3.7681735157966614\n",
      "loss at step 400: 3.810551917552948\n",
      "validation loss: 4.034210497538249\n",
      "validation accuracy: 6.787353923205343%\n",
      "loss at step 420: 3.756807065010071\n",
      "loss at step 440: 3.706703209877014\n",
      "loss at step 460: 3.831369996070862\n",
      "loss at step 480: 3.6715137481689455\n",
      "loss at step 500: 3.779381442070007\n",
      "loss at step 520: 3.7312689781188966\n",
      "loss at step 540: 3.6152003526687624\n",
      "loss at step 560: 3.6145357131958007\n",
      "loss at step 580: 3.6945917129516603\n",
      "loss at step 600: 3.6465631484985352\n",
      "loss at step 620: 3.6515214681625365\n",
      "loss at step 640: 3.580258142948151\n",
      "loss at step 660: 3.6273854970932007\n",
      "loss at step 680: 3.568354618549347\n",
      "loss at step 700: 3.6872965693473816\n",
      "loss at step 720: 3.5938130974769593\n",
      "loss at step 740: 3.5623600125312804\n",
      "loss at step 760: 3.548217403888702\n",
      "loss at step 780: 3.539596605300903\n",
      "loss at step 800: 3.550891363620758\n",
      "validation loss: 3.767513756752014\n",
      "validation accuracy: 7.314273789649415%\n",
      "loss at step 820: 3.5137772798538207\n",
      "loss at step 840: 3.5499584078788757\n",
      "loss at step 860: 3.5846980452537536\n",
      "loss at step 880: 3.532167983055115\n",
      "loss at step 900: 3.5456437945365904\n",
      "loss at step 920: 3.473503589630127\n",
      "loss at step 940: 3.5072280287742617\n",
      "loss at step 960: 3.491306483745575\n",
      "loss at step 980: 3.4998051524162292\n",
      "loss at step 1000: 3.554908049106598\n",
      "loss at step 1020: 3.5747060418128966\n",
      "loss at step 1040: 3.543783497810364\n",
      "loss at step 1060: 3.5057345986366273\n",
      "loss at step 1080: 3.464038920402527\n",
      "loss at step 1100: 3.48888623714447\n",
      "loss at step 1120: 3.4708704829216\n",
      "loss at step 1140: 3.500671458244324\n",
      "loss at step 1160: 3.5198220729827883\n",
      "loss at step 1180: 3.484594941139221\n",
      "loss at step 1200: 3.4575127959251404\n",
      "validation loss: 3.65242392539978\n",
      "validation accuracy: 8.164649415692821%\n",
      "loss at step 1220: 3.489265525341034\n",
      "loss at step 1240: 3.5714839696884155\n",
      "loss at step 1260: 3.43141165971756\n",
      "loss at step 1280: 3.5038999199867247\n",
      "loss at step 1300: 3.4580307960510255\n",
      "loss at step 1320: 3.4408175110816956\n",
      "loss at step 1340: 3.4244541049003603\n",
      "loss at step 1360: 3.4471516370773316\n",
      "loss at step 1380: 3.3921005725860596\n",
      "loss at step 1400: 3.4624038100242616\n",
      "loss at step 1420: 3.4178056597709654\n",
      "loss at step 1440: 3.4349681973457336\n",
      "loss at step 1460: 3.406816065311432\n",
      "loss at step 1480: 3.486594533920288\n",
      "loss at step 1500: 3.3872220396995543\n",
      "loss at step 1520: 3.4634222626686095\n",
      "loss at step 1540: 3.435411250591278\n",
      "loss at step 1560: 3.3409481167793276\n",
      "loss at step 1580: 3.4534283876419067\n",
      "loss at step 1600: 3.4325077891349793\n",
      "validation loss: 3.58068598429362\n",
      "validation accuracy: 7.914232053422371%\n",
      "loss at step 1620: 3.4672754645347594\n",
      "loss at step 1640: 3.347002184391022\n",
      "loss at step 1660: 3.4049446105957033\n",
      "loss at step 1680: 3.2862067580223084\n",
      "loss at step 1700: 3.453312110900879\n",
      "loss at step 1720: 3.3848121643066404\n",
      "loss at step 1740: 3.387905740737915\n",
      "loss at step 1760: 3.356485116481781\n",
      "loss at step 1780: 3.338104009628296\n",
      "loss at step 1800: 3.3607117891311646\n",
      "loss at step 1820: 3.3411010980606077\n",
      "loss at step 1840: 3.4154463171958924\n",
      "loss at step 1860: 3.3699005842208862\n",
      "loss at step 1880: 3.410442864894867\n",
      "loss at step 1900: 3.418602466583252\n",
      "loss at step 1920: 3.4308276057243345\n",
      "loss at step 1940: 3.365238058567047\n",
      "loss at step 1960: 3.351945185661316\n",
      "loss at step 1980: 3.403771722316742\n",
      "loss at step 2000: 3.3798001289367674\n",
      "validation loss: 3.5449328517913816\n",
      "validation accuracy: 8.660267111853088%\n",
      "loss at step 2020: 3.3425788998603823\n",
      "loss at step 2040: 3.319264125823975\n",
      "loss at step 2060: 3.394079840183258\n",
      "loss at step 2080: 3.296358644962311\n",
      "loss at step 2100: 3.336894929409027\n",
      "loss at step 2120: 3.3217463374137877\n",
      "loss at step 2140: 3.3359006524085997\n",
      "loss at step 2160: 3.334839868545532\n",
      "loss at step 2180: 3.288300633430481\n",
      "loss at step 2200: 3.3224949479103087\n",
      "loss at step 2220: 3.3523261904716493\n",
      "loss at step 2240: 3.3754207730293273\n",
      "loss at step 2260: 3.3085082292556764\n",
      "loss at step 2280: 3.345985400676727\n",
      "loss at step 2300: 3.2675327181816103\n",
      "loss at step 2320: 3.311201786994934\n",
      "loss at step 2340: 3.422169601917267\n",
      "loss at step 2360: 3.377267861366272\n",
      "loss at step 2380: 3.308018159866333\n",
      "loss at step 2400: 3.3407516717910766\n",
      "validation loss: 3.5149681409200033\n",
      "validation accuracy: 8.065525876460768%\n",
      "loss at step 2420: 3.2994808554649353\n",
      "loss at step 2440: 3.289099872112274\n",
      "loss at step 2460: 3.204926908016205\n",
      "loss at step 2480: 3.3410420536994936\n",
      "loss at step 2500: 3.3734574556350707\n",
      "loss at step 2520: 3.324077033996582\n",
      "loss at step 2540: 3.3650464534759523\n",
      "loss at step 2560: 3.3162861704826354\n",
      "loss at step 2580: 3.3536280393600464\n",
      "loss at step 2600: 3.387395989894867\n",
      "loss at step 2620: 3.3008108615875242\n",
      "loss at step 2640: 3.267950987815857\n",
      "loss at step 2660: 3.315680706501007\n",
      "loss at step 2680: 3.281283986568451\n",
      "loss at step 2700: 3.3603190302848818\n",
      "loss at step 2720: 3.3853434801101683\n",
      "loss at step 2740: 3.256915545463562\n",
      "loss at step 2760: 3.236005175113678\n",
      "loss at step 2780: 3.3345782995224\n",
      "loss at step 2800: 3.2895487427711485\n",
      "validation loss: 3.497975368499756\n",
      "validation accuracy: 8.613313856427379%\n",
      "loss at step 2820: 3.2417838096618654\n",
      "loss at step 2840: 3.3318774342536925\n",
      "loss at step 2860: 3.272347831726074\n",
      "loss at step 2880: 3.279472529888153\n",
      "loss at step 2900: 3.2444355607032778\n",
      "loss at step 2920: 3.304303765296936\n",
      "loss at step 2940: 3.261625146865845\n",
      "loss at step 2960: 3.3837171316146852\n",
      "loss at step 2980: 3.3126840949058534\n",
      "loss at step 3000: 3.2313539385795593\n",
      "loss at step 3020: 3.236228811740875\n",
      "loss at step 3040: 3.2188215136528013\n",
      "loss at step 3060: 3.301651620864868\n",
      "loss at step 3080: 3.284838914871216\n",
      "loss at step 3100: 3.2030568957328795\n",
      "loss at step 3120: 3.235882353782654\n",
      "loss at step 3140: 3.270344603061676\n",
      "loss at step 3160: 3.1983112692832947\n",
      "loss at step 3180: 3.2889007925987244\n",
      "loss at step 3200: 3.2698357820510866\n",
      "validation loss: 3.4337391392389933\n",
      "validation accuracy: 9.098497495826377%\n",
      "loss at step 3220: 3.263386404514313\n",
      "loss at step 3240: 3.3391095876693724\n",
      "loss at step 3260: 3.268175113201141\n",
      "loss at step 3280: 3.201871967315674\n",
      "loss at step 3300: 3.296235227584839\n",
      "loss at step 3320: 3.2143921971321108\n",
      "loss at step 3340: 3.2861374974250794\n",
      "loss at step 3360: 3.260668671131134\n",
      "loss at step 3380: 3.261797475814819\n",
      "loss at step 3400: 3.2969168424606323\n",
      "loss at step 3420: 3.2460426330566405\n",
      "loss at step 3440: 3.2327738761901856\n",
      "loss at step 3460: 3.2479109287261965\n",
      "loss at step 3480: 3.3091610431671143\n",
      "loss at step 3500: 3.1986521005630495\n",
      "loss at step 3520: 3.2509073972702027\n",
      "loss at step 3540: 3.3476327776908876\n",
      "loss at step 3560: 3.1986511945724487\n",
      "loss at step 3580: 3.289389503002167\n",
      "loss at step 3600: 3.2614575624465942\n",
      "validation loss: 3.416716674168905\n",
      "validation accuracy: 9.484557595993321%\n",
      "loss at step 3620: 3.213095462322235\n",
      "loss at step 3640: 3.2837355732917786\n",
      "loss at step 3660: 3.2822935461997984\n",
      "loss at step 3680: 3.1982131600379944\n",
      "loss at step 3700: 3.2611167192459107\n",
      "loss at step 3720: 3.2452615976333616\n",
      "loss at step 3740: 3.189968740940094\n",
      "loss at step 3760: 3.2766382694244385\n",
      "loss at step 3780: 3.2998820662498476\n",
      "loss at step 3800: 3.279159867763519\n",
      "loss at step 3820: 3.275946855545044\n",
      "loss at step 3840: 3.233194851875305\n",
      "loss at step 3860: 3.2544869422912597\n",
      "loss at step 3880: 3.2948657274246216\n",
      "loss at step 3900: 3.2324094653129576\n",
      "loss at step 3920: 3.269564139842987\n",
      "loss at step 3940: 3.236022245883942\n",
      "loss at step 3960: 3.1700457453727724\n",
      "loss at step 3980: 3.2581733107566833\n",
      "loss at step 4000: 3.2279407262802122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 3.4052259667714435\n",
      "validation accuracy: 9.24457429048414%\n",
      "loss at step 4020: 3.1789023518562316\n",
      "loss at step 4040: 3.2641913294792175\n",
      "loss at step 4060: 3.177927255630493\n",
      "loss at step 4080: 3.2050485610961914\n",
      "loss at step 4100: 3.2564444184303283\n",
      "loss at step 4120: 3.162487733364105\n",
      "loss at step 4140: 3.192570388317108\n",
      "loss at step 4160: 3.2074580430984496\n",
      "loss at step 4180: 3.2347574710845945\n",
      "loss at step 4200: 3.1917534470558167\n",
      "loss at step 4220: 3.2261887431144713\n",
      "loss at step 4240: 3.1985196232795716\n",
      "loss at step 4260: 3.2420174837112428\n",
      "loss at step 4280: 3.220473349094391\n",
      "loss at step 4300: 3.1718783020973205\n",
      "loss at step 4320: 3.1213488936424256\n",
      "loss at step 4340: 3.2046050906181334\n",
      "loss at step 4360: 3.2713900446891784\n",
      "loss at step 4380: 3.225289046764374\n",
      "loss at step 4400: 3.1465413928031922\n",
      "validation loss: 3.3603880818684897\n",
      "validation accuracy: 9.307178631051753%\n",
      "loss at step 4420: 3.1918266654014587\n",
      "loss at step 4440: 3.1917948484420777\n",
      "loss at step 4460: 3.2935174465179444\n",
      "loss at step 4480: 3.199791896343231\n",
      "loss at step 4500: 3.2156781792640685\n",
      "loss at step 4520: 3.196705961227417\n",
      "loss at step 4540: 3.1467873454093933\n",
      "loss at step 4560: 3.1496125936508177\n",
      "loss at step 4580: 3.127543807029724\n",
      "loss at step 4600: 3.2162680506706236\n",
      "loss at step 4620: 3.221375894546509\n",
      "loss at step 4640: 3.217782175540924\n",
      "loss at step 4660: 3.240075635910034\n",
      "loss at step 4680: 3.254497706890106\n",
      "loss at step 4700: 3.21086722612381\n",
      "loss at step 4720: 3.1472952723503114\n",
      "loss at step 4740: 3.1538251876831054\n",
      "loss at step 4760: 3.1699246764183044\n",
      "loss at step 4780: 3.2523606538772585\n",
      "loss at step 4800: 3.143801915645599\n",
      "validation loss: 3.38594536781311\n",
      "validation accuracy: 9.307178631051753%\n",
      "loss at step 4820: 3.274223434925079\n",
      "loss at step 4840: 3.186649572849274\n",
      "loss at step 4860: 3.161018657684326\n",
      "loss at step 4880: 3.1930309891700746\n",
      "loss at step 4900: 3.240886962413788\n",
      "loss at step 4920: 3.2401123881340026\n",
      "loss at step 4940: 3.2441720843315123\n",
      "loss at step 4960: 3.2022556900978087\n",
      "loss at step 4980: 3.1823893427848815\n",
      "loss at step 5000: 3.170304608345032\n",
      "loss at step 5020: 3.225197982788086\n",
      "loss at step 5040: 3.223167395591736\n",
      "loss at step 5060: 3.2032217144966126\n",
      "loss at step 5080: 3.2413482308387755\n",
      "loss at step 5100: 3.2186951994895936\n",
      "loss at step 5120: 3.1462902188301087\n",
      "loss at step 5140: 3.1533602833747865\n",
      "loss at step 5160: 3.0800784468650817\n",
      "loss at step 5180: 3.163555896282196\n",
      "loss at step 5200: 3.199887692928314\n",
      "validation loss: 3.3394398164749144\n",
      "validation accuracy: 9.834098497495827%\n",
      "loss at step 5220: 3.2097448110580444\n",
      "loss at step 5240: 3.1680140256881715\n",
      "loss at step 5260: 3.185634458065033\n",
      "loss at step 5280: 3.219908881187439\n",
      "loss at step 5300: 3.181726312637329\n",
      "loss at step 5320: 3.2586816906929017\n",
      "loss at step 5340: 3.167469024658203\n",
      "loss at step 5360: 3.1455276012420654\n",
      "loss at step 5380: 3.138654911518097\n",
      "loss at step 5400: 3.213850772380829\n",
      "loss at step 5420: 3.1283097147941588\n",
      "loss at step 5440: 3.1549739241600037\n",
      "loss at step 5460: 3.2170049786567687\n",
      "loss at step 5480: 3.2146286725997926\n",
      "loss at step 5500: 3.090758776664734\n",
      "loss at step 5520: 3.1361350297927855\n",
      "loss at step 5540: 3.141441059112549\n",
      "loss at step 5560: 3.1809924840927124\n",
      "loss at step 5580: 3.1625879645347594\n",
      "loss at step 5600: 3.2149297833442687\n",
      "validation loss: 3.3058108552296956\n",
      "validation accuracy: 9.823664440734557%\n",
      "loss at step 5620: 3.112679100036621\n",
      "loss at step 5640: 3.1840800404548646\n",
      "loss at step 5660: 3.2598034620285032\n",
      "loss at step 5680: 3.287079429626465\n",
      "loss at step 5700: 3.1548160314559937\n",
      "loss at step 5720: 3.18328400850296\n",
      "loss at step 5740: 3.169731616973877\n",
      "loss at step 5760: 3.104707455635071\n",
      "loss at step 5780: 3.203488624095917\n",
      "loss at step 5800: 3.1711271166801454\n",
      "loss at step 5820: 3.1253392338752746\n",
      "loss at step 5840: 3.1390083193778993\n",
      "loss at step 5860: 3.127161777019501\n",
      "loss at step 5880: 3.1083567261695864\n",
      "loss at step 5900: 3.168211054801941\n",
      "loss at step 5920: 3.163322937488556\n",
      "loss at step 5940: 3.2567975759506225\n",
      "loss at step 5960: 3.169167733192444\n",
      "loss at step 5980: 3.17657310962677\n",
      "loss at step 6000: 3.229836869239807\n",
      "validation loss: 3.3016204039255777\n",
      "validation accuracy: 10.3610183639399%\n",
      "loss at step 6020: 3.111521053314209\n",
      "loss at step 6040: 3.225587487220764\n",
      "loss at step 6060: 3.13105970621109\n",
      "loss at step 6080: 3.078478443622589\n",
      "loss at step 6100: 3.099349546432495\n",
      "loss at step 6120: 3.10810045003891\n",
      "loss at step 6140: 3.196535658836365\n",
      "loss at step 6160: 3.156123864650726\n",
      "loss at step 6180: 3.141645359992981\n",
      "loss at step 6200: 3.213159441947937\n",
      "loss at step 6220: 3.1548353672027587\n",
      "loss at step 6240: 3.0882585048675537\n",
      "loss at step 6260: 3.1925835490226744\n",
      "loss at step 6280: 3.1292120575904847\n",
      "loss at step 6300: 3.1644574880599974\n",
      "loss at step 6320: 3.159101736545563\n",
      "loss at step 6340: 3.2327715277671816\n",
      "loss at step 6360: 3.1838664293289183\n",
      "loss at step 6380: 3.15883492231369\n",
      "loss at step 6400: 3.201206052303314\n",
      "validation loss: 3.2809618536631264\n",
      "validation accuracy: 10.256677796327212%\n",
      "loss at step 6420: 3.0583606362342834\n",
      "loss at step 6440: 3.1444281220436094\n",
      "loss at step 6460: 3.1090900778770445\n",
      "loss at step 6480: 3.130047821998596\n",
      "loss at step 6500: 3.1133442282676698\n",
      "loss at step 6520: 3.1480159401893615\n",
      "loss at step 6540: 3.148287904262543\n",
      "loss at step 6560: 3.155971086025238\n",
      "loss at step 6580: 3.105110502243042\n",
      "loss at step 6600: 3.2083314776420595\n",
      "loss at step 6620: 3.1176071047782896\n",
      "loss at step 6640: 3.142746901512146\n",
      "loss at step 6660: 3.0801506876945495\n",
      "loss at step 6680: 3.1782379269599916\n",
      "loss at step 6700: 3.109402048587799\n",
      "loss at step 6720: 3.055630087852478\n",
      "loss at step 6740: 3.1957855939865114\n",
      "loss at step 6760: 3.2048173427581785\n",
      "loss at step 6780: 3.1613576769828797\n",
      "loss at step 6800: 3.141323006153107\n",
      "validation loss: 3.3502929639816283\n",
      "validation accuracy: 7.423831385642737%\n",
      "loss at step 6820: 3.108823871612549\n",
      "loss at step 6840: 3.15712993144989\n",
      "loss at step 6860: 3.184495186805725\n",
      "loss at step 6880: 3.11231609582901\n",
      "loss at step 6900: 3.1501442790031433\n",
      "loss at step 6920: 3.102106213569641\n",
      "loss at step 6940: 3.0889710068702696\n",
      "loss at step 6960: 3.162738490104675\n",
      "loss at step 6980: 3.1464204668998716\n",
      "loss at step 7000: 3.092785131931305\n",
      "loss at step 7020: 3.1881479620933533\n",
      "loss at step 7040: 3.107064354419708\n",
      "loss at step 7060: 3.09774090051651\n",
      "loss at step 7080: 3.12144421339035\n",
      "loss at step 7100: 3.1992302417755125\n",
      "loss at step 7120: 3.1182606339454653\n",
      "loss at step 7140: 3.169904887676239\n",
      "loss at step 7160: 3.108354127407074\n",
      "loss at step 7180: 3.173454463481903\n",
      "loss at step 7200: 3.1236337661743163\n",
      "validation loss: 3.260692893664042\n",
      "validation accuracy: 10.465358931552588%\n",
      "loss at step 7220: 3.1746203184127806\n",
      "loss at step 7240: 3.0730029106140138\n",
      "loss at step 7260: 3.1627981662750244\n",
      "loss at step 7280: 3.1355640649795533\n",
      "loss at step 7300: 3.1279593825340273\n",
      "loss at step 7320: 3.1162368655204773\n",
      "loss at step 7340: 3.106635046005249\n",
      "loss at step 7360: 3.2086947202682494\n",
      "loss at step 7380: 3.0771708607673647\n",
      "loss at step 7400: 3.1380083560943604\n",
      "loss at step 7420: 3.092010188102722\n",
      "loss at step 7440: 3.0966544151306152\n",
      "loss at step 7460: 3.1131974816322328\n",
      "loss at step 7480: 3.0988393664360045\n",
      "loss at step 7500: 3.1237738966941833\n",
      "loss at step 7520: 3.1109772324562073\n",
      "loss at step 7540: 3.1585439443588257\n",
      "loss at step 7560: 3.1323944449424745\n",
      "loss at step 7580: 3.091749334335327\n",
      "loss at step 7600: 3.121842610836029\n",
      "validation loss: 3.311360003153483\n",
      "validation accuracy: 10.037562604340568%\n",
      "loss at step 7620: 3.153369128704071\n",
      "loss at step 7640: 3.156485176086426\n",
      "loss at step 7660: 3.093612289428711\n",
      "loss at step 7680: 3.0547974944114684\n",
      "loss at step 7700: 3.0161519050598145\n",
      "loss at step 7720: 3.180259275436401\n",
      "loss at step 7740: 3.0931785225868227\n",
      "loss at step 7760: 3.1887346148490905\n",
      "loss at step 7780: 3.13005074262619\n",
      "loss at step 7800: 3.0972006559371947\n",
      "loss at step 7820: 3.0471675276756285\n",
      "loss at step 7840: 3.133992075920105\n",
      "loss at step 7860: 3.039440619945526\n",
      "loss at step 7880: 3.1986536264419554\n",
      "loss at step 7900: 3.1329755187034607\n",
      "loss at step 7920: 3.1147029280662535\n",
      "loss at step 7940: 3.1313987016677856\n",
      "loss at step 7960: 3.0435362815856934\n",
      "loss at step 7980: 3.108965277671814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at step 8000: 3.114830756187439\n",
      "validation loss: 3.235477124849955\n",
      "validation accuracy: 10.079298831385643%\n",
      "loss at step 8020: 3.128624749183655\n",
      "loss at step 8040: 3.111064302921295\n",
      "loss at step 8060: 3.221082878112793\n",
      "loss at step 8080: 3.1077369809150697\n",
      "loss at step 8100: 3.012094271183014\n",
      "loss at step 8120: 3.03189754486084\n",
      "loss at step 8140: 3.1080143094062804\n",
      "loss at step 8160: 3.0648762583732605\n",
      "loss at step 8180: 3.0929367184638976\n",
      "loss at step 8200: 3.075855779647827\n",
      "loss at step 8220: 3.095041048526764\n",
      "loss at step 8240: 3.0323700189590452\n",
      "loss at step 8260: 3.1083167552948\n",
      "loss at step 8280: 3.100862979888916\n",
      "loss at step 8300: 3.156915545463562\n",
      "loss at step 8320: 3.057618248462677\n",
      "loss at step 8340: 3.0737389087677003\n",
      "loss at step 8360: 3.158937060832977\n",
      "loss at step 8380: 3.063744866847992\n",
      "loss at step 8400: 3.063803267478943\n",
      "validation loss: 3.210866157213847\n",
      "validation accuracy: 10.929674457429048%\n",
      "loss at step 8420: 3.16995393037796\n",
      "loss at step 8440: 3.1445447206497192\n",
      "loss at step 8460: 3.080084943771362\n",
      "loss at step 8480: 3.085732841491699\n",
      "loss at step 8500: 3.0592994570732115\n",
      "loss at step 8520: 3.1319888114929197\n",
      "loss at step 8540: 3.1274685859680176\n",
      "loss at step 8560: 3.057730758190155\n",
      "loss at step 8580: 3.0928656339645384\n",
      "loss at step 8600: 3.0819387555122377\n",
      "loss at step 8620: 3.103783130645752\n",
      "loss at step 8640: 3.045433557033539\n",
      "loss at step 8660: 3.0562217950820925\n",
      "loss at step 8680: 3.077562320232391\n",
      "loss at step 8700: 3.069850194454193\n",
      "loss at step 8720: 3.1784082293510436\n",
      "loss at step 8740: 3.13589528799057\n",
      "loss at step 8760: 3.0882201910018923\n",
      "loss at step 8780: 3.063237488269806\n",
      "loss at step 8800: 3.141373085975647\n",
      "validation loss: 3.223122223218282\n",
      "validation accuracy: 10.66882303839733%\n",
      "loss at step 8820: 3.094023108482361\n",
      "loss at step 8840: 3.17200049161911\n",
      "loss at step 8860: 2.995798981189728\n",
      "loss at step 8880: 3.093529772758484\n",
      "loss at step 8900: 3.0188239097595213\n",
      "loss at step 8920: 3.143970715999603\n",
      "loss at step 8940: 2.984293794631958\n",
      "loss at step 8960: 3.1253246307373046\n",
      "loss at step 8980: 3.072639000415802\n",
      "loss at step 9000: 3.0868714570999147\n",
      "loss at step 9020: 3.146597123146057\n",
      "loss at step 9040: 3.096495544910431\n",
      "loss at step 9060: 3.093921256065369\n",
      "loss at step 9080: 3.1706905484199526\n",
      "loss at step 9100: 3.075187587738037\n",
      "loss at step 9120: 3.1025423645973205\n",
      "loss at step 9140: 3.111037349700928\n",
      "loss at step 9160: 3.0102914452552794\n",
      "loss at step 9180: 3.202008318901062\n",
      "loss at step 9200: 3.106417953968048\n",
      "validation loss: 3.1911418279012045\n",
      "validation accuracy: 11.190525876460768%\n",
      "loss at step 9220: 3.035524880886078\n",
      "loss at step 9240: 3.0040624022483824\n",
      "loss at step 9260: 3.075078475475311\n",
      "loss at step 9280: 3.080867278575897\n",
      "loss at step 9300: 3.107862639427185\n",
      "loss at step 9320: 3.114491009712219\n",
      "loss at step 9340: 3.0854039907455446\n",
      "loss at step 9360: 3.0372970342636108\n",
      "loss at step 9380: 3.0549315094947813\n",
      "loss at step 9400: 3.04789936542511\n",
      "loss at step 9420: 3.075324332714081\n",
      "loss at step 9440: 3.0871875643730164\n",
      "loss at step 9460: 3.1473661780357363\n",
      "loss at step 9480: 3.0774263739585876\n",
      "loss at step 9500: 3.1053746819496153\n",
      "loss at step 9520: 3.0577955842018127\n",
      "loss at step 9540: 3.098160719871521\n",
      "loss at step 9560: 3.0655354142189024\n",
      "loss at step 9580: 3.091194760799408\n",
      "loss at step 9600: 3.055843377113342\n",
      "validation loss: 3.21983118057251\n",
      "validation accuracy: 10.632303839732888%\n",
      "loss at step 9620: 3.119293284416199\n",
      "loss at step 9640: 3.010361921787262\n",
      "loss at step 9660: 3.094131076335907\n",
      "loss at step 9680: 3.046357297897339\n",
      "loss at step 9700: 3.070358693599701\n",
      "loss at step 9720: 3.0773604273796082\n",
      "loss at step 9740: 3.0989336252212523\n",
      "loss at step 9760: 3.0482956647872923\n",
      "loss at step 9780: 3.00627201795578\n",
      "loss at step 9800: 3.2060201168060303\n",
      "loss at step 9820: 3.051167333126068\n",
      "loss at step 9840: 3.0364241361618043\n",
      "loss at step 9860: 3.0255847334861756\n",
      "loss at step 9880: 3.038426923751831\n",
      "loss at step 9900: 2.971686089038849\n",
      "loss at step 9920: 3.1448965549468992\n",
      "loss at step 9940: 3.0945200085639955\n",
      "loss at step 9960: 3.0535472989082337\n",
      "loss at step 9980: 3.025958812236786\n",
      "loss at step 10000: 3.1054388523101806\n",
      "validation loss: 3.189391385714213\n",
      "validation accuracy: 10.929674457429048%\n",
      "loss at step 10020: 3.035224676132202\n",
      "loss at step 10040: 3.096083474159241\n",
      "loss at step 10060: 3.1141215682029726\n",
      "loss at step 10080: 3.1040130138397215\n",
      "loss at step 10100: 3.047789013385773\n",
      "loss at step 10120: 3.0518829464912414\n",
      "loss at step 10140: 3.0769892811775206\n",
      "loss at step 10160: 3.039759945869446\n",
      "loss at step 10180: 2.998803400993347\n",
      "loss at step 10200: 3.0633174180984497\n",
      "loss at step 10220: 3.084712898731232\n",
      "loss at step 10240: 3.0602805256843566\n",
      "loss at step 10260: 3.1040321350097657\n",
      "loss at step 10280: 2.990989458560944\n",
      "loss at step 10300: 3.053037774562836\n",
      "loss at step 10320: 3.032458984851837\n",
      "loss at step 10340: 3.0722692131996157\n",
      "loss at step 10360: 3.129328393936157\n",
      "loss at step 10380: 3.0729081869125365\n",
      "loss at step 10400: 3.1378607988357543\n",
      "validation loss: 3.198068143526713\n",
      "validation accuracy: 10.820116861435727%\n",
      "loss at step 10420: 3.0197749614715574\n",
      "loss at step 10440: 3.0839457750320434\n",
      "loss at step 10460: 3.0683174967765807\n",
      "loss at step 10480: 3.049256420135498\n",
      "loss at step 10500: 3.051543354988098\n",
      "loss at step 10520: 3.0688862800598145\n",
      "loss at step 10540: 3.0546145677566527\n",
      "loss at step 10560: 3.0870542168617248\n",
      "loss at step 10580: 3.0412153363227845\n",
      "loss at step 10600: 3.0115228295326233\n",
      "loss at step 10620: 3.0269877791404722\n",
      "loss at step 10640: 3.0661818981170654\n",
      "loss at step 10660: 3.0837358832359314\n",
      "loss at step 10680: 3.0671440601348876\n",
      "loss at step 10700: 3.085602414608002\n",
      "loss at step 10720: 2.978134608268738\n",
      "loss at step 10740: 3.025060164928436\n",
      "loss at step 10760: 3.1005228877067568\n",
      "loss at step 10780: 3.0519096851348877\n",
      "loss at step 10800: 3.0944212317466735\n",
      "validation loss: 3.1529694525400798\n",
      "validation accuracy: 11.232262103505843%\n",
      "loss at step 10820: 3.1549980640411377\n",
      "loss at step 10840: 3.0814727306365968\n",
      "loss at step 10860: 3.0999935030937196\n",
      "loss at step 10880: 3.069884479045868\n",
      "loss at step 10900: 3.093216896057129\n",
      "loss at step 10920: 3.0127581119537354\n",
      "loss at step 10940: 3.136329936981201\n",
      "loss at step 10960: 3.039872443675995\n",
      "loss at step 10980: 3.019363284111023\n",
      "loss at step 11000: 2.993453288078308\n",
      "loss at step 11020: 2.9963536858558655\n",
      "loss at step 11040: 2.9881896138191224\n",
      "loss at step 11060: 3.0830668568611146\n",
      "loss at step 11080: 3.0602046728134153\n",
      "loss at step 11100: 3.019976353645325\n",
      "loss at step 11120: 3.03772394657135\n",
      "loss at step 11140: 3.0179944753646852\n",
      "loss at step 11160: 3.0318714380264282\n",
      "loss at step 11180: 3.0640793442726135\n",
      "loss at step 11200: 3.104641878604889\n",
      "validation loss: 3.1675471957524617\n",
      "validation accuracy: 10.856636060100167%\n",
      "loss at step 11220: 3.052813458442688\n",
      "loss at step 11240: 3.073098051548004\n",
      "loss at step 11260: 3.0280767679214478\n",
      "loss at step 11280: 3.06314697265625\n",
      "loss at step 11300: 2.974433410167694\n",
      "loss at step 11320: 3.0254000425338745\n",
      "loss at step 11340: 3.0652124166488646\n",
      "loss at step 11360: 3.045314919948578\n",
      "loss at step 11380: 2.9937133193016052\n",
      "loss at step 11400: 2.9765068531036376\n",
      "loss at step 11420: 2.992470014095306\n",
      "loss at step 11440: 3.025304889678955\n",
      "loss at step 11460: 3.070116567611694\n",
      "loss at step 11480: 3.0229106068611147\n",
      "loss at step 11500: 2.974201273918152\n",
      "loss at step 11520: 3.0462703347206115\n",
      "loss at step 11540: 2.9994029879570006\n",
      "loss at step 11560: 3.0498625874519347\n",
      "loss at step 11580: 3.074270260334015\n",
      "loss at step 11600: 3.024523615837097\n",
      "validation loss: 3.1562841002146405\n",
      "validation accuracy: 11.070534223706177%\n",
      "loss at step 11620: 3.062232506275177\n",
      "loss at step 11640: 3.0404328107833862\n",
      "loss at step 11660: 2.996848404407501\n",
      "loss at step 11680: 3.02213830947876\n",
      "loss at step 11700: 3.0377739310264587\n",
      "loss at step 11720: 2.946715760231018\n",
      "loss at step 11740: 3.0853134036064147\n",
      "loss at step 11760: 3.110211706161499\n",
      "loss at step 11780: 3.0660373330116273\n",
      "loss at step 11800: 2.9851542949676513\n",
      "loss at step 11820: 3.0520655274391175\n",
      "loss at step 11840: 3.0081419110298158\n",
      "loss at step 11860: 3.0624622106552124\n",
      "loss at step 11880: 3.0570492506027223\n",
      "loss at step 11900: 3.0829777836799623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at step 11920: 3.0050369024276735\n",
      "loss at step 11940: 3.0233883261680603\n",
      "loss at step 11960: 3.042835235595703\n",
      "loss at step 11980: 3.0677093982696535\n",
      "loss at step 12000: 3.0230388879776\n",
      "validation loss: 3.129176624615987\n",
      "validation accuracy: 11.675709515859767%\n",
      "loss at step 12020: 3.055820894241333\n",
      "loss at step 12040: 3.0321760416030883\n",
      "loss at step 12060: 3.080505061149597\n",
      "loss at step 12080: 2.967938709259033\n",
      "loss at step 12100: 3.0617630958557127\n",
      "loss at step 12120: 3.060823452472687\n",
      "loss at step 12140: 3.043993330001831\n",
      "loss at step 12160: 2.953952968120575\n",
      "loss at step 12180: 3.0513290762901306\n",
      "loss at step 12200: 3.071247625350952\n",
      "loss at step 12220: 2.9987076997756956\n",
      "loss at step 12240: 3.024428296089172\n",
      "loss at step 12260: 3.0340832114219665\n",
      "loss at step 12280: 3.039249229431152\n",
      "loss at step 12300: 3.025560128688812\n",
      "loss at step 12320: 2.966801941394806\n",
      "loss at step 12340: 3.074874687194824\n",
      "loss at step 12360: 2.9879834771156313\n",
      "loss at step 12380: 3.0438493847846986\n",
      "loss at step 12400: 3.079745042324066\n",
      "validation loss: 3.200933044751485\n",
      "validation accuracy: 10.757512520868113%\n",
      "loss at step 12420: 3.1432470083236694\n",
      "loss at step 12440: 3.0712801098823546\n",
      "loss at step 12460: 2.9646761894226072\n",
      "loss at step 12480: 2.9395530223846436\n",
      "loss at step 12500: 3.0202109575271607\n",
      "loss at step 12520: 3.019967460632324\n",
      "loss at step 12540: 3.0136769652366637\n",
      "loss at step 12560: 2.964786410331726\n",
      "loss at step 12580: 2.9733171820640565\n",
      "loss at step 12600: 3.036171793937683\n",
      "loss at step 12620: 3.0397608518600463\n",
      "loss at step 12640: 3.0270942091941833\n",
      "loss at step 12660: 3.0518593072891234\n",
      "loss at step 12680: 2.9804118394851686\n",
      "loss at step 12700: 2.954395794868469\n",
      "loss at step 12720: 3.034534549713135\n",
      "loss at step 12740: 3.0071523070335386\n",
      "loss at step 12760: 3.0513758063316345\n",
      "loss at step 12780: 3.053744685649872\n",
      "loss at step 12800: 3.1212125778198243\n",
      "validation loss: 3.142386263211568\n",
      "validation accuracy: 11.487896494156928%\n",
      "loss at step 12820: 2.9755672812461853\n",
      "loss at step 12840: 3.0873298168182375\n",
      "loss at step 12860: 2.998221719264984\n",
      "loss at step 12880: 2.9790355563163757\n",
      "loss at step 12900: 3.0185063123703\n",
      "loss at step 12920: 3.0726765275001524\n",
      "loss at step 12940: 3.034471845626831\n",
      "loss at step 12960: 3.0871908187866213\n",
      "loss at step 12980: 2.9767313838005065\n",
      "loss at step 13000: 2.9632283091545104\n",
      "loss at step 13020: 3.0369311690330507\n",
      "loss at step 13040: 2.9276782631874085\n",
      "loss at step 13060: 3.0291311979293822\n",
      "loss at step 13080: 3.0300531148910523\n",
      "loss at step 13100: 3.028820204734802\n",
      "loss at step 13120: 3.003950047492981\n",
      "loss at step 13140: 3.0219621896743774\n",
      "loss at step 13160: 3.009967315196991\n",
      "loss at step 13180: 3.0563939809799194\n",
      "loss at step 13200: 3.1022948741912844\n",
      "validation loss: 3.17502268632253\n",
      "validation accuracy: 11.326168614357263%\n",
      "loss at step 13220: 3.0468495965003966\n",
      "loss at step 13240: 3.0423152804374696\n",
      "loss at step 13260: 3.017374873161316\n",
      "loss at step 13280: 2.9946258783340456\n",
      "loss at step 13300: 3.0200143694877624\n",
      "loss at step 13320: 3.0232116103172304\n",
      "loss at step 13340: 2.9225226402282716\n",
      "loss at step 13360: 3.099350154399872\n",
      "loss at step 13380: 3.0493019819259644\n",
      "loss at step 13400: 3.0441177129745483\n",
      "loss at step 13420: 2.995386576652527\n",
      "loss at step 13440: 3.055020582675934\n",
      "loss at step 13460: 3.007286822795868\n",
      "loss at step 13480: 2.9627830743789674\n",
      "loss at step 13500: 3.023239493370056\n",
      "loss at step 13520: 3.0111757040023805\n",
      "loss at step 13540: 2.9255013823509217\n",
      "loss at step 13560: 2.9794038414955137\n",
      "loss at step 13580: 3.0422560214996337\n",
      "loss at step 13600: 2.9142238974571226\n",
      "validation loss: 3.1112522649765015\n",
      "validation accuracy: 11.628756260434056%\n",
      "loss at step 13620: 3.1128290772438048\n",
      "loss at step 13640: 2.9981984972953795\n",
      "loss at step 13660: 2.979092264175415\n",
      "loss at step 13680: 3.0046011090278624\n",
      "loss at step 13700: 2.963867115974426\n",
      "loss at step 13720: 2.9843658804893494\n",
      "loss at step 13740: 2.9877817153930666\n",
      "loss at step 13760: 2.9864863991737365\n",
      "loss at step 13780: 3.078001630306244\n",
      "loss at step 13800: 2.93353408575058\n",
      "loss at step 13820: 3.0145173192024233\n",
      "loss at step 13840: 3.0104599356651307\n",
      "loss at step 13860: 2.9969409346580504\n",
      "loss at step 13880: 2.933981168270111\n",
      "loss at step 13900: 2.983620846271515\n",
      "loss at step 13920: 3.0683538556098937\n",
      "loss at step 13940: 3.002582323551178\n",
      "loss at step 13960: 3.02578341960907\n",
      "loss at step 13980: 2.984866499900818\n",
      "loss at step 14000: 3.0104724764823914\n",
      "validation loss: 3.1644167248408\n",
      "validation accuracy: 11.315734557595993%\n",
      "loss at step 14020: 3.005881631374359\n",
      "loss at step 14040: 3.0339658856391907\n",
      "loss at step 14060: 3.0308719635009767\n",
      "loss at step 14080: 2.9580358624458314\n",
      "loss at step 14100: 2.9206634759902954\n",
      "loss at step 14120: 2.9644039988517763\n",
      "loss at step 14140: 3.08241810798645\n",
      "loss at step 14160: 2.9503931641578673\n",
      "loss at step 14180: 3.07924028635025\n",
      "loss at step 14200: 3.0049310326576233\n",
      "loss at step 14220: 3.0211010694503786\n",
      "loss at step 14240: 3.0140049934387205\n",
      "loss at step 14260: 2.9906599164009093\n",
      "loss at step 14280: 3.097924065589905\n",
      "loss at step 14300: 3.017513656616211\n",
      "loss at step 14320: 3.0897608041763305\n",
      "loss at step 14340: 3.0330382227897643\n",
      "loss at step 14360: 3.017573869228363\n",
      "loss at step 14380: 3.0924087166786194\n",
      "loss at step 14400: 2.9258668184280396\n",
      "validation loss: 3.1125117985407513\n",
      "validation accuracy: 11.482679465776293%\n",
      "loss at step 14420: 3.0011932253837585\n",
      "loss at step 14440: 2.941356587409973\n",
      "loss at step 14460: 2.887032854557037\n",
      "loss at step 14480: 2.959624409675598\n",
      "loss at step 14500: 3.0205474615097048\n",
      "loss at step 14520: 2.9672128915786744\n",
      "loss at step 14540: 3.0365578055381777\n",
      "loss at step 14560: 2.9205228447914124\n",
      "loss at step 14580: 3.028666603565216\n",
      "loss at step 14600: 3.0134329557418824\n",
      "loss at step 14620: 2.9720402002334594\n",
      "loss at step 14640: 3.0293975949287413\n",
      "loss at step 14660: 3.0572227597236634\n",
      "loss at step 14680: 2.9988852620124815\n",
      "loss at step 14700: 2.9318318128585816\n",
      "loss at step 14720: 2.9351608872413637\n",
      "loss at step 14740: 3.077152156829834\n",
      "loss at step 14760: 3.012320578098297\n",
      "loss at step 14780: 2.9659706592559814\n",
      "loss at step 14800: 3.032914710044861\n",
      "validation loss: 3.1006047169367474\n",
      "validation accuracy: 11.576585976627712%\n",
      "loss at step 14820: 2.990949547290802\n",
      "loss at step 14840: 2.9661993980407715\n",
      "loss at step 14860: 3.0012669444084166\n",
      "loss at step 14880: 2.9073386073112486\n",
      "loss at step 14900: 3.0241045236587523\n",
      "loss at step 14920: 3.032801830768585\n",
      "loss at step 14940: 3.05520977973938\n",
      "loss at step 14960: 3.0253598928451537\n",
      "loss at step 14980: 2.92454217672348\n",
      "loss at step 15000: 3.04101015329361\n",
      "loss at step 15020: 2.9623720407485963\n",
      "loss at step 15040: 3.004222071170807\n",
      "loss at step 15060: 2.994655430316925\n",
      "loss at step 15080: 3.013580930233002\n",
      "loss at step 15100: 2.978698229789734\n",
      "loss at step 15120: 2.930661416053772\n",
      "loss at step 15140: 2.940584421157837\n",
      "loss at step 15160: 2.9789693713188172\n",
      "loss at step 15180: 3.030101954936981\n",
      "loss at step 15200: 2.990565812587738\n",
      "validation loss: 3.0875075960159304\n",
      "validation accuracy: 12.051335559265443%\n",
      "loss at step 15220: 3.0315160036087034\n",
      "loss at step 15240: 3.009184455871582\n",
      "loss at step 15260: 2.9683239698410033\n",
      "loss at step 15280: 2.9591479897499084\n",
      "loss at step 15300: 2.9704779148101808\n",
      "loss at step 15320: 2.9786210656166077\n",
      "loss at step 15340: 2.939243125915527\n",
      "loss at step 15360: 3.029543626308441\n",
      "loss at step 15380: 3.0201783537864686\n",
      "loss at step 15400: 3.055346441268921\n",
      "loss at step 15420: 2.9796881437301637\n",
      "loss at step 15440: 3.007674539089203\n",
      "loss at step 15460: 2.9537646651268004\n",
      "loss at step 15480: 2.9732340693473818\n",
      "loss at step 15500: 2.9080039739608763\n",
      "loss at step 15520: 2.9020771384239197\n",
      "loss at step 15540: 3.0205679655075075\n",
      "loss at step 15560: 2.986160326004028\n",
      "loss at step 15580: 2.949885380268097\n",
      "loss at step 15600: 2.9907307028770447\n",
      "validation loss: 3.1056532096862792\n",
      "validation accuracy: 11.753964941569283%\n",
      "loss at step 15620: 2.992674541473389\n",
      "loss at step 15640: 2.972311186790466\n",
      "loss at step 15660: 2.9843560338020323\n",
      "loss at step 15680: 3.01197030544281\n",
      "loss at step 15700: 2.9551008105278016\n",
      "loss at step 15720: 3.043199098110199\n",
      "loss at step 15740: 3.0463065266609193\n",
      "loss at step 15760: 2.9670813202857973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at step 15780: 3.0605921387672423\n",
      "loss at step 15800: 2.914723575115204\n",
      "loss at step 15820: 2.9062338352203367\n",
      "loss at step 15840: 3.02288738489151\n",
      "loss at step 15860: 3.0138417363166807\n",
      "loss at step 15880: 3.025317406654358\n",
      "loss at step 15900: 3.053657019138336\n",
      "loss at step 15920: 2.9703461170196532\n",
      "loss at step 15940: 2.924605929851532\n",
      "loss at step 15960: 2.962989556789398\n",
      "loss at step 15980: 2.967585599422455\n",
      "loss at step 16000: 2.9600739002227785\n",
      "validation loss: 3.0751457357406617\n",
      "validation accuracy: 12.087854757929883%\n",
      "loss at step 16020: 2.9348230719566346\n",
      "loss at step 16040: 2.980185353755951\n",
      "loss at step 16060: 2.9786244869232177\n",
      "loss at step 16080: 2.979057991504669\n",
      "loss at step 16100: 2.991746699810028\n",
      "loss at step 16120: 2.9444692492485047\n",
      "loss at step 16140: 2.9237182974815368\n",
      "loss at step 16160: 2.987958514690399\n",
      "loss at step 16180: 2.978611671924591\n",
      "loss at step 16200: 2.98181414604187\n",
      "loss at step 16220: 2.967973160743713\n",
      "loss at step 16240: 2.931735026836395\n",
      "loss at step 16260: 2.9502426981925964\n",
      "loss at step 16280: 2.9451584100723265\n",
      "loss at step 16300: 2.9877365112304686\n",
      "loss at step 16320: 2.9948360562324523\n",
      "loss at step 16340: 2.9756710529327393\n",
      "loss at step 16360: 2.958776044845581\n",
      "loss at step 16380: 2.9352006673812867\n",
      "loss at step 16400: 2.9334041118621825\n",
      "validation loss: 3.0854382514953613\n",
      "validation accuracy: 12.02525041736227%\n",
      "loss at step 16420: 2.918125069141388\n",
      "loss at step 16440: 2.9673648476600647\n",
      "loss at step 16460: 2.9411715507507323\n",
      "loss at step 16480: 2.883427584171295\n",
      "loss at step 16500: 2.9445896625518797\n",
      "loss at step 16520: 2.980478250980377\n",
      "loss at step 16540: 2.924523484706879\n",
      "loss at step 16560: 2.945625114440918\n",
      "loss at step 16580: 2.9631926655769347\n",
      "loss at step 16600: 2.9077826499938966\n",
      "loss at step 16620: 2.862565243244171\n",
      "loss at step 16640: 2.9680049300193785\n",
      "loss at step 16660: 2.9532947897911073\n",
      "loss at step 16680: 2.9637845754623413\n",
      "loss at step 16700: 2.9753051161766053\n",
      "loss at step 16720: 2.9726726174354554\n",
      "loss at step 16740: 2.952680468559265\n",
      "loss at step 16760: 3.00275719165802\n",
      "loss at step 16780: 2.9703692317008974\n",
      "loss at step 16800: 2.9853558778762816\n",
      "validation loss: 3.0687740214665733\n",
      "validation accuracy: 12.113939899833055%\n",
      "loss at step 16820: 2.8735902428627016\n",
      "loss at step 16840: 2.9224777817726135\n",
      "loss at step 16860: 2.9082557201385497\n",
      "loss at step 16880: 2.9175289034843446\n",
      "loss at step 16900: 2.9730836391448974\n",
      "loss at step 16920: 2.941361057758331\n",
      "loss at step 16940: 2.9399449586868287\n",
      "loss at step 16960: 2.862669277191162\n",
      "loss at step 16980: 2.9368392705917357\n",
      "loss at step 17000: 3.0396448135375977\n",
      "loss at step 17020: 2.968002450466156\n",
      "loss at step 17040: 2.967603087425232\n",
      "loss at step 17060: 3.0429640769958497\n",
      "loss at step 17080: 2.936882531642914\n",
      "loss at step 17100: 2.971145212650299\n",
      "loss at step 17120: 3.0064512968063353\n",
      "loss at step 17140: 2.907410502433777\n",
      "loss at step 17160: 2.9774434566497803\n",
      "loss at step 17180: 2.9353449940681458\n",
      "loss at step 17200: 2.9751975774765014\n",
      "validation loss: 3.080619099934896\n",
      "validation accuracy: 11.545283806343907%\n",
      "loss at step 17220: 2.984655725955963\n",
      "loss at step 17240: 2.9727270722389223\n",
      "loss at step 17260: 2.930089020729065\n",
      "loss at step 17280: 2.9836980223655702\n",
      "loss at step 17300: 2.880577766895294\n",
      "loss at step 17320: 2.9344850063323973\n",
      "loss at step 17340: 2.9897493362426757\n",
      "loss at step 17360: 2.9527461171150207\n",
      "loss at step 17380: 2.9867326974868775\n",
      "loss at step 17400: 2.913275682926178\n",
      "loss at step 17420: 2.9508264899253844\n",
      "loss at step 17440: 2.9315571427345275\n",
      "loss at step 17460: 2.975480687618256\n",
      "loss at step 17480: 2.9935723185539245\n",
      "loss at step 17500: 2.970129334926605\n",
      "loss at step 17520: 3.0102470397949217\n",
      "loss at step 17540: 2.9719198107719422\n",
      "loss at step 17560: 2.9283246278762816\n",
      "loss at step 17580: 2.853823745250702\n",
      "loss at step 17600: 3.001001811027527\n",
      "validation loss: 3.0548479477564494\n",
      "validation accuracy: 11.785267111853088%\n",
      "loss at step 17620: 3.0113983750343323\n",
      "loss at step 17640: 2.8796066522598265\n",
      "loss at step 17660: 2.9339924693107604\n",
      "loss at step 17680: 2.9627383589744567\n",
      "loss at step 17700: 2.906320035457611\n",
      "loss at step 17720: 2.8406140327453615\n",
      "loss at step 17740: 3.003671395778656\n",
      "loss at step 17760: 2.956338131427765\n",
      "loss at step 17780: 2.9435264706611632\n",
      "loss at step 17800: 2.899008595943451\n",
      "loss at step 17820: 2.9709632635116576\n",
      "loss at step 17840: 2.958760976791382\n",
      "loss at step 17860: 2.922394108772278\n",
      "loss at step 17880: 2.990147817134857\n",
      "loss at step 17900: 2.97401305437088\n",
      "loss at step 17920: 2.9421247959136965\n",
      "loss at step 17940: 2.9788427710533143\n",
      "loss at step 17960: 2.967996609210968\n",
      "loss at step 17980: 2.9201876878738404\n",
      "loss at step 18000: 2.941854178905487\n",
      "validation loss: 3.087749967575073\n",
      "validation accuracy: 11.602671118530884%\n",
      "loss at step 18020: 2.9206658601760864\n",
      "loss at step 18040: 3.02332090139389\n",
      "loss at step 18060: 2.9759071946144102\n",
      "loss at step 18080: 2.894530463218689\n",
      "loss at step 18100: 2.9448574662208555\n",
      "loss at step 18120: 2.9148103475570677\n",
      "loss at step 18140: 2.92845219373703\n",
      "loss at step 18160: 2.915965187549591\n",
      "loss at step 18180: 2.94862060546875\n",
      "loss at step 18200: 2.9549599289894104\n",
      "loss at step 18220: 2.9556174516677856\n",
      "loss at step 18240: 2.9496944546699524\n",
      "loss at step 18260: 2.948808252811432\n",
      "loss at step 18280: 2.984957790374756\n",
      "loss at step 18300: 2.9895437002182006\n",
      "loss at step 18320: 2.896469295024872\n",
      "loss at step 18340: 2.93359717130661\n",
      "loss at step 18360: 3.0337045669555662\n",
      "loss at step 18380: 2.9473606824874876\n",
      "loss at step 18400: 2.9488240122795104\n",
      "validation loss: 3.0587306102116902\n",
      "validation accuracy: 12.207846410684473%\n",
      "loss at step 18420: 2.974714386463165\n",
      "loss at step 18440: 2.840873968601227\n",
      "loss at step 18460: 2.974634253978729\n",
      "loss at step 18480: 2.8870375752449036\n",
      "loss at step 18500: 2.923749804496765\n",
      "loss at step 18520: 3.0614203929901125\n",
      "loss at step 18540: 2.915036308765411\n",
      "loss at step 18560: 2.903782677650452\n",
      "loss at step 18580: 3.024248170852661\n",
      "loss at step 18600: 2.9495837569236754\n",
      "loss at step 18620: 2.922239661216736\n",
      "loss at step 18640: 2.91385840177536\n",
      "loss at step 18660: 2.9859577775001527\n",
      "loss at step 18680: 2.8920791506767274\n",
      "loss at step 18700: 2.9690611124038697\n",
      "loss at step 18720: 2.9565756916999817\n",
      "loss at step 18740: 2.999614655971527\n",
      "loss at step 18760: 2.921702742576599\n",
      "loss at step 18780: 2.899166202545166\n",
      "loss at step 18800: 2.9859037160873414\n",
      "validation loss: 3.0534633604685464\n",
      "validation accuracy: 11.946994991652755%\n",
      "loss at step 18820: 2.94744690656662\n",
      "loss at step 18840: 2.814818012714386\n",
      "loss at step 18860: 2.932048296928406\n",
      "loss at step 18880: 2.9493153929710387\n",
      "loss at step 18900: 2.9578338980674745\n",
      "loss at step 18920: 2.9140341520309447\n",
      "loss at step 18940: 2.977270555496216\n",
      "loss at step 18960: 2.994143307209015\n",
      "loss at step 18980: 2.944341707229614\n",
      "loss at step 19000: 2.9580817103385924\n",
      "loss at step 19020: 3.0081531405448914\n",
      "loss at step 19040: 2.900889015197754\n",
      "loss at step 19060: 2.860466814041138\n",
      "loss at step 19080: 3.0202114820480346\n",
      "loss at step 19100: 2.902229738235474\n",
      "loss at step 19120: 2.926822078227997\n",
      "loss at step 19140: 2.980498456954956\n",
      "loss at step 19160: 2.9268810510635377\n",
      "loss at step 19180: 2.9101746559143065\n",
      "loss at step 19200: 2.9270731091499327\n",
      "validation loss: 3.064235146840413\n",
      "validation accuracy: 12.213063439065108%\n",
      "loss at step 19220: 2.913629972934723\n",
      "loss at step 19240: 2.939676308631897\n",
      "loss at step 19260: 2.962695574760437\n",
      "loss at step 19280: 2.874348771572113\n",
      "loss at step 19300: 2.9248289823532105\n",
      "loss at step 19320: 2.9769556164741515\n",
      "loss at step 19340: 2.9162935495376585\n",
      "loss at step 19360: 2.9737952947616577\n",
      "loss at step 19380: 2.926535987854004\n",
      "loss at step 19400: 2.8321431517601012\n",
      "loss at step 19420: 3.0432233214378357\n",
      "loss at step 19440: 3.0113436222076415\n",
      "loss at step 19460: 2.9463367581367494\n",
      "loss at step 19480: 2.997298169136047\n",
      "loss at step 19500: 2.941081404685974\n",
      "loss at step 19520: 2.877323639392853\n",
      "loss at step 19540: 2.8454123735427856\n",
      "loss at step 19560: 2.8880463600158692\n",
      "loss at step 19580: 2.883936655521393\n",
      "loss at step 19600: 2.9794604182243347\n",
      "validation loss: 3.2006032482783\n",
      "validation accuracy: 11.712228714524207%\n",
      "loss at step 19620: 3.0169781684875487\n",
      "loss at step 19640: 3.023118829727173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at step 19660: 2.8733541488647463\n",
      "loss at step 19680: 3.0222919821739196\n",
      "loss at step 19700: 2.969448411464691\n",
      "loss at step 19720: 2.92838397026062\n",
      "loss at step 19740: 2.961627697944641\n",
      "loss at step 19760: 2.9649990439414977\n",
      "loss at step 19780: 2.9534488201141356\n",
      "loss at step 19800: 3.004178774356842\n",
      "loss at step 19820: 2.952987849712372\n",
      "loss at step 19840: 2.8952455878257752\n",
      "loss at step 19860: 2.927228546142578\n",
      "loss at step 19880: 2.8792880892753603\n",
      "loss at step 19900: 2.9473291993141175\n",
      "loss at step 19920: 2.9616614103317263\n",
      "loss at step 19940: 2.8822675824165342\n",
      "loss at step 19960: 2.9603505730628967\n",
      "loss at step 19980: 2.8559457778930666\n",
      "loss at step 20000: 3.005399703979492\n",
      "validation loss: 3.0161498181025186\n",
      "validation accuracy: 12.588689482470786%\n",
      "loss at step 20020: 2.950360369682312\n",
      "loss at step 20040: 2.921996772289276\n",
      "loss at step 20060: 2.8846943974494934\n",
      "loss at step 20080: 2.944383442401886\n",
      "loss at step 20100: 3.00750390291214\n",
      "loss at step 20120: 2.8378545522689818\n",
      "loss at step 20140: 2.9715126156806946\n",
      "loss at step 20160: 2.808638334274292\n",
      "loss at step 20180: 2.9221349000930785\n",
      "loss at step 20200: 2.9682042360305787\n",
      "loss at step 20220: 2.9011128306388856\n",
      "loss at step 20240: 2.9729838728904725\n",
      "loss at step 20260: 2.8814172983169555\n",
      "loss at step 20280: 2.9659401297569277\n",
      "loss at step 20300: 2.86286084651947\n",
      "loss at step 20320: 2.90041823387146\n",
      "loss at step 20340: 2.970808374881744\n",
      "loss at step 20360: 2.8515517473220826\n",
      "loss at step 20380: 2.9474161148071287\n",
      "loss at step 20400: 2.932567536830902\n",
      "validation loss: 3.0060729201634726\n",
      "validation accuracy: 12.781719532554256%\n",
      "loss at step 20420: 2.869327890872955\n",
      "loss at step 20440: 2.860227274894714\n",
      "loss at step 20460: 2.995893347263336\n",
      "loss at step 20480: 2.938115179538727\n",
      "loss at step 20500: 3.008282482624054\n",
      "loss at step 20520: 2.9249181151390076\n",
      "loss at step 20540: 2.9211480140686037\n",
      "loss at step 20560: 2.8966073989868164\n",
      "loss at step 20580: 2.8996844410896303\n",
      "loss at step 20600: 2.938025987148285\n",
      "loss at step 20620: 2.8837666153907775\n",
      "loss at step 20640: 2.9116308093070984\n",
      "loss at step 20660: 2.8997251391410828\n",
      "loss at step 20680: 2.9047473073005676\n",
      "loss at step 20700: 2.971231150627136\n",
      "loss at step 20720: 2.93525310754776\n",
      "loss at step 20740: 2.9469278216362\n",
      "loss at step 20760: 2.931907796859741\n",
      "loss at step 20780: 2.8518369793891907\n",
      "loss at step 20800: 2.9358021140098574\n",
      "validation loss: 3.0346854368845624\n",
      "validation accuracy: 12.051335559265443%\n",
      "loss at step 20820: 2.9486483931541443\n",
      "loss at step 20840: 2.859795129299164\n",
      "loss at step 20860: 2.882890009880066\n",
      "loss at step 20880: 2.9702000975608827\n",
      "loss at step 20900: 2.937760925292969\n",
      "loss at step 20920: 2.900557744503021\n",
      "loss at step 20940: 2.9345890164375303\n",
      "loss at step 20960: 2.9572490215301515\n",
      "loss at step 20980: 2.8688390493392943\n",
      "loss at step 21000: 2.8636072158813475\n",
      "loss at step 21020: 2.9578373193740846\n",
      "loss at step 21040: 2.897536075115204\n",
      "loss at step 21060: 2.913705515861511\n",
      "loss at step 21080: 2.895331597328186\n",
      "loss at step 21100: 2.867563021183014\n",
      "loss at step 21120: 2.936935341358185\n",
      "loss at step 21140: 2.9402999758720396\n",
      "loss at step 21160: 2.9558377265930176\n",
      "loss at step 21180: 2.891489017009735\n",
      "loss at step 21200: 2.926301729679108\n",
      "validation loss: 3.016113166809082\n",
      "validation accuracy: 12.280884808013354%\n",
      "loss at step 21220: 2.8845929741859435\n",
      "loss at step 21240: 2.878484880924225\n",
      "loss at step 21260: 2.932022416591644\n",
      "loss at step 21280: 2.967177760601044\n",
      "loss at step 21300: 2.8615611672401426\n",
      "loss at step 21320: 2.8905576944351195\n",
      "loss at step 21340: 2.9500420689582825\n",
      "loss at step 21360: 2.916179895401001\n",
      "loss at step 21380: 2.9372027039527895\n",
      "loss at step 21400: 2.907539117336273\n",
      "loss at step 21420: 2.9410642981529236\n",
      "loss at step 21440: 2.898580551147461\n",
      "loss at step 21460: 2.9838014245033264\n",
      "loss at step 21480: 2.904649484157562\n",
      "loss at step 21500: 2.868173360824585\n",
      "loss at step 21520: 2.8004294991493226\n",
      "loss at step 21540: 2.78819500207901\n",
      "loss at step 21560: 2.946673095226288\n",
      "loss at step 21580: 2.8807740569114686\n",
      "loss at step 21600: 2.9288016676902773\n",
      "validation loss: 3.004990111986796\n",
      "validation accuracy: 12.395659432387312%\n",
      "loss at step 21620: 2.8533623576164246\n",
      "loss at step 21640: 2.945339226722717\n",
      "loss at step 21660: 2.891169548034668\n",
      "loss at step 21680: 2.8834344267845156\n",
      "loss at step 21700: 2.9086088299751283\n",
      "loss at step 21720: 2.9014152765274046\n",
      "loss at step 21740: 2.8715910077095033\n",
      "loss at step 21760: 2.907626712322235\n",
      "loss at step 21780: 2.9143624901771545\n",
      "loss at step 21800: 2.9225573182106017\n",
      "loss at step 21820: 2.871239352226257\n",
      "loss at step 21840: 2.8679210901260377\n",
      "loss at step 21860: 2.918833243846893\n",
      "loss at step 21880: 2.8963462471961976\n",
      "loss at step 21900: 2.894170653820038\n",
      "loss at step 21920: 2.937592113018036\n",
      "loss at step 21940: 2.9301206946372984\n",
      "loss at step 21960: 2.8823460698127747\n",
      "loss at step 21980: 2.8555799961090087\n",
      "loss at step 22000: 2.8545642733573913\n",
      "validation loss: 3.0086817820866902\n",
      "validation accuracy: 12.301752921535893%\n",
      "loss at step 22020: 2.9303733110427856\n",
      "loss at step 22040: 2.9485596418380737\n",
      "loss at step 22060: 2.915502965450287\n",
      "loss at step 22080: 2.979552149772644\n",
      "loss at step 22100: 2.9345261454582214\n",
      "loss at step 22120: 2.925577986240387\n",
      "loss at step 22140: 3.0259608149528505\n",
      "loss at step 22160: 2.9263623714447022\n",
      "loss at step 22180: 2.911174988746643\n",
      "loss at step 22200: 2.9007068395614626\n",
      "loss at step 22220: 3.005685496330261\n",
      "loss at step 22240: 2.86807986497879\n",
      "loss at step 22260: 2.9748754143714904\n",
      "loss at step 22280: 2.9030596733093263\n",
      "loss at step 22300: 2.9550400972366333\n",
      "loss at step 22320: 2.9319032788276673\n",
      "loss at step 22340: 2.891273581981659\n",
      "loss at step 22360: 2.9127839326858522\n",
      "loss at step 22380: 2.8995579719543456\n",
      "loss at step 22400: 2.929039251804352\n",
      "validation loss: 3.014128333727519\n",
      "validation accuracy: 12.385225375626044%\n",
      "loss at step 22420: 2.9212110161781313\n",
      "loss at step 22440: 2.8146780371665954\n",
      "loss at step 22460: 2.9284982442855836\n",
      "loss at step 22480: 2.894105875492096\n",
      "loss at step 22500: 2.8272602200508117\n",
      "loss at step 22520: 2.9012372732162475\n",
      "loss at step 22540: 2.84966584444046\n",
      "loss at step 22560: 3.0004779577255247\n",
      "loss at step 22580: 2.9798694968223574\n",
      "loss at step 22600: 2.8965977787971497\n",
      "loss at step 22620: 2.896556043624878\n",
      "loss at step 22640: 2.924087679386139\n",
      "loss at step 22660: 2.908049428462982\n",
      "loss at step 22680: 2.9049750924110413\n",
      "loss at step 22700: 2.886305606365204\n",
      "loss at step 22720: 2.9003302812576295\n",
      "loss at step 22740: 2.8735748767852782\n",
      "loss at step 22760: 2.9055835127830507\n",
      "loss at step 22780: 2.8819980144500734\n",
      "loss at step 22800: 2.8586060762405396\n",
      "validation loss: 3.0298404343922933\n",
      "validation accuracy: 12.45304674457429%\n",
      "loss at step 22820: 2.9315086364746095\n",
      "loss at step 22840: 2.900684952735901\n",
      "loss at step 22860: 2.9070473670959474\n",
      "loss at step 22880: 2.887365627288818\n",
      "loss at step 22900: 2.900019085407257\n",
      "loss at step 22920: 2.9058138847351076\n",
      "loss at step 22940: 2.826592779159546\n",
      "loss at step 22960: 2.816423034667969\n",
      "loss at step 22980: 2.8032089471817017\n",
      "loss at step 23000: 2.8914891600608827\n",
      "loss at step 23020: 2.9954081654548643\n",
      "loss at step 23040: 2.880561923980713\n",
      "loss at step 23060: 2.9456369519233703\n",
      "loss at step 23080: 2.8354941487312315\n",
      "loss at step 23100: 2.923426961898804\n",
      "loss at step 23120: 2.939865791797638\n",
      "loss at step 23140: 2.8480807304382325\n",
      "loss at step 23160: 2.911291813850403\n",
      "loss at step 23180: 2.895187020301819\n",
      "loss at step 23200: 2.8438782334327697\n",
      "validation loss: 2.9981654914220175\n",
      "validation accuracy: 12.390442404006679%\n",
      "loss at step 23220: 2.8934043526649473\n",
      "loss at step 23240: 2.963593053817749\n",
      "loss at step 23260: 2.8741085052490236\n",
      "loss at step 23280: 2.9329764008522035\n",
      "loss at step 23300: 2.901649463176727\n",
      "loss at step 23320: 2.887140166759491\n",
      "loss at step 23340: 2.88855699300766\n",
      "loss at step 23360: 2.9172905683517456\n",
      "loss at step 23380: 2.8708176732063295\n",
      "loss at step 23400: 2.889760875701904\n",
      "loss at step 23420: 3.0008237838745115\n",
      "loss at step 23440: 2.829125154018402\n",
      "loss at step 23460: 2.9349993467330933\n",
      "loss at step 23480: 2.9099552512168883\n",
      "loss at step 23500: 2.875960075855255\n",
      "loss at step 23520: 2.9031049489974974\n",
      "loss at step 23540: 2.8778003573417665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at step 23560: 2.8911163449287414\n",
      "loss at step 23580: 2.934093940258026\n",
      "loss at step 23600: 2.887026333808899\n",
      "validation loss: 2.9883971865971883\n",
      "validation accuracy: 12.93823038397329%\n",
      "loss at step 23620: 2.939064919948578\n",
      "loss at step 23640: 3.0214253067970276\n",
      "loss at step 23660: 2.8852784276008605\n",
      "loss at step 23680: 2.818788480758667\n",
      "loss at step 23700: 2.9638047575950623\n",
      "loss at step 23720: 2.8777089595794676\n",
      "loss at step 23740: 2.9331770658493044\n",
      "loss at step 23760: 2.9146593809127808\n",
      "loss at step 23780: 2.924949753284454\n",
      "loss at step 23800: 2.856704044342041\n",
      "loss at step 23820: 2.9359554052352905\n",
      "loss at step 23840: 2.894316041469574\n",
      "loss at step 23860: 2.8419658541679382\n",
      "loss at step 23880: 2.907114124298096\n",
      "loss at step 23900: 2.804286277294159\n",
      "loss at step 23920: 2.9262362241744997\n",
      "loss at step 23940: 2.7953301668167114\n",
      "loss at step 23960: 2.7992517828941343\n",
      "loss at step 23980: 2.8759032845497132\n",
      "loss at step 24000: 2.879504954814911\n",
      "validation loss: 2.974695094426473\n",
      "validation accuracy: 13.120826377295492%\n",
      "loss at step 24020: 2.8866199374198915\n",
      "loss at step 24040: 2.842275607585907\n",
      "loss at step 24060: 2.9709249258041384\n",
      "loss at step 24080: 2.923212695121765\n",
      "loss at step 24100: 2.9665279150009156\n",
      "loss at step 24120: 2.8905568957328795\n",
      "loss at step 24140: 2.893786311149597\n",
      "loss at step 24160: 2.837188696861267\n",
      "loss at step 24180: 2.8821031928062437\n",
      "loss at step 24200: 2.7653693079948427\n",
      "loss at step 24220: 2.8936521768569947\n",
      "loss at step 24240: 2.82872953414917\n",
      "loss at step 24260: 2.895652985572815\n",
      "loss at step 24280: 2.8959399104118346\n",
      "loss at step 24300: 2.8684136152267454\n",
      "loss at step 24320: 2.895880675315857\n",
      "loss at step 24340: 2.92342871427536\n",
      "loss at step 24360: 2.8663317918777467\n",
      "loss at step 24380: 2.916807246208191\n",
      "loss at step 24400: 2.8454768657684326\n",
      "validation loss: 3.0074316676457724\n",
      "validation accuracy: 12.051335559265443%\n",
      "loss at step 24420: 2.832845902442932\n",
      "loss at step 24440: 2.9128371596336367\n",
      "loss at step 24460: 2.879225993156433\n",
      "loss at step 24480: 2.8352208375930785\n",
      "loss at step 24500: 2.8569995522499085\n",
      "loss at step 24520: 2.967583978176117\n",
      "loss at step 24540: 2.951312530040741\n",
      "loss at step 24560: 2.928274917602539\n",
      "loss at step 24580: 2.853125309944153\n",
      "loss at step 24600: 2.915175664424896\n",
      "loss at step 24620: 2.89818160533905\n",
      "loss at step 24640: 2.8212403297424316\n",
      "loss at step 24660: 2.8069207191467287\n",
      "loss at step 24680: 2.882113254070282\n",
      "loss at step 24700: 2.8426461696624754\n",
      "loss at step 24720: 2.866349768638611\n",
      "loss at step 24740: 2.9259763240814207\n",
      "loss at step 24760: 2.897370457649231\n",
      "loss at step 24780: 2.8947734117507933\n",
      "loss at step 24800: 2.920205092430115\n",
      "validation loss: 3.019847904841105\n",
      "validation accuracy: 12.672161936560936%\n",
      "loss at step 24820: 2.8258801817893984\n",
      "loss at step 24840: 2.94873331785202\n",
      "loss at step 24860: 2.8825683116912844\n",
      "loss at step 24880: 2.8973331809043885\n",
      "loss at step 24900: 2.8554778933525085\n",
      "loss at step 24920: 2.8655975699424743\n",
      "loss at step 24940: 2.843027424812317\n",
      "loss at step 24960: 2.8876847982406617\n",
      "loss at step 24980: 2.942628502845764\n",
      "loss at step 25000: 2.887362563610077\n",
      "loss at step 25020: 2.8468472480773928\n",
      "loss at step 25040: 2.852978777885437\n",
      "loss at step 25060: 2.877710211277008\n",
      "loss at step 25080: 2.859414291381836\n",
      "loss at step 25100: 2.8350038170814513\n",
      "loss at step 25120: 2.9153769850730895\n",
      "loss at step 25140: 2.8496907711029054\n",
      "loss at step 25160: 2.81503369808197\n",
      "loss at step 25180: 2.823034930229187\n",
      "loss at step 25200: 2.8963809967041017\n",
      "validation loss: 2.985726353327433\n",
      "validation accuracy: 12.745200333889816%\n",
      "loss at step 25220: 2.95091712474823\n",
      "loss at step 25240: 2.8231853246688843\n",
      "loss at step 25260: 2.9488439202308654\n",
      "loss at step 25280: 2.907191777229309\n",
      "loss at step 25300: 2.9576502323150633\n",
      "loss at step 25320: 2.9470738410949706\n",
      "loss at step 25340: 2.833743941783905\n",
      "loss at step 25360: 2.9024502992630006\n",
      "loss at step 25380: 2.8774449944496157\n",
      "loss at step 25400: 2.884017896652222\n",
      "loss at step 25420: 2.8086140036582945\n",
      "loss at step 25440: 2.8900937676429748\n",
      "loss at step 25460: 2.9429056525230406\n",
      "loss at step 25480: 2.909050798416138\n",
      "loss at step 25500: 2.9097013592720034\n",
      "loss at step 25520: 2.8025253653526305\n",
      "loss at step 25540: 2.9033837795257567\n",
      "loss at step 25560: 2.9055112838745116\n",
      "loss at step 25580: 2.902617299556732\n",
      "loss at step 25600: 2.8468979477882383\n",
      "validation loss: 2.964552343686422\n",
      "validation accuracy: 12.933013355592655%\n",
      "loss at step 25620: 2.8613141894340517\n",
      "loss at step 25640: 2.872975778579712\n",
      "loss at step 25660: 2.999994325637817\n",
      "loss at step 25680: 2.7873329401016234\n",
      "loss at step 25700: 2.926127481460571\n",
      "loss at step 25720: 2.8603041768074036\n",
      "loss at step 25740: 2.8486266255378725\n",
      "loss at step 25760: 2.784803032875061\n",
      "loss at step 25780: 2.8722842931747437\n",
      "loss at step 25800: 2.924896240234375\n",
      "loss at step 25820: 2.79207980632782\n",
      "loss at step 25840: 2.886992311477661\n",
      "loss at step 25860: 2.878796136379242\n",
      "loss at step 25880: 2.829134833812714\n",
      "loss at step 25900: 2.88776535987854\n",
      "loss at step 25920: 2.83191921710968\n",
      "loss at step 25940: 2.8803319334983826\n",
      "loss at step 25960: 2.8157090187072753\n",
      "loss at step 25980: 2.7751639246940614\n",
      "loss at step 26000: 2.849360704421997\n",
      "validation loss: 2.989454086621602\n",
      "validation accuracy: 13.10517529215359%\n",
      "loss at step 26020: 2.8539392352104187\n",
      "loss at step 26040: 2.8227805495262146\n",
      "loss at step 26060: 2.8333847165107726\n",
      "loss at step 26080: 2.862946403026581\n",
      "loss at step 26100: 2.860756349563599\n",
      "loss at step 26120: 2.7604338765144347\n",
      "loss at step 26140: 2.8975625038146973\n",
      "loss at step 26160: 2.819292223453522\n",
      "loss at step 26180: 2.8296712756156923\n",
      "loss at step 26200: 2.893090033531189\n",
      "loss at step 26220: 2.8295498967170714\n",
      "loss at step 26240: 2.9605763792991637\n",
      "loss at step 26260: 2.8959729433059693\n",
      "loss at step 26280: 2.812489926815033\n",
      "loss at step 26300: 2.8429422616958617\n",
      "loss at step 26320: 2.818617594242096\n",
      "loss at step 26340: 2.8472405910491942\n",
      "loss at step 26360: 2.8412522792816164\n",
      "loss at step 26380: 2.875753939151764\n",
      "loss at step 26400: 2.889319324493408\n",
      "validation loss: 2.9810888401667275\n",
      "validation accuracy: 12.426961602671119%\n",
      "loss at step 26420: 2.8629205226898193\n",
      "loss at step 26440: 2.7848132133483885\n",
      "loss at step 26460: 2.9096227526664733\n",
      "loss at step 26480: 2.8360856890678408\n",
      "loss at step 26500: 2.903282308578491\n",
      "loss at step 26520: 2.8008373498916628\n",
      "loss at step 26540: 2.868136537075043\n",
      "loss at step 26560: 2.81579749584198\n",
      "loss at step 26580: 2.8575900197029114\n",
      "loss at step 26600: 2.806429624557495\n",
      "loss at step 26620: 2.851344406604767\n",
      "loss at step 26640: 2.820547676086426\n",
      "loss at step 26660: 2.88836590051651\n",
      "loss at step 26680: 2.8120090126991273\n",
      "loss at step 26700: 2.8665181517601015\n",
      "loss at step 26720: 2.833767604827881\n",
      "loss at step 26740: 2.8434191703796388\n",
      "loss at step 26760: 2.8241394758224487\n",
      "loss at step 26780: 2.7953705310821535\n",
      "loss at step 26800: 2.8330217480659483\n",
      "validation loss: 2.977968002955119\n",
      "validation accuracy: 12.979966611018364%\n",
      "loss at step 26820: 2.803687024116516\n",
      "loss at step 26840: 2.8454062700271607\n",
      "loss at step 26860: 2.8620131492614744\n",
      "loss at step 26880: 2.851109063625336\n",
      "loss at step 26900: 2.8894042134284974\n",
      "loss at step 26920: 2.8818573236465452\n",
      "loss at step 26940: 2.871258854866028\n",
      "loss at step 26960: 2.8814584970474244\n",
      "loss at step 26980: 2.8576295137405396\n",
      "loss at step 27000: 2.8957109332084654\n",
      "loss at step 27020: 2.9020522475242614\n",
      "loss at step 27040: 2.871175241470337\n",
      "loss at step 27060: 2.857196938991547\n",
      "loss at step 27080: 2.8318338274955748\n",
      "loss at step 27100: 2.853971040248871\n",
      "loss at step 27120: 2.8346689462661745\n",
      "loss at step 27140: 2.846944606304169\n",
      "loss at step 27160: 2.8224495530128477\n",
      "loss at step 27180: 2.834078311920166\n",
      "loss at step 27200: 2.8838876843452455\n",
      "validation loss: 2.971579319636027\n",
      "validation accuracy: 12.912145242070117%\n",
      "loss at step 27220: 2.861103928089142\n",
      "loss at step 27240: 2.8483764290809632\n",
      "loss at step 27260: 2.821215581893921\n",
      "loss at step 27280: 2.819844734668732\n",
      "loss at step 27300: 2.8086791396141053\n",
      "loss at step 27320: 2.9045735478401182\n",
      "loss at step 27340: 2.887014627456665\n",
      "loss at step 27360: 2.8033742666244508\n",
      "loss at step 27380: 2.849716770648956\n",
      "loss at step 27400: 2.837488520145416\n",
      "loss at step 27420: 2.8550192594528196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at step 27440: 2.745735466480255\n",
      "loss at step 27460: 2.828411340713501\n",
      "loss at step 27480: 2.890606963634491\n",
      "loss at step 27500: 2.8152225971221925\n",
      "loss at step 27520: 2.892170810699463\n",
      "loss at step 27540: 2.7564960598945616\n",
      "loss at step 27560: 2.8761111736297607\n",
      "loss at step 27580: 2.878416621685028\n",
      "loss at step 27600: 2.875919210910797\n",
      "validation loss: 2.9683462317784626\n",
      "validation accuracy: 12.979966611018364%\n",
      "loss at step 27620: 2.8221802949905395\n",
      "loss at step 27640: 2.8912249207496643\n",
      "loss at step 27660: 2.860300874710083\n",
      "loss at step 27680: 2.9220999479293823\n",
      "loss at step 27700: 2.8966709852218626\n",
      "loss at step 27720: 2.7727614998817445\n",
      "loss at step 27740: 2.8241966009140014\n",
      "loss at step 27760: 2.865434455871582\n",
      "loss at step 27780: 2.798726940155029\n",
      "loss at step 27800: 2.8246052503585815\n",
      "loss at step 27820: 2.846036970615387\n",
      "loss at step 27840: 2.8893505573272704\n",
      "loss at step 27860: 2.8836658239364623\n",
      "loss at step 27880: 2.7954781532287596\n",
      "loss at step 27900: 2.8193070888519287\n",
      "loss at step 27920: 2.8504105687141417\n",
      "loss at step 27940: 2.8456510424613954\n",
      "loss at step 27960: 2.9335831999778748\n",
      "loss at step 27980: 2.859381318092346\n",
      "loss at step 28000: 2.975042188167572\n",
      "validation loss: 2.967962835629781\n",
      "validation accuracy: 12.588689482470786%\n",
      "loss at step 28020: 2.8054035663604737\n",
      "loss at step 28040: 2.839727544784546\n",
      "loss at step 28060: 2.9288350224494932\n",
      "loss at step 28080: 2.771114730834961\n",
      "loss at step 28100: 2.779938018321991\n",
      "loss at step 28120: 2.787813723087311\n",
      "loss at step 28140: 2.78841747045517\n",
      "loss at step 28160: 2.83948267698288\n",
      "loss at step 28180: 2.8385729670524595\n",
      "loss at step 28200: 2.832982432842255\n",
      "loss at step 28220: 2.817317986488342\n",
      "loss at step 28240: 2.7875383257865907\n",
      "loss at step 28260: 2.7754212617874146\n",
      "loss at step 28280: 2.8411313772201536\n",
      "loss at step 28300: 2.8691636085510255\n",
      "loss at step 28320: 2.8977516055107118\n",
      "loss at step 28340: 2.7987769722938536\n",
      "loss at step 28360: 2.9134788513183594\n",
      "loss at step 28380: 2.768640089035034\n",
      "loss at step 28400: 2.772828555107117\n",
      "validation loss: 2.964520508448283\n",
      "validation accuracy: 13.261686143572621%\n",
      "loss at step 28420: 2.8121803522109987\n",
      "loss at step 28440: 2.869479787349701\n",
      "loss at step 28460: 2.8279497385025025\n",
      "loss at step 28480: 2.8265193581581114\n",
      "loss at step 28500: 2.8519673943519592\n",
      "loss at step 28520: 2.9056540369987487\n",
      "loss at step 28540: 2.763331043720245\n",
      "loss at step 28560: 2.924950897693634\n",
      "loss at step 28580: 2.806751573085785\n",
      "loss at step 28600: 2.875402843952179\n",
      "loss at step 28620: 2.8152492523193358\n",
      "loss at step 28640: 2.8294106245040895\n",
      "loss at step 28660: 2.793735957145691\n",
      "loss at step 28680: 2.91768798828125\n",
      "loss at step 28700: 2.8264339208602904\n",
      "loss at step 28720: 2.913674795627594\n",
      "loss at step 28740: 2.859982764720917\n",
      "loss at step 28760: 2.843297028541565\n",
      "loss at step 28780: 2.948296809196472\n",
      "loss at step 28800: 2.7991681337356566\n",
      "validation loss: 2.962845958073934\n",
      "validation accuracy: 13.04778797996661%\n",
      "loss at step 28820: 2.9158005595207213\n",
      "loss at step 28840: 2.8974161744117737\n",
      "loss at step 28860: 2.8488062977790833\n",
      "loss at step 28880: 2.8950737357139587\n",
      "loss at step 28900: 2.813810932636261\n",
      "loss at step 28920: 2.8175389409065246\n",
      "loss at step 28940: 2.795825111865997\n",
      "loss at step 28960: 2.884338545799255\n",
      "loss at step 28980: 2.8859092354774476\n",
      "loss at step 29000: 2.8207480192184446\n",
      "loss at step 29020: 2.9183755993843077\n",
      "loss at step 29040: 2.8620801091194155\n",
      "loss at step 29060: 2.852726471424103\n",
      "loss at step 29080: 2.8204904556274415\n",
      "loss at step 29100: 2.84353905916214\n",
      "loss at step 29120: 2.7772138953208922\n",
      "loss at step 29140: 2.8047885656356812\n",
      "loss at step 29160: 2.833134400844574\n",
      "loss at step 29180: 2.8564974069595337\n",
      "loss at step 29200: 2.8825979232788086\n",
      "validation loss: 2.9704903888702394\n",
      "validation accuracy: 12.901711185308848%\n",
      "loss at step 29220: 2.8771783828735353\n",
      "loss at step 29240: 2.88761385679245\n",
      "loss at step 29260: 2.876002609729767\n",
      "loss at step 29280: 2.876560938358307\n",
      "loss at step 29300: 2.864274263381958\n",
      "loss at step 29320: 2.8336175680160522\n",
      "loss at step 29340: 2.9042415499687193\n",
      "loss at step 29360: 2.8192825436592104\n",
      "loss at step 29380: 2.886774492263794\n",
      "loss at step 29400: 2.834162104129791\n",
      "loss at step 29420: 2.8198558688163757\n",
      "loss at step 29440: 2.8375663638114927\n",
      "loss at step 29460: 2.8118449211120606\n",
      "loss at step 29480: 2.842461049556732\n",
      "loss at step 29500: 2.8421942710876467\n",
      "loss at step 29520: 2.852812147140503\n",
      "loss at step 29540: 2.895219826698303\n",
      "loss at step 29560: 2.799299430847168\n",
      "loss at step 29580: 2.8691686630249023\n",
      "loss at step 29600: 2.8149482011795044\n",
      "validation loss: 2.934597905476888\n",
      "validation accuracy: 13.141694490818029%\n",
      "loss at step 29620: 2.8234921216964723\n",
      "loss at step 29640: 2.8579843759536745\n",
      "loss at step 29660: 2.8891067266464234\n",
      "loss at step 29680: 2.8993116855621337\n",
      "loss at step 29700: 2.8973423838615417\n",
      "loss at step 29720: 2.8777571320533752\n",
      "loss at step 29740: 2.808731186389923\n",
      "loss at step 29760: 2.7353034377098084\n",
      "loss at step 29780: 2.8057536482810974\n",
      "loss at step 29800: 2.8816148400306703\n",
      "loss at step 29820: 2.8928192377090456\n",
      "loss at step 29840: 2.8346486926078795\n",
      "loss at step 29860: 2.7450493812561034\n",
      "loss at step 29880: 2.841380548477173\n",
      "loss at step 29900: 2.868822765350342\n",
      "loss at step 29920: 2.820780634880066\n",
      "loss at step 29940: 2.8180040597915648\n",
      "loss at step 29960: 2.874658000469208\n",
      "loss at step 29980: 2.914494526386261\n",
      "loss at step 30000: 2.900452268123627\n",
      "validation loss: 2.9654222106933594\n",
      "validation accuracy: 13.439065108514189%\n",
      "loss at step 30020: 2.8410923004150392\n",
      "loss at step 30040: 2.881714904308319\n",
      "loss at step 30060: 2.9064603805541993\n",
      "loss at step 30080: 2.8319828629493715\n",
      "loss at step 30100: 2.846911871433258\n",
      "loss at step 30120: 2.7657615423202513\n",
      "loss at step 30140: 2.8131433725357056\n",
      "loss at step 30160: 2.857572543621063\n",
      "loss at step 30180: 2.8510347485542296\n",
      "loss at step 30200: 2.7987290143966677\n",
      "loss at step 30220: 2.8005465507507323\n",
      "loss at step 30240: 2.8124282479286196\n",
      "loss at step 30260: 2.7782488465309143\n",
      "loss at step 30280: 2.7882142901420592\n",
      "loss at step 30300: 2.861800515651703\n",
      "loss at step 30320: 2.9353545665740968\n",
      "loss at step 30340: 2.845548939704895\n",
      "loss at step 30360: 2.8847964644432067\n",
      "loss at step 30380: 2.879258370399475\n",
      "loss at step 30400: 2.847217059135437\n",
      "validation loss: 2.947094152768453\n",
      "validation accuracy: 13.439065108514189%\n",
      "loss at step 30420: 2.8170058488845826\n",
      "loss at step 30440: 2.902118647098541\n",
      "loss at step 30460: 2.8017445683479307\n",
      "loss at step 30480: 2.8441978812217714\n",
      "loss at step 30500: 2.755219352245331\n",
      "loss at step 30520: 2.848021423816681\n",
      "loss at step 30540: 2.827933943271637\n",
      "loss at step 30560: 2.902545082569122\n",
      "loss at step 30580: 2.842989981174469\n",
      "loss at step 30600: 2.808939576148987\n",
      "loss at step 30620: 2.8111480832099915\n",
      "loss at step 30640: 2.807530403137207\n",
      "loss at step 30660: 2.833341729640961\n",
      "loss at step 30680: 2.8427897810935976\n",
      "loss at step 30700: 2.920794141292572\n",
      "loss at step 30720: 2.860391819477081\n",
      "loss at step 30740: 2.787750267982483\n",
      "loss at step 30760: 2.8293498039245604\n",
      "loss at step 30780: 2.8953295707702638\n",
      "loss at step 30800: 2.87975035905838\n",
      "validation loss: 2.95007542292277\n",
      "validation accuracy: 12.75041736227045%\n",
      "loss at step 30820: 2.8245620846748354\n",
      "loss at step 30840: 2.858913552761078\n",
      "loss at step 30860: 2.90037807226181\n",
      "loss at step 30880: 2.8360502123832703\n",
      "loss at step 30900: 2.8556703090667725\n",
      "loss at step 30920: 2.929310977458954\n",
      "loss at step 30940: 2.7958918809890747\n",
      "loss at step 30960: 2.8447543382644653\n",
      "loss at step 30980: 2.792671763896942\n",
      "loss at step 31000: 2.876850688457489\n",
      "loss at step 31020: 2.835680103302002\n",
      "loss at step 31040: 2.83817880153656\n",
      "loss at step 31060: 2.806762456893921\n",
      "loss at step 31080: 2.8332836866378783\n",
      "loss at step 31100: 2.7633968949317933\n",
      "loss at step 31120: 2.780623471736908\n",
      "loss at step 31140: 2.8053361415863036\n",
      "loss at step 31160: 2.920182979106903\n",
      "loss at step 31180: 2.8284143567085267\n",
      "loss at step 31200: 2.845449459552765\n",
      "validation loss: 2.9535176118214923\n",
      "validation accuracy: 13.225166944908182%\n",
      "loss at step 31220: 2.7703410267829893\n",
      "loss at step 31240: 2.839515447616577\n",
      "loss at step 31260: 2.848731541633606\n",
      "loss at step 31280: 2.835086190700531\n",
      "loss at step 31300: 2.8230129361152647\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at step 31320: 2.7807181239128114\n",
      "loss at step 31340: 2.81596497297287\n",
      "loss at step 31360: 2.847038757801056\n",
      "loss at step 31380: 2.8042336463928224\n",
      "loss at step 31400: 2.829120969772339\n",
      "loss at step 31420: 2.8319985270500183\n",
      "loss at step 31440: 2.8390973567962647\n",
      "loss at step 31460: 2.816698944568634\n",
      "loss at step 31480: 2.815987539291382\n",
      "loss at step 31500: 2.823652780056\n",
      "loss at step 31520: 2.817155909538269\n",
      "loss at step 31540: 2.8306984424591066\n",
      "loss at step 31560: 2.8561968207359314\n",
      "loss at step 31580: 2.800739860534668\n",
      "loss at step 31600: 2.8324445605278017\n",
      "validation loss: 2.9354896847407024\n",
      "validation accuracy: 13.230383973288815%\n",
      "loss at step 31620: 2.8472894072532653\n",
      "loss at step 31640: 2.7653544545173645\n",
      "loss at step 31660: 2.8218127250671388\n",
      "loss at step 31680: 2.8751335978507995\n",
      "loss at step 31700: 2.8020596861839295\n",
      "loss at step 31720: 2.8508884072303773\n",
      "loss at step 31740: 2.763071596622467\n",
      "loss at step 31760: 2.794885849952698\n",
      "loss at step 31780: 2.8570738434791565\n",
      "loss at step 31800: 2.8560311913490297\n",
      "loss at step 31820: 2.8244245409965516\n",
      "loss at step 31840: 2.7583000540733336\n",
      "loss at step 31860: 2.842803144454956\n",
      "loss at step 31880: 2.8590858459472654\n",
      "loss at step 31900: 2.8722558975219727\n",
      "loss at step 31920: 2.764016342163086\n",
      "loss at step 31940: 2.814509963989258\n",
      "loss at step 31960: 2.813527548313141\n",
      "loss at step 31980: 2.8796040534973146\n",
      "loss at step 32000: 2.8933760643005373\n",
      "validation loss: 2.948165470759074\n",
      "validation accuracy: 13.089524207011685%\n",
      "loss at step 32020: 2.903540813922882\n",
      "loss at step 32040: 2.7939934611320494\n",
      "loss at step 32060: 2.8547792553901674\n",
      "loss at step 32080: 2.868812787532806\n",
      "loss at step 32100: 2.7960958957672117\n",
      "loss at step 32120: 2.8668108701705934\n",
      "loss at step 32140: 2.90383003950119\n",
      "loss at step 32160: 2.8570819735527038\n",
      "loss at step 32180: 2.8624937891960145\n",
      "loss at step 32200: 2.8480215311050414\n",
      "loss at step 32220: 2.7780092120170594\n",
      "loss at step 32240: 2.784600329399109\n",
      "loss at step 32260: 2.8264111280441284\n",
      "loss at step 32280: 2.856631910800934\n",
      "loss at step 32300: 2.8683510184288026\n",
      "loss at step 32320: 2.809276854991913\n",
      "loss at step 32340: 2.9091052293777464\n",
      "loss at step 32360: 2.946108090877533\n",
      "loss at step 32380: 2.8664902567863466\n",
      "loss at step 32400: 2.8468526244163512\n",
      "validation loss: 2.989537819226583\n",
      "validation accuracy: 13.037353923205341%\n",
      "loss at step 32420: 2.909534251689911\n",
      "loss at step 32440: 2.8277647376060484\n",
      "loss at step 32460: 2.9078513741493226\n",
      "loss at step 32480: 2.8656113743782043\n",
      "loss at step 32500: 2.8700862646102907\n",
      "loss at step 32520: 2.8882545590400697\n",
      "loss at step 32540: 2.8440224051475527\n",
      "loss at step 32560: 2.880705440044403\n",
      "loss at step 32580: 2.802139461040497\n",
      "loss at step 32600: 2.891277527809143\n",
      "loss at step 32620: 2.7996335864067077\n",
      "loss at step 32640: 2.769584763050079\n",
      "loss at step 32660: 2.788729751110077\n",
      "loss at step 32680: 2.8000819802284242\n",
      "loss at step 32700: 2.843747115135193\n",
      "loss at step 32720: 2.8161176323890684\n",
      "loss at step 32740: 2.8129101634025573\n",
      "loss at step 32760: 2.7663474678993225\n",
      "loss at step 32780: 2.8131823778152465\n",
      "loss at step 32800: 2.8364741683006285\n",
      "validation loss: 2.935812381108602\n",
      "validation accuracy: 13.845993322203674%\n",
      "loss at step 32820: 2.863910901546478\n",
      "loss at step 32840: 2.8077764987945555\n",
      "loss at step 32860: 2.881135368347168\n",
      "loss at step 32880: 2.8238492250442504\n",
      "loss at step 32900: 2.8155407428741457\n",
      "loss at step 32920: 2.7850602984428408\n",
      "loss at step 32940: 2.7860217452049256\n",
      "loss at step 32960: 2.867558312416077\n",
      "loss at step 32980: 2.7770848870277405\n",
      "loss at step 33000: 2.807036292552948\n",
      "loss at step 33020: 2.784955549240112\n",
      "loss at step 33040: 2.780051863193512\n",
      "loss at step 33060: 2.8102617383003237\n",
      "loss at step 33080: 2.8000499606132507\n",
      "loss at step 33100: 2.8207756996154787\n",
      "loss at step 33120: 2.8722211837768556\n",
      "loss at step 33140: 2.8519245624542235\n",
      "loss at step 33160: 2.833098268508911\n",
      "loss at step 33180: 2.91821311712265\n",
      "loss at step 33200: 2.785655677318573\n",
      "validation loss: 2.9192049884796143\n",
      "validation accuracy: 13.699916527545911%\n",
      "loss at step 33220: 2.845290744304657\n",
      "loss at step 33240: 2.781831240653992\n",
      "loss at step 33260: 2.8294766664505007\n",
      "loss at step 33280: 2.794991683959961\n",
      "loss at step 33300: 2.814101183414459\n",
      "loss at step 33320: 2.860623288154602\n",
      "loss at step 33340: 2.8722015500068663\n",
      "loss at step 33360: 2.8277605772018433\n",
      "loss at step 33380: 2.7968393325805665\n",
      "loss at step 33400: 2.83010196685791\n",
      "loss at step 33420: 2.835499179363251\n",
      "loss at step 33440: 2.8197681665420533\n",
      "loss at step 33460: 2.828639340400696\n",
      "loss at step 33480: 2.8814196944236756\n",
      "loss at step 33500: 2.792905402183533\n",
      "loss at step 33520: 2.80141738653183\n",
      "loss at step 33540: 2.7503849029541017\n",
      "loss at step 33560: 2.786259388923645\n",
      "loss at step 33580: 2.860466647148132\n",
      "loss at step 33600: 2.823819899559021\n",
      "validation loss: 2.92269037882487\n",
      "validation accuracy: 13.647746243739567%\n",
      "loss at step 33620: 2.7054118871688844\n",
      "loss at step 33640: 2.7815421342849733\n",
      "loss at step 33660: 2.8711337804794312\n",
      "loss at step 33680: 2.890604531764984\n",
      "loss at step 33700: 2.8507800936698913\n",
      "loss at step 33720: 2.8710233092308046\n",
      "loss at step 33740: 2.863939666748047\n",
      "loss at step 33760: 2.84711149930954\n",
      "loss at step 33780: 2.8249797105789183\n",
      "loss at step 33800: 2.9067180514335633\n",
      "loss at step 33820: 2.8360848784446717\n",
      "loss at step 33840: 2.811359179019928\n",
      "loss at step 33860: 2.8398184061050413\n",
      "loss at step 33880: 2.8166033148765566\n",
      "loss at step 33900: 2.8471164464950562\n",
      "loss at step 33920: 2.7759578227996826\n",
      "loss at step 33940: 2.8306450724601744\n",
      "loss at step 33960: 2.814753460884094\n",
      "loss at step 33980: 2.822892200946808\n",
      "loss at step 34000: 2.7360890388488768\n",
      "validation loss: 2.9504255930582683\n",
      "validation accuracy: 13.126043405676127%\n",
      "loss at step 34020: 2.7996662855148315\n",
      "loss at step 34040: 2.797634243965149\n",
      "loss at step 34060: 2.7762090802192687\n",
      "loss at step 34080: 2.812237799167633\n",
      "loss at step 34100: 2.7964781165122985\n",
      "loss at step 34120: 2.8373063445091247\n",
      "loss at step 34140: 2.7551193833351135\n",
      "loss at step 34160: 2.9203258514404298\n",
      "loss at step 34180: 2.747414600849152\n",
      "loss at step 34200: 2.806853938102722\n",
      "loss at step 34220: 2.89529435634613\n",
      "loss at step 34240: 2.8046175956726076\n",
      "loss at step 34260: 2.837489974498749\n",
      "loss at step 34280: 2.832826220989227\n",
      "loss at step 34300: 2.8250691294670105\n",
      "loss at step 34320: 2.8392435193061827\n",
      "loss at step 34340: 2.7592678904533385\n",
      "loss at step 34360: 2.8048975467681885\n",
      "loss at step 34380: 2.787924921512604\n",
      "loss at step 34400: 2.8195453524589538\n",
      "validation loss: 2.905380088488261\n",
      "validation accuracy: 13.277337228714526%\n",
      "loss at step 34420: 2.813929498195648\n",
      "loss at step 34440: 2.7612775206565856\n",
      "loss at step 34460: 2.8064847111701967\n",
      "loss at step 34480: 2.8317511081695557\n",
      "loss at step 34500: 2.7926263093948362\n",
      "loss at step 34520: 2.779988956451416\n",
      "loss at step 34540: 2.893856370449066\n",
      "loss at step 34560: 2.8217289805412293\n",
      "loss at step 34580: 2.827366089820862\n",
      "loss at step 34600: 2.7898321390151977\n",
      "loss at step 34620: 2.8075590252876284\n",
      "loss at step 34640: 2.7932614922523498\n",
      "loss at step 34660: 2.867393267154694\n",
      "loss at step 34680: 2.8730411529541016\n",
      "loss at step 34700: 2.7221962928771974\n",
      "loss at step 34720: 2.811091661453247\n",
      "loss at step 34740: 2.8088879585266113\n",
      "loss at step 34760: 2.8601097106933593\n",
      "loss at step 34780: 2.806161117553711\n",
      "loss at step 34800: 2.726579964160919\n",
      "validation loss: 2.9073484706878663\n",
      "validation accuracy: 13.527754590984975%\n",
      "loss at step 34820: 2.849658966064453\n",
      "loss at step 34840: 2.8011678099632262\n",
      "loss at step 34860: 2.7798834919929503\n",
      "loss at step 34880: 2.820999014377594\n",
      "loss at step 34900: 2.796790027618408\n",
      "loss at step 34920: 2.8203405857086183\n",
      "loss at step 34940: 2.817006731033325\n",
      "loss at step 34960: 2.8167620420455934\n",
      "loss at step 34980: 2.8306902050971985\n",
      "loss at step 35000: 2.8796204209327696\n",
      "loss at step 35020: 2.856365168094635\n",
      "loss at step 35040: 2.7695738554000853\n",
      "loss at step 35060: 2.848935437202454\n",
      "loss at step 35080: 2.8018367648124696\n",
      "loss at step 35100: 2.825907123088837\n",
      "loss at step 35120: 2.826631796360016\n",
      "loss at step 35140: 2.8338955283164977\n",
      "loss at step 35160: 2.789944899082184\n",
      "loss at step 35180: 2.7885092973709105\n",
      "loss at step 35200: 2.841697895526886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 2.918790752092997\n",
      "validation accuracy: 13.52253756260434%\n",
      "loss at step 35220: 2.8136191725730897\n",
      "loss at step 35240: 2.823182666301727\n",
      "loss at step 35260: 2.8808452129364013\n",
      "loss at step 35280: 2.843585479259491\n",
      "loss at step 35300: 2.77946195602417\n",
      "loss at step 35320: 2.8024762868881226\n",
      "loss at step 35340: 2.8228626608848573\n",
      "loss at step 35360: 2.8412224531173704\n",
      "loss at step 35380: 2.7441245317459106\n",
      "loss at step 35400: 2.764326107501984\n",
      "loss at step 35420: 2.8242959499359133\n",
      "loss at step 35440: 2.83495637178421\n",
      "loss at step 35460: 2.8213343739509584\n",
      "loss at step 35480: 2.8211952209472657\n",
      "loss at step 35500: 2.7946226358413697\n",
      "loss at step 35520: 2.7875479102134704\n",
      "loss at step 35540: 2.772036814689636\n",
      "loss at step 35560: 2.8317281007766724\n",
      "loss at step 35580: 2.7961588978767393\n",
      "loss at step 35600: 2.8505170941352844\n",
      "validation loss: 2.9317983134587604\n",
      "validation accuracy: 12.865191986644408%\n",
      "loss at step 35620: 2.766214406490326\n",
      "loss at step 35640: 2.7851132988929748\n",
      "loss at step 35660: 2.8099019885063172\n",
      "loss at step 35680: 2.8555629372596742\n",
      "loss at step 35700: 2.8440656065940857\n",
      "loss at step 35720: 2.7738700032234194\n",
      "loss at step 35740: 2.8278796911239623\n",
      "loss at step 35760: 2.7921660423278807\n",
      "loss at step 35780: 2.829698371887207\n",
      "loss at step 35800: 2.8989986300468447\n",
      "loss at step 35820: 2.764322519302368\n",
      "loss at step 35840: 2.819561815261841\n",
      "loss at step 35860: 2.816970646381378\n",
      "loss at step 35880: 2.8887845396995546\n",
      "loss at step 35900: 2.8504819869995117\n",
      "loss at step 35920: 2.691693913936615\n",
      "loss at step 35940: 2.8334365487098694\n",
      "loss at step 35960: 2.838483119010925\n",
      "loss at step 35980: 2.8152342438697815\n",
      "loss at step 36000: 2.7942973971366882\n",
      "validation loss: 2.915566441218058\n",
      "validation accuracy: 13.397328881469114%\n",
      "loss at step 36020: 2.866860795021057\n",
      "loss at step 36040: 2.759813165664673\n",
      "loss at step 36060: 2.7142922401428224\n",
      "loss at step 36080: 2.8590109825134276\n",
      "loss at step 36100: 2.7446200132369993\n",
      "loss at step 36120: 2.7596586108207704\n",
      "loss at step 36140: 2.9028099536895753\n",
      "loss at step 36160: 2.732184815406799\n",
      "loss at step 36180: 2.75655198097229\n",
      "loss at step 36200: 2.778393578529358\n",
      "loss at step 36220: 2.7991826057434084\n",
      "loss at step 36240: 2.769365978240967\n",
      "loss at step 36260: 2.859948682785034\n",
      "loss at step 36280: 2.8884327888488768\n",
      "loss at step 36300: 2.8194420099258424\n",
      "loss at step 36320: 2.7439436674118043\n",
      "loss at step 36340: 2.791734850406647\n",
      "loss at step 36360: 2.775791311264038\n",
      "loss at step 36380: 2.7543214082717897\n",
      "loss at step 36400: 2.78936048746109\n",
      "validation loss: 2.901971712112427\n",
      "validation accuracy: 13.35559265442404%\n",
      "loss at step 36420: 2.8437304973602293\n",
      "loss at step 36440: 2.8350667476654055\n",
      "loss at step 36460: 2.875914251804352\n",
      "loss at step 36480: 2.7888877391815186\n",
      "loss at step 36500: 2.820209527015686\n",
      "loss at step 36520: 2.801514375209808\n",
      "loss at step 36540: 2.8395761609077455\n",
      "loss at step 36560: 2.7986377477645874\n",
      "loss at step 36580: 2.8167280077934267\n",
      "loss at step 36600: 2.8954747676849366\n",
      "loss at step 36620: 2.7953848838806152\n",
      "loss at step 36640: 2.748802995681763\n",
      "loss at step 36660: 2.787097144126892\n",
      "loss at step 36680: 2.771199083328247\n",
      "loss at step 36700: 2.819511353969574\n",
      "loss at step 36720: 2.800624907016754\n",
      "loss at step 36740: 2.8098390102386475\n",
      "loss at step 36760: 2.7669668197631836\n",
      "loss at step 36780: 2.7995160937309267\n",
      "loss at step 36800: 2.776149106025696\n",
      "validation loss: 2.8984824403127036\n",
      "validation accuracy: 13.799040066777962%\n",
      "loss at step 36820: 2.65905476808548\n",
      "loss at step 36840: 2.768406641483307\n",
      "loss at step 36860: 2.7914835453033446\n",
      "loss at step 36880: 2.7776384234428404\n",
      "loss at step 36900: 2.8369164943695067\n",
      "loss at step 36920: 2.781755518913269\n",
      "loss at step 36940: 2.857702136039734\n",
      "loss at step 36960: 2.867067873477936\n",
      "loss at step 36980: 2.826666867733002\n",
      "loss at step 37000: 2.843278968334198\n",
      "loss at step 37020: 2.794163739681244\n",
      "loss at step 37040: 2.7808128595352173\n",
      "loss at step 37060: 2.766034555435181\n",
      "loss at step 37080: 2.793803834915161\n",
      "loss at step 37100: 2.808173131942749\n",
      "loss at step 37120: 2.8773953318595886\n",
      "loss at step 37140: 2.788562548160553\n",
      "loss at step 37160: 2.7904021739959717\n",
      "loss at step 37180: 2.7449700832366943\n",
      "loss at step 37200: 2.726993238925934\n",
      "validation loss: 2.9014326151212058\n",
      "validation accuracy: 13.52253756260434%\n",
      "loss at step 37220: 2.8565068364143373\n",
      "loss at step 37240: 2.8369304299354554\n",
      "loss at step 37260: 2.884550952911377\n",
      "loss at step 37280: 2.7399781703948975\n",
      "loss at step 37300: 2.788235294818878\n",
      "loss at step 37320: 2.887946331501007\n",
      "loss at step 37340: 2.789211356639862\n",
      "loss at step 37360: 2.8173617362976073\n",
      "loss at step 37380: 2.76376736164093\n",
      "loss at step 37400: 2.7948402166366577\n",
      "loss at step 37420: 2.778011107444763\n",
      "loss at step 37440: 2.801394283771515\n",
      "loss at step 37460: 2.7710245490074157\n",
      "loss at step 37480: 2.7920020461082458\n",
      "loss at step 37500: 2.8474446296691895\n",
      "loss at step 37520: 2.8353177070617677\n",
      "loss at step 37540: 2.844773459434509\n",
      "loss at step 37560: 2.7480339884757994\n",
      "loss at step 37580: 2.8164258718490602\n",
      "loss at step 37600: 2.7832987546920775\n",
      "validation loss: 2.906176341374715\n",
      "validation accuracy: 13.611227045075125%\n",
      "loss at step 37620: 2.8646389484405517\n",
      "loss at step 37640: 2.8651529431343077\n",
      "loss at step 37660: 2.8275493383407593\n",
      "loss at step 37680: 2.7941561222076414\n",
      "loss at step 37700: 2.741316223144531\n",
      "loss at step 37720: 2.7211544632911684\n",
      "loss at step 37740: 2.768894648551941\n",
      "loss at step 37760: 2.797385537624359\n",
      "loss at step 37780: 2.8067394852638246\n",
      "loss at step 37800: 2.842738115787506\n",
      "loss at step 37820: 2.838883471488953\n",
      "loss at step 37840: 2.7936787605285645\n",
      "loss at step 37860: 2.70978821516037\n",
      "loss at step 37880: 2.8554656147956847\n",
      "loss at step 37900: 2.78981237411499\n",
      "loss at step 37920: 2.8318502068519593\n",
      "loss at step 37940: 2.8090071082115173\n",
      "loss at step 37960: 2.8780574202537537\n",
      "loss at step 37980: 2.810128629207611\n",
      "loss at step 38000: 2.8132718443870544\n",
      "validation loss: 2.906073408126831\n",
      "validation accuracy: 13.6529632721202%\n",
      "loss at step 38020: 2.765311396121979\n",
      "loss at step 38040: 2.7994969129562377\n",
      "loss at step 38060: 2.8430983662605285\n",
      "loss at step 38080: 2.798346424102783\n",
      "loss at step 38100: 2.7259485602378843\n",
      "loss at step 38120: 2.8496255397796633\n",
      "loss at step 38140: 2.747301971912384\n",
      "loss at step 38160: 2.824675214290619\n",
      "loss at step 38180: 2.676234245300293\n",
      "loss at step 38200: 2.8435551643371584\n",
      "loss at step 38220: 2.78302184343338\n",
      "loss at step 38240: 2.725577104091644\n",
      "loss at step 38260: 2.7800572991371153\n",
      "loss at step 38280: 2.8118470072746278\n",
      "loss at step 38300: 2.8367587685585023\n",
      "loss at step 38320: 2.7844582915306093\n",
      "loss at step 38340: 2.833399498462677\n",
      "loss at step 38360: 2.7808485984802247\n",
      "loss at step 38380: 2.7138169527053835\n",
      "loss at step 38400: 2.8828911185264587\n",
      "validation loss: 2.883380338350932\n",
      "validation accuracy: 14.049457429048415%\n",
      "loss at step 38420: 2.8526877880096437\n",
      "loss at step 38440: 2.833022749423981\n",
      "loss at step 38460: 2.8142883658409117\n",
      "loss at step 38480: 2.859091877937317\n",
      "loss at step 38500: 2.828117001056671\n",
      "loss at step 38520: 2.730487561225891\n",
      "loss at step 38540: 2.8476539731025694\n",
      "loss at step 38560: 2.765672242641449\n",
      "loss at step 38580: 2.8166158556938172\n",
      "loss at step 38600: 2.7540412664413454\n",
      "loss at step 38620: 2.7398369550704955\n",
      "loss at step 38640: 2.7334428668022155\n",
      "loss at step 38660: 2.712793004512787\n",
      "loss at step 38680: 2.750033235549927\n",
      "loss at step 38700: 2.8303382635116576\n",
      "loss at step 38720: 2.8623620867729187\n",
      "loss at step 38740: 2.8220635890960692\n",
      "loss at step 38760: 2.72975606918335\n",
      "loss at step 38780: 2.752265226840973\n",
      "loss at step 38800: 2.734119713306427\n",
      "validation loss: 2.912506466706594\n",
      "validation accuracy: 13.752086811352255%\n",
      "loss at step 38820: 2.9518901109695435\n",
      "loss at step 38840: 2.787737238407135\n",
      "loss at step 38860: 2.815790319442749\n",
      "loss at step 38880: 2.7949498295783997\n",
      "loss at step 38900: 2.7449607133865355\n",
      "loss at step 38920: 2.7513585567474363\n",
      "loss at step 38940: 2.813339877128601\n",
      "loss at step 38960: 2.8891404271125793\n",
      "loss at step 38980: 2.7294395089149477\n",
      "loss at step 39000: 2.764479410648346\n",
      "loss at step 39020: 2.778737819194794\n",
      "loss at step 39040: 2.8387611746788024\n",
      "loss at step 39060: 2.7890095472335816\n",
      "loss at step 39080: 2.8521576881408692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at step 39100: 2.792754077911377\n",
      "loss at step 39120: 2.8294769644737245\n",
      "loss at step 39140: 2.7910500168800354\n",
      "loss at step 39160: 2.8229071259498597\n",
      "loss at step 39180: 2.770267367362976\n",
      "loss at step 39200: 2.7944493889808655\n",
      "validation loss: 2.894195751349131\n",
      "validation accuracy: 13.606010016694492%\n",
      "loss at step 39220: 2.7460877299308777\n",
      "loss at step 39240: 2.7500099539756775\n",
      "loss at step 39260: 2.84627822637558\n",
      "loss at step 39280: 2.776406693458557\n",
      "loss at step 39300: 2.8199816942214966\n",
      "loss at step 39320: 2.753622353076935\n",
      "loss at step 39340: 2.8161106944084167\n",
      "loss at step 39360: 2.7645013093948365\n",
      "loss at step 39380: 2.821846973896027\n",
      "loss at step 39400: 2.8638893485069277\n",
      "loss at step 39420: 2.7836749792099\n",
      "loss at step 39440: 2.805334508419037\n",
      "loss at step 39460: 2.7995888590812683\n",
      "loss at step 39480: 2.858773875236511\n",
      "loss at step 39500: 2.7547016620635985\n",
      "loss at step 39520: 2.7877488493919373\n",
      "loss at step 39540: 2.7915931820869444\n",
      "loss at step 39560: 2.8331016659736634\n",
      "loss at step 39580: 2.856508660316467\n",
      "loss at step 39600: 2.716137444972992\n",
      "validation loss: 2.880907828013102\n",
      "validation accuracy: 13.360809682804675%\n",
      "loss at step 39620: 2.837581694126129\n",
      "loss at step 39640: 2.791542339324951\n",
      "loss at step 39660: 2.874739396572113\n",
      "loss at step 39680: 2.8177699208259583\n",
      "loss at step 39700: 2.7553943276405333\n",
      "loss at step 39720: 2.796548295021057\n",
      "loss at step 39740: 2.813895046710968\n",
      "loss at step 39760: 2.81617192029953\n",
      "loss at step 39780: 2.8340757131576537\n",
      "loss at step 39800: 2.786025619506836\n",
      "loss at step 39820: 2.7984322786331175\n",
      "loss at step 39840: 2.850106585025787\n",
      "loss at step 39860: 2.810649037361145\n",
      "loss at step 39880: 2.8265325307846068\n",
      "loss at step 39900: 2.8032088041305543\n",
      "loss at step 39920: 2.8149468898773193\n",
      "loss at step 39940: 2.8374378204345705\n",
      "loss at step 39960: 2.8255693912506104\n",
      "loss at step 39980: 2.786674606800079\n",
      "loss at step 40000: 2.7864654898643493\n",
      "validation loss: 2.8966712888081867\n",
      "validation accuracy: 13.699916527545911%\n",
      "loss at step 40020: 2.8366285800933837\n",
      "loss at step 40040: 2.7435355186462402\n",
      "loss at step 40060: 2.7812259435653686\n",
      "loss at step 40080: 2.7803521752357483\n",
      "loss at step 40100: 2.8212536334991456\n",
      "loss at step 40120: 2.721780014038086\n",
      "loss at step 40140: 2.779028058052063\n",
      "loss at step 40160: 2.7951438426971436\n",
      "loss at step 40180: 2.821586275100708\n",
      "loss at step 40200: 2.856786572933197\n",
      "loss at step 40220: 2.7496399521827697\n",
      "loss at step 40240: 2.7712332010269165\n",
      "loss at step 40260: 2.7749068260192873\n",
      "loss at step 40280: 2.810803496837616\n",
      "loss at step 40300: 2.7264131784439085\n",
      "loss at step 40320: 2.76151282787323\n",
      "loss at step 40340: 2.796699559688568\n",
      "loss at step 40360: 2.745075857639313\n",
      "loss at step 40380: 2.7979621410369875\n",
      "loss at step 40400: 2.8171539664268495\n",
      "validation loss: 2.9092812665303547\n",
      "validation accuracy: 13.694699499165274%\n",
      "loss at step 40420: 2.7725005865097048\n",
      "loss at step 40440: 2.7632122159004213\n",
      "loss at step 40460: 2.8150081515312193\n",
      "loss at step 40480: 2.747640573978424\n",
      "loss at step 40500: 2.756907081604004\n",
      "loss at step 40520: 2.8452075242996218\n",
      "loss at step 40540: 2.7804348826408387\n",
      "loss at step 40560: 2.752664566040039\n",
      "loss at step 40580: 2.856603002548218\n",
      "loss at step 40600: 2.8562361121177675\n",
      "loss at step 40620: 2.800781953334808\n",
      "loss at step 40640: 2.750618207454681\n",
      "loss at step 40660: 2.770201528072357\n",
      "loss at step 40680: 2.8427807211875917\n",
      "loss at step 40700: 2.820896363258362\n",
      "loss at step 40720: 2.7456715941429137\n",
      "loss at step 40740: 2.848632645606995\n",
      "loss at step 40760: 2.7378374338150024\n",
      "loss at step 40780: 2.868905079364777\n",
      "loss at step 40800: 2.77918746471405\n",
      "validation loss: 2.8747754208246867\n",
      "validation accuracy: 13.908597662771285%\n",
      "loss at step 40820: 2.785219395160675\n",
      "loss at step 40840: 2.8458165407180784\n",
      "loss at step 40860: 2.7309582114219664\n",
      "loss at step 40880: 2.7502424836158754\n",
      "loss at step 40900: 2.810810148715973\n",
      "loss at step 40920: 2.817602574825287\n",
      "loss at step 40940: 2.8180528044700623\n",
      "loss at step 40960: 2.7574858903884887\n",
      "loss at step 40980: 2.850912880897522\n",
      "loss at step 41000: 2.777847635746002\n",
      "loss at step 41020: 2.70718138217926\n",
      "loss at step 41040: 2.755515968799591\n",
      "loss at step 41060: 2.824169671535492\n",
      "loss at step 41080: 2.7774079203605653\n",
      "loss at step 41100: 2.793863022327423\n",
      "loss at step 41120: 2.863669741153717\n",
      "loss at step 41140: 2.744653034210205\n",
      "loss at step 41160: 2.722300535440445\n",
      "loss at step 41180: 2.7902029037475584\n",
      "loss at step 41200: 2.7254810452461244\n",
      "validation loss: 2.900148804982503\n",
      "validation accuracy: 14.101627712854759%\n",
      "loss at step 41220: 2.8312491297721865\n",
      "loss at step 41240: 2.8767802596092222\n",
      "loss at step 41260: 2.6757198572158813\n",
      "loss at step 41280: 2.771789538860321\n",
      "loss at step 41300: 2.8303640604019167\n",
      "loss at step 41320: 2.8482279896736147\n",
      "loss at step 41340: 2.7941380381584167\n",
      "loss at step 41360: 2.827663171291351\n",
      "loss at step 41380: 2.871694266796112\n",
      "loss at step 41400: 2.8083552360534667\n",
      "loss at step 41420: 2.838392972946167\n",
      "loss at step 41440: 2.7725269079208372\n",
      "loss at step 41460: 2.761961209774017\n",
      "loss at step 41480: 2.771533250808716\n",
      "loss at step 41500: 2.8664182662963866\n",
      "loss at step 41520: 2.745936381816864\n",
      "loss at step 41540: 2.7353869915008544\n",
      "loss at step 41560: 2.7880900740623473\n",
      "loss at step 41580: 2.735729765892029\n",
      "loss at step 41600: 2.849487233161926\n",
      "validation loss: 2.865601898829142\n",
      "validation accuracy: 14.237270450751252%\n",
      "loss at step 41620: 2.769054186344147\n",
      "loss at step 41640: 2.8277177691459654\n",
      "loss at step 41660: 2.730544626712799\n",
      "loss at step 41680: 2.7831573724746703\n",
      "loss at step 41700: 2.804519736766815\n",
      "loss at step 41720: 2.804814171791077\n",
      "loss at step 41740: 2.7597983717918395\n",
      "loss at step 41760: 2.699211084842682\n",
      "loss at step 41780: 2.7329273223876953\n",
      "loss at step 41800: 2.820021092891693\n",
      "loss at step 41820: 2.8027798175811767\n",
      "loss at step 41840: 2.75993674993515\n",
      "loss at step 41860: 2.6881998777389526\n",
      "loss at step 41880: 2.7206865429878233\n",
      "loss at step 41900: 2.868321216106415\n",
      "loss at step 41920: 2.8134297132492065\n",
      "loss at step 41940: 2.732594919204712\n",
      "loss at step 41960: 2.800406503677368\n",
      "loss at step 41980: 2.809124970436096\n",
      "loss at step 42000: 2.8081000804901124\n",
      "validation loss: 2.9355440664291383\n",
      "validation accuracy: 13.491235392320533%\n",
      "loss at step 42020: 2.7479094862937927\n",
      "loss at step 42040: 2.838211143016815\n",
      "loss at step 42060: 2.8095155119895936\n",
      "loss at step 42080: 2.7142656445503235\n",
      "loss at step 42100: 2.758339262008667\n",
      "loss at step 42120: 2.7738242983818053\n",
      "loss at step 42140: 2.757142174243927\n",
      "loss at step 42160: 2.7815675735473633\n",
      "loss at step 42180: 2.827368986606598\n",
      "loss at step 42200: 2.8148205161094664\n",
      "loss at step 42220: 2.7272272944450378\n",
      "loss at step 42240: 2.6973580002784727\n",
      "loss at step 42260: 2.8278972268104554\n",
      "loss at step 42280: 2.6900145053863525\n",
      "loss at step 42300: 2.7669144749641417\n",
      "loss at step 42320: 2.8095862865448\n",
      "loss at step 42340: 2.7721270322799683\n",
      "loss at step 42360: 2.808023011684418\n",
      "loss at step 42380: 2.7898589372634888\n",
      "loss at step 42400: 2.7479979395866394\n",
      "validation loss: 2.8759970633188883\n",
      "validation accuracy: 13.8199081803005%\n",
      "loss at step 42420: 2.766593837738037\n",
      "loss at step 42440: 2.8056935548782347\n",
      "loss at step 42460: 2.799196887016296\n",
      "loss at step 42480: 2.824553906917572\n",
      "loss at step 42500: 2.751826786994934\n",
      "loss at step 42520: 2.765838289260864\n",
      "loss at step 42540: 2.806849408149719\n",
      "loss at step 42560: 2.774501848220825\n",
      "loss at step 42580: 2.80801762342453\n",
      "loss at step 42600: 2.851709544658661\n",
      "loss at step 42620: 2.8207913994789124\n",
      "loss at step 42640: 2.7989020347595215\n",
      "loss at step 42660: 2.771484136581421\n",
      "loss at step 42680: 2.832529354095459\n",
      "loss at step 42700: 2.780144762992859\n",
      "loss at step 42720: 2.7849105477333067\n",
      "loss at step 42740: 2.708186113834381\n",
      "loss at step 42760: 2.7253811955451965\n",
      "loss at step 42780: 2.7769650340080263\n",
      "loss at step 42800: 2.7999433517456054\n",
      "validation loss: 2.8773648675282795\n",
      "validation accuracy: 13.726001669449081%\n",
      "loss at step 42820: 2.780239522457123\n",
      "loss at step 42840: 2.7906264543533323\n",
      "loss at step 42860: 2.7788193941116335\n",
      "loss at step 42880: 2.715172803401947\n",
      "loss at step 42900: 2.830822837352753\n",
      "loss at step 42920: 2.777995431423187\n",
      "loss at step 42940: 2.768094563484192\n",
      "loss at step 42960: 2.764871907234192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at step 42980: 2.8076645851135256\n",
      "loss at step 43000: 2.7098663091659545\n",
      "loss at step 43020: 2.7693687319755553\n",
      "loss at step 43040: 2.7816125750541687\n",
      "loss at step 43060: 2.7404900550842286\n",
      "loss at step 43080: 2.7885075330734255\n",
      "loss at step 43100: 2.686580407619476\n",
      "loss at step 43120: 2.8048353791236877\n",
      "loss at step 43140: 2.7701401472091676\n",
      "loss at step 43160: 2.8033422350883486\n",
      "loss at step 43180: 2.8665018677711487\n",
      "loss at step 43200: 2.7691388010978697\n",
      "validation loss: 2.8685682741800944\n",
      "validation accuracy: 13.757303839732888%\n",
      "loss at step 43220: 2.8172378301620484\n",
      "loss at step 43240: 2.795344150066376\n",
      "loss at step 43260: 2.792289471626282\n",
      "loss at step 43280: 2.8147246122360228\n",
      "loss at step 43300: 2.7371333718299864\n",
      "loss at step 43320: 2.7172937631607055\n",
      "loss at step 43340: 2.755329728126526\n",
      "loss at step 43360: 2.7495023727416994\n",
      "loss at step 43380: 2.765028154850006\n",
      "loss at step 43400: 2.826498103141785\n",
      "loss at step 43420: 2.7623222947120665\n",
      "loss at step 43440: 2.8129791378974915\n",
      "loss at step 43460: 2.7647812604904174\n",
      "loss at step 43480: 2.7782600283622743\n",
      "loss at step 43500: 2.8281903743743895\n",
      "loss at step 43520: 2.7622314214706423\n",
      "loss at step 43540: 2.723651885986328\n",
      "loss at step 43560: 2.7583571672439575\n",
      "loss at step 43580: 2.796031856536865\n",
      "loss at step 43600: 2.8278805017471313\n",
      "validation loss: 2.868487680753072\n",
      "validation accuracy: 14.018155258764608%\n",
      "loss at step 43620: 2.7811060309410096\n",
      "loss at step 43640: 2.7764613032341003\n",
      "loss at step 43660: 2.7839244842529296\n",
      "loss at step 43680: 2.75270836353302\n",
      "loss at step 43700: 2.7790567755699156\n",
      "loss at step 43720: 2.833853006362915\n",
      "loss at step 43740: 2.724738049507141\n",
      "loss at step 43760: 2.8197766065597536\n",
      "loss at step 43780: 2.8035900712013246\n",
      "loss at step 43800: 2.7892716407775877\n",
      "loss at step 43820: 2.7633232474327087\n",
      "loss at step 43840: 2.7397710561752318\n",
      "loss at step 43860: 2.726878249645233\n",
      "loss at step 43880: 2.785637068748474\n",
      "loss at step 43900: 2.8349884510040284\n",
      "loss at step 43920: 2.7422034978866576\n",
      "loss at step 43940: 2.781803107261658\n",
      "loss at step 43960: 2.7644914746284486\n",
      "loss at step 43980: 2.730730438232422\n",
      "loss at step 44000: 2.8176987886428835\n",
      "validation loss: 2.906256012916565\n",
      "validation accuracy: 13.637312186978297%\n",
      "loss at step 44020: 2.821632242202759\n",
      "loss at step 44040: 2.895398938655853\n",
      "loss at step 44060: 2.806258237361908\n",
      "loss at step 44080: 2.752614462375641\n",
      "loss at step 44100: 2.8237538933753967\n",
      "loss at step 44120: 2.8279003739356994\n",
      "loss at step 44140: 2.804756593704224\n",
      "loss at step 44160: 2.7896305561065673\n",
      "loss at step 44180: 2.7070165991783144\n",
      "loss at step 44200: 2.8222860455513\n",
      "loss at step 44220: 2.799030363559723\n",
      "loss at step 44240: 2.7741291880607606\n",
      "loss at step 44260: 2.799439013004303\n",
      "loss at step 44280: 2.751198446750641\n",
      "loss at step 44300: 2.800726020336151\n",
      "loss at step 44320: 2.7630532026290893\n",
      "loss at step 44340: 2.82082484960556\n",
      "loss at step 44360: 2.828418457508087\n",
      "loss at step 44380: 2.8594648003578187\n",
      "loss at step 44400: 2.774500107765198\n",
      "validation loss: 2.883545508384705\n",
      "validation accuracy: 13.872078464106844%\n",
      "loss at step 44420: 2.848414623737335\n",
      "loss at step 44440: 2.8009876132011415\n",
      "loss at step 44460: 2.829594361782074\n",
      "loss at step 44480: 2.8354540824890138\n",
      "loss at step 44500: 2.7917123198509217\n",
      "loss at step 44520: 2.79388245344162\n",
      "loss at step 44540: 2.791520524024963\n",
      "loss at step 44560: 2.668379557132721\n",
      "loss at step 44580: 2.784863221645355\n",
      "loss at step 44600: 2.822977638244629\n",
      "loss at step 44620: 2.810347080230713\n",
      "loss at step 44640: 2.770449662208557\n",
      "loss at step 44660: 2.7009405136108398\n",
      "loss at step 44680: 2.70192004442215\n",
      "loss at step 44700: 2.750047028064728\n",
      "loss at step 44720: 2.7407689094543457\n",
      "loss at step 44740: 2.754652667045593\n",
      "loss at step 44760: 2.738487684726715\n",
      "loss at step 44780: 2.8372249007225037\n",
      "loss at step 44800: 2.866687226295471\n",
      "validation loss: 2.877470655441284\n",
      "validation accuracy: 14.143363939899833%\n",
      "loss at step 44820: 2.804127264022827\n",
      "loss at step 44840: 2.7726383924484255\n",
      "loss at step 44860: 2.7886057615280153\n",
      "loss at step 44880: 2.8273531675338743\n",
      "loss at step 44900: 2.7709155082702637\n",
      "loss at step 44920: 2.82125608921051\n",
      "loss at step 44940: 2.831343686580658\n",
      "loss at step 44960: 2.728442871570587\n",
      "loss at step 44980: 2.771176648139954\n",
      "loss at step 45000: 2.798805224895477\n",
      "loss at step 45020: 2.8448195099830627\n",
      "loss at step 45040: 2.817654013633728\n",
      "loss at step 45060: 2.8745479822158813\n",
      "loss at step 45080: 2.7442089438438417\n",
      "loss at step 45100: 2.753955936431885\n",
      "loss at step 45120: 2.793581414222717\n",
      "loss at step 45140: 2.755009210109711\n",
      "loss at step 45160: 2.790539062023163\n",
      "loss at step 45180: 2.819524919986725\n",
      "loss at step 45200: 2.7815982341766357\n",
      "validation loss: 2.88887171904246\n",
      "validation accuracy: 13.465150250417363%\n",
      "loss at step 45220: 2.7571397542953493\n",
      "loss at step 45240: 2.775615096092224\n",
      "loss at step 45260: 2.7921342134475706\n",
      "loss at step 45280: 2.758687210083008\n",
      "loss at step 45300: 2.7851473569869993\n",
      "loss at step 45320: 2.719615399837494\n",
      "loss at step 45340: 2.7646637916564942\n",
      "loss at step 45360: 2.7186797857284546\n",
      "loss at step 45380: 2.765759015083313\n",
      "loss at step 45400: 2.8028265714645384\n",
      "loss at step 45420: 2.7126736283302306\n",
      "loss at step 45440: 2.746115815639496\n",
      "loss at step 45460: 2.76940199136734\n",
      "loss at step 45480: 2.778131401538849\n",
      "loss at step 45500: 2.7708698868751527\n",
      "loss at step 45520: 2.7467820048332214\n",
      "loss at step 45540: 2.834946537017822\n",
      "loss at step 45560: 2.7186909556388854\n",
      "loss at step 45580: 2.931252956390381\n",
      "loss at step 45600: 2.7598085761070252\n",
      "validation loss: 2.863781900405884\n",
      "validation accuracy: 13.945116861435727%\n",
      "loss at step 45620: 2.785830223560333\n",
      "loss at step 45640: 2.8312862396240233\n",
      "loss at step 45660: 2.710140359401703\n",
      "loss at step 45680: 2.7793119072914125\n",
      "loss at step 45700: 2.7769961953163147\n",
      "loss at step 45720: 2.8801827311515806\n",
      "loss at step 45740: 2.7755512237548827\n",
      "loss at step 45760: 2.808726966381073\n",
      "loss at step 45780: 2.773159456253052\n",
      "loss at step 45800: 2.755851221084595\n",
      "loss at step 45820: 2.783173608779907\n",
      "loss at step 45840: 2.850780189037323\n",
      "loss at step 45860: 2.769230914115906\n",
      "loss at step 45880: 2.8068820953369142\n",
      "loss at step 45900: 2.774194133281708\n",
      "loss at step 45920: 2.854803812503815\n",
      "loss at step 45940: 2.800819754600525\n",
      "loss at step 45960: 2.7731295824050903\n",
      "loss at step 45980: 2.731802725791931\n",
      "loss at step 46000: 2.763339710235596\n",
      "validation loss: 2.858214464187622\n",
      "validation accuracy: 13.673831385642737%\n",
      "loss at step 46020: 2.7361258268356323\n",
      "loss at step 46040: 2.711751198768616\n",
      "loss at step 46060: 2.7110755801200868\n",
      "loss at step 46080: 2.665476143360138\n",
      "loss at step 46100: 2.7363205552101135\n",
      "loss at step 46120: 2.8315029740333557\n",
      "loss at step 46140: 2.7561862468719482\n",
      "loss at step 46160: 2.7567601442337035\n",
      "loss at step 46180: 2.8000677704811094\n",
      "loss at step 46200: 2.753715193271637\n",
      "loss at step 46220: 2.6921635985374452\n",
      "loss at step 46240: 2.8404157638549803\n",
      "loss at step 46260: 2.7659040689468384\n",
      "loss at step 46280: 2.8320943236351015\n",
      "loss at step 46300: 2.839594531059265\n",
      "loss at step 46320: 2.7997573375701905\n",
      "loss at step 46340: 2.728740727901459\n",
      "loss at step 46360: 2.7956692695617678\n",
      "loss at step 46380: 2.7757062554359435\n",
      "loss at step 46400: 2.7871421337127686\n",
      "validation loss: 2.8590729920069378\n",
      "validation accuracy: 14.049457429048415%\n",
      "loss at step 46420: 2.7384635329246523\n",
      "loss at step 46440: 2.8026896357536315\n",
      "loss at step 46460: 2.7698207259178163\n",
      "loss at step 46480: 2.800939691066742\n",
      "loss at step 46500: 2.690489614009857\n",
      "loss at step 46520: 2.8190650582313537\n",
      "loss at step 46540: 2.753205859661102\n",
      "loss at step 46560: 2.755918097496033\n",
      "loss at step 46580: 2.8117292761802672\n",
      "loss at step 46600: 2.76122761964798\n",
      "loss at step 46620: 2.8029022336006166\n",
      "loss at step 46640: 2.763311576843262\n",
      "loss at step 46660: 2.807227849960327\n",
      "loss at step 46680: 2.7447402477264404\n",
      "loss at step 46700: 2.839998984336853\n",
      "loss at step 46720: 2.7810518026351927\n",
      "loss at step 46740: 2.7739508867263796\n",
      "loss at step 46760: 2.7532298445701597\n",
      "loss at step 46780: 2.8029029965400696\n",
      "loss at step 46800: 2.74309755563736\n",
      "validation loss: 2.8467332299550376\n",
      "validation accuracy: 14.012938230383973%\n",
      "loss at step 46820: 2.8001466274261473\n",
      "loss at step 46840: 2.800443112850189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at step 46860: 2.7825340747833254\n",
      "loss at step 46880: 2.8455321907997133\n",
      "loss at step 46900: 2.7569782376289367\n",
      "loss at step 46920: 2.7729106545448303\n",
      "loss at step 46940: 2.807039797306061\n",
      "loss at step 46960: 2.800514554977417\n",
      "loss at step 46980: 2.661225140094757\n",
      "loss at step 47000: 2.7731064796447753\n",
      "loss at step 47020: 2.757568371295929\n",
      "loss at step 47040: 2.86082661151886\n",
      "loss at step 47060: 2.7160537600517274\n",
      "loss at step 47080: 2.744175112247467\n",
      "loss at step 47100: 2.7443116784095762\n",
      "loss at step 47120: 2.758276093006134\n",
      "loss at step 47140: 2.8226725697517394\n",
      "loss at step 47160: 2.7191230773925783\n",
      "loss at step 47180: 2.75035582780838\n",
      "loss at step 47200: 2.833278167247772\n",
      "validation loss: 2.876226096153259\n",
      "validation accuracy: 13.757303839732888%\n",
      "loss at step 47220: 2.770278573036194\n",
      "loss at step 47240: 2.6442154169082643\n",
      "loss at step 47260: 2.7812323331832887\n",
      "loss at step 47280: 2.8133098602294924\n",
      "loss at step 47300: 2.7786020398139955\n",
      "loss at step 47320: 2.7778807520866393\n",
      "loss at step 47340: 2.7709800720214846\n",
      "loss at step 47360: 2.844837987422943\n",
      "loss at step 47380: 2.795965850353241\n",
      "loss at step 47400: 2.7427318930625915\n",
      "loss at step 47420: 2.6793607234954835\n",
      "loss at step 47440: 2.7121540307998657\n",
      "loss at step 47460: 2.7736717224121095\n",
      "loss at step 47480: 2.751489543914795\n",
      "loss at step 47500: 2.8137198805809023\n",
      "loss at step 47520: 2.782693696022034\n",
      "loss at step 47540: 2.7589865207672117\n",
      "loss at step 47560: 2.6891925930976868\n",
      "loss at step 47580: 2.7557636380195616\n",
      "loss at step 47600: 2.727060329914093\n",
      "validation loss: 2.847580024401347\n",
      "validation accuracy: 14.576377295492488%\n",
      "loss at step 47620: 2.783359682559967\n",
      "loss at step 47640: 2.702641022205353\n",
      "loss at step 47660: 2.7499306321144106\n",
      "loss at step 47680: 2.7517770290374757\n",
      "loss at step 47700: 2.7552585363388062\n",
      "loss at step 47720: 2.813929522037506\n",
      "loss at step 47740: 2.830017650127411\n",
      "loss at step 47760: 2.75366325378418\n",
      "loss at step 47780: 2.729675126075745\n",
      "loss at step 47800: 2.7338053584098816\n",
      "loss at step 47820: 2.812974452972412\n",
      "loss at step 47840: 2.744421291351318\n",
      "loss at step 47860: 2.7764689326286316\n",
      "loss at step 47880: 2.7967167139053344\n",
      "loss at step 47900: 2.741248440742493\n",
      "loss at step 47920: 2.7324124574661255\n",
      "loss at step 47940: 2.7699623584747313\n",
      "loss at step 47960: 2.8176041722297667\n",
      "loss at step 47980: 2.8226130485534666\n",
      "loss at step 48000: 2.785534369945526\n",
      "validation loss: 2.839103455543518\n",
      "validation accuracy: 14.545075125208681%\n",
      "loss at step 48020: 2.756708598136902\n",
      "loss at step 48040: 2.714614510536194\n",
      "loss at step 48060: 2.734582018852234\n",
      "loss at step 48080: 2.8142649292945863\n",
      "loss at step 48100: 2.755139148235321\n",
      "loss at step 48120: 2.752896726131439\n",
      "loss at step 48140: 2.7805607199668883\n",
      "loss at step 48160: 2.793179428577423\n",
      "loss at step 48180: 2.7798046231269837\n",
      "loss at step 48200: 2.7166205286979674\n",
      "loss at step 48220: 2.6877391219139097\n",
      "loss at step 48240: 2.83812997341156\n",
      "loss at step 48260: 2.6864442229270935\n",
      "loss at step 48280: 2.7769803524017336\n",
      "loss at step 48300: 2.7733197808265686\n",
      "loss at step 48320: 2.764883077144623\n",
      "loss at step 48340: 2.7959220886230467\n",
      "loss at step 48360: 2.741747558116913\n",
      "loss at step 48380: 2.748565447330475\n",
      "loss at step 48400: 2.797644829750061\n",
      "validation loss: 2.8489880990982055\n",
      "validation accuracy: 14.059891485809683%\n",
      "loss at step 48420: 2.751438093185425\n",
      "loss at step 48440: 2.720565938949585\n",
      "loss at step 48460: 2.8319127559661865\n",
      "loss at step 48480: 2.719913923740387\n",
      "loss at step 48500: 2.8445352435112\n",
      "loss at step 48520: 2.7787975907325744\n",
      "loss at step 48540: 2.727704620361328\n",
      "loss at step 48560: 2.781231701374054\n",
      "loss at step 48580: 2.77684690952301\n",
      "loss at step 48600: 2.7237680673599245\n",
      "loss at step 48620: 2.776935708522797\n",
      "loss at step 48640: 2.768930172920227\n",
      "loss at step 48660: 2.778006839752197\n",
      "loss at step 48680: 2.7365959525108337\n",
      "loss at step 48700: 2.8095775961875917\n",
      "loss at step 48720: 2.7985056757926943\n",
      "loss at step 48740: 2.7782773971557617\n",
      "loss at step 48760: 2.731959545612335\n",
      "loss at step 48780: 2.7561448931694033\n",
      "loss at step 48800: 2.7889345288276672\n",
      "validation loss: 2.8616518640518187\n",
      "validation accuracy: 13.444282136894826%\n",
      "loss at step 48820: 2.82170375585556\n",
      "loss at step 48840: 2.7846111178398134\n",
      "loss at step 48860: 2.7318267822265625\n",
      "loss at step 48880: 2.7429492235183717\n",
      "loss at step 48900: 2.7397727489471437\n",
      "loss at step 48920: 2.7919630289077757\n",
      "loss at step 48940: 2.7240074992179872\n",
      "loss at step 48960: 2.7733695983886717\n",
      "loss at step 48980: 2.720234990119934\n",
      "loss at step 49000: 2.7788933157920837\n",
      "loss at step 49020: 2.74701132774353\n",
      "loss at step 49040: 2.8094055533409117\n",
      "loss at step 49060: 2.787192702293396\n",
      "loss at step 49080: 2.8237075567245484\n",
      "loss at step 49100: 2.735854911804199\n",
      "loss at step 49120: 2.761599612236023\n",
      "loss at step 49140: 2.758385705947876\n",
      "loss at step 49160: 2.8263845443725586\n",
      "loss at step 49180: 2.721000444889069\n",
      "loss at step 49200: 2.8176246881484985\n",
      "validation loss: 2.848792119026184\n",
      "validation accuracy: 14.3050918196995%\n",
      "loss at step 49220: 2.7421817898750307\n",
      "loss at step 49240: 2.804456186294556\n",
      "loss at step 49260: 2.784925162792206\n",
      "loss at step 49280: 2.7423981070518493\n",
      "loss at step 49300: 2.7793076634407043\n",
      "loss at step 49320: 2.821579432487488\n",
      "loss at step 49340: 2.702460539340973\n",
      "loss at step 49360: 2.8306875824928284\n",
      "loss at step 49380: 2.7271093249320986\n",
      "loss at step 49400: 2.696753144264221\n",
      "loss at step 49420: 2.7747301697731017\n",
      "loss at step 49440: 2.732628619670868\n",
      "loss at step 49460: 2.738061082363129\n",
      "loss at step 49480: 2.7746656775474547\n",
      "loss at step 49500: 2.7856030225753785\n",
      "loss at step 49520: 2.689049851894379\n",
      "loss at step 49540: 2.7468284487724306\n",
      "loss at step 49560: 2.7890330076217653\n",
      "loss at step 49580: 2.7982938170433043\n",
      "loss at step 49600: 2.721164643764496\n",
      "validation loss: 2.8655732242266336\n",
      "validation accuracy: 14.393781302170282%\n",
      "loss at step 49620: 2.7815509676933288\n",
      "loss at step 49640: 2.8303348779678346\n",
      "loss at step 49660: 2.7824328422546385\n",
      "loss at step 49680: 2.7048951268196104\n",
      "loss at step 49700: 2.777110016345978\n",
      "loss at step 49720: 2.7507132291793823\n",
      "loss at step 49740: 2.697646880149841\n",
      "loss at step 49760: 2.7812722444534304\n",
      "loss at step 49780: 2.7455311894416807\n",
      "loss at step 49800: 2.7930960297584533\n",
      "loss at step 49820: 2.838458442687988\n",
      "loss at step 49840: 2.6860849618911744\n",
      "loss at step 49860: 2.8212448835372923\n",
      "loss at step 49880: 2.733601117134094\n",
      "loss at step 49900: 2.768625259399414\n",
      "loss at step 49920: 2.744202721118927\n",
      "loss at step 49940: 2.7701131224632265\n",
      "loss at step 49960: 2.7917927742004394\n",
      "loss at step 49980: 2.768796277046204\n",
      "loss at step 50000: 2.719892346858978\n",
      "validation loss: 2.848325680096944\n",
      "validation accuracy: 14.372913188647745%\n",
      "loss at step 50020: 2.808137571811676\n",
      "loss at step 50040: 2.7611009001731874\n",
      "loss at step 50060: 2.6980247378349302\n",
      "loss at step 50080: 2.732297253608704\n",
      "loss at step 50100: 2.797846782207489\n",
      "loss at step 50120: 2.810619366168976\n",
      "loss at step 50140: 2.706215727329254\n",
      "loss at step 50160: 2.763928759098053\n",
      "loss at step 50180: 2.6806584000587463\n",
      "loss at step 50200: 2.7428313851356507\n",
      "loss at step 50220: 2.7219626545906066\n",
      "loss at step 50240: 2.809384250640869\n",
      "loss at step 50260: 2.710315978527069\n",
      "loss at step 50280: 2.748998057842255\n",
      "loss at step 50300: 2.715788745880127\n",
      "loss at step 50320: 2.7948972821235656\n",
      "loss at step 50340: 2.748480296134949\n",
      "loss at step 50360: 2.786794292926788\n",
      "loss at step 50380: 2.715848755836487\n",
      "loss at step 50400: 2.764210915565491\n",
      "validation loss: 2.857127407391866\n",
      "validation accuracy: 14.096410684474122%\n",
      "loss at step 50420: 2.7074074268341066\n",
      "loss at step 50440: 2.7700398564338684\n",
      "loss at step 50460: 2.793054795265198\n",
      "loss at step 50480: 2.7976852893829345\n",
      "loss at step 50500: 2.704783523082733\n",
      "loss at step 50520: 2.6701985955238343\n",
      "loss at step 50540: 2.7919756174087524\n",
      "loss at step 50560: 2.8277469277381897\n",
      "loss at step 50580: 2.850078523159027\n",
      "loss at step 50600: 2.767969512939453\n",
      "loss at step 50620: 2.8220431208610535\n",
      "loss at step 50640: 2.8382821559906004\n",
      "loss at step 50660: 2.7927305936813354\n",
      "loss at step 50680: 2.750222611427307\n",
      "loss at step 50700: 2.737588334083557\n",
      "loss at step 50720: 2.702511155605316\n",
      "loss at step 50740: 2.7226292729377746\n",
      "loss at step 50760: 2.779880237579346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at step 50780: 2.773786163330078\n",
      "loss at step 50800: 2.7333736062049865\n",
      "validation loss: 2.833524515628815\n",
      "validation accuracy: 14.284223706176963%\n",
      "loss at step 50820: 2.7207476258277894\n",
      "loss at step 50840: 2.7761268734931948\n",
      "loss at step 50860: 2.7427863836288453\n",
      "loss at step 50880: 2.8087934017181397\n",
      "loss at step 50900: 2.788203001022339\n",
      "loss at step 50920: 2.8196742296218873\n",
      "loss at step 50940: 2.7619823813438416\n",
      "loss at step 50960: 2.739240312576294\n",
      "loss at step 50980: 2.6895206689834597\n",
      "loss at step 51000: 2.7420029759407045\n",
      "loss at step 51020: 2.7467488169670107\n",
      "loss at step 51040: 2.753857409954071\n",
      "loss at step 51060: 2.776341104507446\n",
      "loss at step 51080: 2.7315117478370667\n",
      "loss at step 51100: 2.7882624745368956\n",
      "loss at step 51120: 2.799879002571106\n",
      "loss at step 51140: 2.7458465814590456\n",
      "loss at step 51160: 2.7568896651268004\n",
      "loss at step 51180: 2.7715800046920775\n",
      "loss at step 51200: 2.746755313873291\n",
      "validation loss: 2.845983176231384\n",
      "validation accuracy: 13.892946577629385%\n",
      "loss at step 51220: 2.723523461818695\n",
      "loss at step 51240: 2.7621206879615783\n",
      "loss at step 51260: 2.7487525582313537\n",
      "loss at step 51280: 2.7164200067520143\n",
      "loss at step 51300: 2.7782623291015627\n",
      "loss at step 51320: 2.739964711666107\n",
      "loss at step 51340: 2.6980751037597654\n",
      "loss at step 51360: 2.7070218086242677\n",
      "loss at step 51380: 2.7331182956695557\n",
      "loss at step 51400: 2.7374913334846496\n",
      "loss at step 51420: 2.7493074774742126\n",
      "loss at step 51440: 2.742511975765228\n",
      "loss at step 51460: 2.6481932163238526\n",
      "loss at step 51480: 2.770479381084442\n",
      "loss at step 51500: 2.794230329990387\n",
      "loss at step 51520: 2.7780442237854004\n",
      "loss at step 51540: 2.7804224491119385\n",
      "loss at step 51560: 2.7301535487174986\n",
      "loss at step 51580: 2.7796818137168886\n",
      "loss at step 51600: 2.8088279247283934\n",
      "validation loss: 2.856822903951009\n",
      "validation accuracy: 13.992070116861436%\n",
      "loss at step 51620: 2.76566481590271\n",
      "loss at step 51640: 2.7321136474609373\n",
      "loss at step 51660: 2.8153329253196717\n",
      "loss at step 51680: 2.8213393449783326\n",
      "loss at step 51700: 2.751805281639099\n",
      "loss at step 51720: 2.791001558303833\n",
      "loss at step 51740: 2.745823872089386\n",
      "loss at step 51760: 2.79186065196991\n",
      "loss at step 51780: 2.742557501792908\n",
      "loss at step 51800: 2.735172164440155\n",
      "loss at step 51820: 2.6671075224876404\n",
      "loss at step 51840: 2.6993660807609556\n",
      "loss at step 51860: 2.6684793412685393\n",
      "loss at step 51880: 2.8230373859405518\n",
      "loss at step 51900: 2.7535301566123964\n",
      "loss at step 51920: 2.797263169288635\n",
      "loss at step 51940: 2.7538472175598145\n",
      "loss at step 51960: 2.7532018542289736\n",
      "loss at step 51980: 2.744043970108032\n",
      "loss at step 52000: 2.830138087272644\n",
      "validation loss: 2.8745324675242108\n",
      "validation accuracy: 14.018155258764608%\n",
      "loss at step 52020: 2.7211148619651793\n",
      "loss at step 52040: 2.703674829006195\n",
      "loss at step 52060: 2.797726607322693\n",
      "loss at step 52080: 2.7634912014007567\n",
      "loss at step 52100: 2.7623966932296753\n",
      "loss at step 52120: 2.6872867226600645\n",
      "loss at step 52140: 2.78398334980011\n",
      "loss at step 52160: 2.7892680168151855\n",
      "loss at step 52180: 2.7530457854270933\n",
      "loss at step 52200: 2.815037477016449\n",
      "loss at step 52220: 2.704215705394745\n",
      "loss at step 52240: 2.785920763015747\n",
      "loss at step 52260: 2.747605526447296\n",
      "loss at step 52280: 2.751010704040527\n",
      "loss at step 52300: 2.7432079911231995\n",
      "loss at step 52320: 2.7250325441360475\n",
      "loss at step 52340: 2.791540288925171\n",
      "loss at step 52360: 2.780258870124817\n",
      "loss at step 52380: 2.7308191299438476\n",
      "loss at step 52400: 2.7618197917938234\n",
      "validation loss: 2.855018073717753\n",
      "validation accuracy: 14.268572621035059%\n",
      "loss at step 52420: 2.78259094953537\n",
      "loss at step 52440: 2.6942962169647218\n",
      "loss at step 52460: 2.7334964632987977\n",
      "loss at step 52480: 2.777923607826233\n",
      "loss at step 52500: 2.748621606826782\n",
      "loss at step 52520: 2.73418208360672\n",
      "loss at step 52540: 2.782158041000366\n",
      "loss at step 52560: 2.7159877181053163\n",
      "loss at step 52580: 2.816027295589447\n",
      "loss at step 52600: 2.7008267283439635\n",
      "loss at step 52620: 2.722987985610962\n",
      "loss at step 52640: 2.7609045028686525\n",
      "loss at step 52660: 2.6774515390396116\n",
      "loss at step 52680: 2.737817406654358\n",
      "loss at step 52700: 2.7620153069496154\n",
      "loss at step 52720: 2.770021915435791\n",
      "loss at step 52740: 2.788599729537964\n",
      "loss at step 52760: 2.799041175842285\n",
      "loss at step 52780: 2.786510407924652\n",
      "loss at step 52800: 2.851873481273651\n",
      "validation loss: 2.8439827299118043\n",
      "validation accuracy: 14.11727879799666%\n",
      "loss at step 52820: 2.7261496067047117\n",
      "loss at step 52840: 2.7208745956420897\n",
      "loss at step 52860: 2.675586074590683\n",
      "loss at step 52880: 2.609579300880432\n",
      "loss at step 52900: 2.6904933929443358\n",
      "loss at step 52920: 2.761333501338959\n",
      "loss at step 52940: 2.782137954235077\n",
      "loss at step 52960: 2.747071611881256\n",
      "loss at step 52980: 2.8044267177581785\n",
      "loss at step 53000: 2.8030320763587953\n",
      "loss at step 53020: 2.8266621232032776\n",
      "loss at step 53040: 2.8183244109153747\n",
      "loss at step 53060: 2.718462347984314\n",
      "loss at step 53080: 2.708216094970703\n",
      "loss at step 53100: 2.787716794013977\n",
      "loss at step 53120: 2.7333956718444825\n",
      "loss at step 53140: 2.8193192958831785\n",
      "loss at step 53160: 2.842834103107452\n",
      "loss at step 53180: 2.735648250579834\n",
      "loss at step 53200: 2.7904473185539245\n",
      "validation loss: 2.860389362970988\n",
      "validation accuracy: 14.3050918196995%\n",
      "loss at step 53220: 2.807067358493805\n",
      "loss at step 53240: 2.6757627844810488\n",
      "loss at step 53260: 2.706395721435547\n",
      "loss at step 53280: 2.7670741200447084\n",
      "loss at step 53300: 2.6869794607162474\n",
      "loss at step 53320: 2.7520878672599793\n",
      "loss at step 53340: 2.7572596192359926\n",
      "loss at step 53360: 2.7281251430511473\n",
      "loss at step 53380: 2.759185814857483\n",
      "loss at step 53400: 2.7348868370056154\n",
      "loss at step 53420: 2.716348946094513\n",
      "loss at step 53440: 2.7307286858558655\n",
      "loss at step 53460: 2.7693188786506653\n",
      "loss at step 53480: 2.774968957901001\n",
      "loss at step 53500: 2.7998299837112426\n",
      "loss at step 53520: 2.7989282250404357\n",
      "loss at step 53540: 2.8316516399383547\n",
      "loss at step 53560: 2.7327752709388733\n",
      "loss at step 53580: 2.7275214076042174\n",
      "loss at step 53600: 2.745179498195648\n",
      "validation loss: 2.8289303414026894\n",
      "validation accuracy: 14.372913188647745%\n",
      "loss at step 53620: 2.7537863492965697\n",
      "loss at step 53640: 2.76302033662796\n",
      "loss at step 53660: 2.788180184364319\n",
      "loss at step 53680: 2.726653504371643\n",
      "loss at step 53700: 2.6713762998580934\n",
      "loss at step 53720: 2.773119592666626\n",
      "loss at step 53740: 2.7814075350761414\n",
      "loss at step 53760: 2.7643195509910585\n",
      "loss at step 53780: 2.7029212713241577\n",
      "loss at step 53800: 2.750418496131897\n",
      "loss at step 53820: 2.6844509482383727\n",
      "loss at step 53840: 2.766584062576294\n",
      "loss at step 53860: 2.751910376548767\n",
      "loss at step 53880: 2.7134456753730776\n",
      "loss at step 53900: 2.75745005607605\n",
      "loss at step 53920: 2.6833083510398863\n",
      "loss at step 53940: 2.7289038777351378\n",
      "loss at step 53960: 2.7471883416175844\n",
      "loss at step 53980: 2.728737497329712\n",
      "loss at step 54000: 2.839909887313843\n",
      "validation loss: 2.879013991355896\n",
      "validation accuracy: 13.934682804674459%\n",
      "loss at step 54020: 2.7347677826881407\n",
      "loss at step 54040: 2.791184461116791\n",
      "loss at step 54060: 2.8270201086997986\n",
      "loss at step 54080: 2.783590018749237\n",
      "loss at step 54100: 2.6511947751045226\n",
      "loss at step 54120: 2.72608243227005\n",
      "loss at step 54140: 2.7641202926635744\n",
      "loss at step 54160: 2.7567833185195925\n",
      "loss at step 54180: 2.7397829055786134\n",
      "loss at step 54200: 2.7526782155036926\n",
      "loss at step 54220: 2.8410826444625856\n",
      "loss at step 54240: 2.7597587943077087\n",
      "loss at step 54260: 2.7586724758148193\n",
      "loss at step 54280: 2.651252734661102\n",
      "loss at step 54300: 2.742608034610748\n",
      "loss at step 54320: 2.8172407746315002\n",
      "loss at step 54340: 2.7757112979888916\n",
      "loss at step 54360: 2.789288103580475\n",
      "loss at step 54380: 2.75597767829895\n",
      "loss at step 54400: 2.672749328613281\n",
      "validation loss: 2.83474517027537\n",
      "validation accuracy: 14.263355592654426%\n",
      "loss at step 54420: 2.7060128092765807\n",
      "loss at step 54440: 2.6806686162948608\n",
      "loss at step 54460: 2.699487900733948\n",
      "loss at step 54480: 2.8012555480003356\n",
      "loss at step 54500: 2.7303635835647584\n",
      "loss at step 54520: 2.7386797189712526\n",
      "loss at step 54540: 2.732919955253601\n",
      "loss at step 54560: 2.756059229373932\n",
      "loss at step 54580: 2.7536007165908813\n",
      "loss at step 54600: 2.756284439563751\n",
      "loss at step 54620: 2.82136754989624\n",
      "loss at step 54640: 2.6903775930404663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at step 54660: 2.7001577377319337\n",
      "loss at step 54680: 2.6975210428237917\n",
      "loss at step 54700: 2.626429057121277\n",
      "loss at step 54720: 2.617084431648254\n",
      "loss at step 54740: 2.779800498485565\n",
      "loss at step 54760: 2.7971866846084597\n",
      "loss at step 54780: 2.7710905194282534\n",
      "loss at step 54800: 2.731093239784241\n",
      "validation loss: 2.8952632919947305\n",
      "validation accuracy: 13.637312186978297%\n",
      "loss at step 54820: 2.764820158481598\n",
      "loss at step 54840: 2.8281885981559753\n",
      "loss at step 54860: 2.7724265456199646\n",
      "loss at step 54880: 2.8015897274017334\n",
      "loss at step 54900: 2.8398864030838014\n",
      "loss at step 54920: 2.768544614315033\n",
      "loss at step 54940: 2.8096962094306948\n",
      "loss at step 54960: 2.764060711860657\n",
      "loss at step 54980: 2.7460445165634155\n",
      "loss at step 55000: 2.8249533414840697\n",
      "loss at step 55020: 2.695324456691742\n",
      "loss at step 55040: 2.7881848931312563\n",
      "loss at step 55060: 2.743749499320984\n",
      "loss at step 55080: 2.782752740383148\n",
      "loss at step 55100: 2.6948406100273132\n",
      "loss at step 55120: 2.7707252621650698\n",
      "loss at step 55140: 2.7508813738822937\n",
      "loss at step 55160: 2.698410713672638\n",
      "loss at step 55180: 2.716429257392883\n",
      "loss at step 55200: 2.6692352890968323\n",
      "validation loss: 2.8205147981643677\n",
      "validation accuracy: 14.430300500834726%\n",
      "loss at step 55220: 2.781607985496521\n",
      "loss at step 55240: 2.790438401699066\n",
      "loss at step 55260: 2.81384094953537\n",
      "loss at step 55280: 2.711287522315979\n",
      "loss at step 55300: 2.7108027935028076\n",
      "loss at step 55320: 2.7134439945220947\n",
      "loss at step 55340: 2.752400815486908\n",
      "loss at step 55360: 2.770986795425415\n",
      "loss at step 55380: 2.6713786482810975\n",
      "loss at step 55400: 2.681282114982605\n",
      "loss at step 55420: 2.7436154961585997\n",
      "loss at step 55440: 2.759950065612793\n",
      "loss at step 55460: 2.73581280708313\n",
      "loss at step 55480: 2.8383363246917725\n",
      "loss at step 55500: 2.7138124346733092\n",
      "loss at step 55520: 2.709575116634369\n",
      "loss at step 55540: 2.7473032236099244\n",
      "loss at step 55560: 2.7737817645072935\n",
      "loss at step 55580: 2.7458849906921388\n",
      "loss at step 55600: 2.722894847393036\n",
      "validation loss: 2.8368170913060506\n",
      "validation accuracy: 14.122495826377296%\n",
      "loss at step 55620: 2.72844774723053\n",
      "loss at step 55640: 2.6959628224372865\n",
      "loss at step 55660: 2.7578316211700438\n",
      "loss at step 55680: 2.786954867839813\n",
      "loss at step 55700: 2.768363058567047\n",
      "loss at step 55720: 2.704593539237976\n",
      "loss at step 55740: 2.7458787679672243\n",
      "loss at step 55760: 2.8266952991485597\n",
      "loss at step 55780: 2.7771684169769286\n",
      "loss at step 55800: 2.7110763669013975\n",
      "loss at step 55820: 2.688862144947052\n",
      "loss at step 55840: 2.777855932712555\n",
      "loss at step 55860: 2.7562573552131653\n",
      "loss at step 55880: 2.7620557069778444\n",
      "loss at step 55900: 2.711056280136108\n",
      "loss at step 55920: 2.7415950775146483\n",
      "loss at step 55940: 2.8263237833976746\n",
      "loss at step 55960: 2.7114874005317686\n",
      "loss at step 55980: 2.7347816228866577\n",
      "loss at step 56000: 2.71686452627182\n",
      "validation loss: 2.854042816162109\n",
      "validation accuracy: 14.3050918196995%\n",
      "loss at step 56020: 2.7820012092590334\n",
      "loss at step 56040: 2.7043792724609377\n",
      "loss at step 56060: 2.7343636751174927\n",
      "loss at step 56080: 2.8183924674987795\n",
      "loss at step 56100: 2.727774453163147\n",
      "loss at step 56120: 2.723283565044403\n",
      "loss at step 56140: 2.7889276266098024\n",
      "loss at step 56160: 2.730517292022705\n",
      "loss at step 56180: 2.813220727443695\n",
      "loss at step 56200: 2.782860004901886\n",
      "loss at step 56220: 2.7022742509841917\n",
      "loss at step 56240: 2.7751854062080383\n",
      "loss at step 56260: 2.735820698738098\n",
      "loss at step 56280: 2.7708052515983583\n",
      "loss at step 56300: 2.796591246128082\n",
      "loss at step 56320: 2.69908721446991\n",
      "loss at step 56340: 2.7356138229370117\n",
      "loss at step 56360: 2.728386390209198\n",
      "loss at step 56380: 2.808835768699646\n",
      "loss at step 56400: 2.765419626235962\n",
      "validation loss: 2.8531433216730755\n",
      "validation accuracy: 14.11727879799666%\n",
      "loss at step 56420: 2.6629318833351134\n",
      "loss at step 56440: 2.734729993343353\n",
      "loss at step 56460: 2.7168179392814635\n",
      "loss at step 56480: 2.712957262992859\n",
      "loss at step 56500: 2.6795454025268555\n",
      "loss at step 56520: 2.717761051654816\n",
      "loss at step 56540: 2.791236913204193\n",
      "loss at step 56560: 2.6738256096839903\n",
      "loss at step 56580: 2.7550758004188536\n",
      "loss at step 56600: 2.6970950841903685\n",
      "loss at step 56620: 2.7537344336509704\n",
      "loss at step 56640: 2.7367936730384828\n",
      "loss at step 56660: 2.6995749473571777\n",
      "loss at step 56680: 2.810573959350586\n",
      "loss at step 56700: 2.7795918583869934\n",
      "loss at step 56720: 2.7397683382034304\n",
      "loss at step 56740: 2.774440824985504\n",
      "loss at step 56760: 2.7247371673583984\n",
      "loss at step 56780: 2.7904265582561494\n",
      "loss at step 56800: 2.6620094656944273\n",
      "validation loss: 2.82368722597758\n",
      "validation accuracy: 14.764190317195325%\n",
      "loss at step 56820: 2.7776190042495728\n",
      "loss at step 56840: 2.7020291447639466\n",
      "loss at step 56860: 2.7366192817687987\n",
      "loss at step 56880: 2.822664964199066\n",
      "loss at step 56900: 2.715946280956268\n",
      "loss at step 56920: 2.800281488895416\n",
      "loss at step 56940: 2.7828047394752504\n",
      "loss at step 56960: 2.771105432510376\n",
      "loss at step 56980: 2.6977530837059023\n",
      "loss at step 57000: 2.7899646043777464\n",
      "loss at step 57020: 2.7611735820770265\n",
      "loss at step 57040: 2.7991634607315063\n",
      "loss at step 57060: 2.7078275442123414\n",
      "loss at step 57080: 2.6659705519676207\n",
      "loss at step 57100: 2.7520518779754637\n",
      "loss at step 57120: 2.766183650493622\n",
      "loss at step 57140: 2.733882224559784\n",
      "loss at step 57160: 2.7749125361442566\n",
      "loss at step 57180: 2.727910566329956\n",
      "loss at step 57200: 2.725417983531952\n",
      "validation loss: 2.835210666656494\n",
      "validation accuracy: 13.689482470784641%\n",
      "loss at step 57220: 2.710026240348816\n",
      "loss at step 57240: 2.778817391395569\n",
      "loss at step 57260: 2.7577529311180116\n",
      "loss at step 57280: 2.7083356738090516\n",
      "loss at step 57300: 2.748172175884247\n",
      "loss at step 57320: 2.747626006603241\n",
      "loss at step 57340: 2.7306936144828797\n",
      "loss at step 57360: 2.6651815295219423\n",
      "loss at step 57380: 2.6908360719680786\n",
      "loss at step 57400: 2.6954708218574526\n",
      "loss at step 57420: 2.731067979335785\n",
      "loss at step 57440: 2.7777496218681335\n",
      "loss at step 57460: 2.7181594729423524\n",
      "loss at step 57480: 2.6912095308303834\n",
      "loss at step 57500: 2.7502841114997865\n",
      "loss at step 57520: 2.688066840171814\n",
      "loss at step 57540: 2.764883315563202\n",
      "loss at step 57560: 2.6931164145469664\n",
      "loss at step 57580: 2.717683267593384\n",
      "loss at step 57600: 2.727283203601837\n",
      "validation loss: 2.8481296571095784\n",
      "validation accuracy: 14.054674457429048%\n",
      "loss at step 57620: 2.691825437545776\n",
      "loss at step 57640: 2.7604809522628786\n",
      "loss at step 57660: 2.7329126358032227\n",
      "loss at step 57680: 2.685076105594635\n",
      "loss at step 57700: 2.7586807131767275\n",
      "loss at step 57720: 2.774114894866943\n",
      "loss at step 57740: 2.7370007395744325\n",
      "loss at step 57760: 2.7239624142646788\n",
      "loss at step 57780: 2.7805957794189453\n",
      "loss at step 57800: 2.8335864782333373\n",
      "loss at step 57820: 2.74608736038208\n",
      "loss at step 57840: 2.7472601890563966\n",
      "loss at step 57860: 2.698827636241913\n",
      "loss at step 57880: 2.8019737601280212\n",
      "loss at step 57900: 2.7511033415794373\n",
      "loss at step 57920: 2.694836366176605\n",
      "loss at step 57940: 2.730251228809357\n",
      "loss at step 57960: 2.7451939225196837\n",
      "loss at step 57980: 2.7655670642852783\n",
      "loss at step 58000: 2.7806822538375853\n",
      "validation loss: 2.8431754811604817\n",
      "validation accuracy: 13.778171953255425%\n",
      "loss at step 58020: 2.771195685863495\n",
      "loss at step 58040: 2.71578049659729\n",
      "loss at step 58060: 2.7320797204971314\n",
      "loss at step 58080: 2.6814033150672913\n",
      "loss at step 58100: 2.74702935218811\n",
      "loss at step 58120: 2.7820234537124633\n",
      "loss at step 58140: 2.672723042964935\n",
      "loss at step 58160: 2.792497193813324\n",
      "loss at step 58180: 2.7365322589874266\n",
      "loss at step 58200: 2.7505327701568603\n",
      "loss at step 58220: 2.755919337272644\n",
      "loss at step 58240: 2.691841799020767\n",
      "loss at step 58260: 2.7635515928268433\n",
      "loss at step 58280: 2.850907099246979\n",
      "loss at step 58300: 2.665309250354767\n",
      "loss at step 58320: 2.75305380821228\n",
      "loss at step 58340: 2.7636789679527283\n",
      "loss at step 58360: 2.7588554501533507\n",
      "loss at step 58380: 2.7780345916748046\n",
      "loss at step 58400: 2.764284980297089\n",
      "validation loss: 2.9444064728418984\n",
      "validation accuracy: 11.034015025041736%\n",
      "loss at step 58420: 2.7345108866691588\n",
      "loss at step 58440: 2.7699464559555054\n",
      "loss at step 58460: 2.743864727020264\n",
      "loss at step 58480: 2.7706384301185607\n",
      "loss at step 58500: 2.756822884082794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at step 58520: 2.7327016830444335\n",
      "loss at step 58540: 2.6798805356025697\n",
      "loss at step 58560: 2.738019955158234\n",
      "loss at step 58580: 2.745971703529358\n",
      "loss at step 58600: 2.7260540962219237\n",
      "loss at step 58620: 2.815381479263306\n",
      "loss at step 58640: 2.7682741045951844\n",
      "loss at step 58660: 2.7829195857048035\n",
      "loss at step 58680: 2.790846061706543\n",
      "loss at step 58700: 2.6524888277053833\n",
      "loss at step 58720: 2.791000282764435\n",
      "loss at step 58740: 2.772694158554077\n",
      "loss at step 58760: 2.7792198419570924\n",
      "loss at step 58780: 2.797192931175232\n",
      "loss at step 58800: 2.7804934859275816\n",
      "validation loss: 2.8259252373377484\n",
      "validation accuracy: 14.352045075125208%\n",
      "loss at step 58820: 2.7067964911460876\n",
      "loss at step 58840: 2.7651066422462462\n",
      "loss at step 58860: 2.7699437737464905\n",
      "loss at step 58880: 2.6713834047317504\n",
      "loss at step 58900: 2.723837506771088\n",
      "loss at step 58920: 2.750230872631073\n",
      "loss at step 58940: 2.643436551094055\n",
      "loss at step 58960: 2.7456119656562805\n",
      "loss at step 58980: 2.7476563811302186\n",
      "loss at step 59000: 2.782952094078064\n",
      "loss at step 59020: 2.754054009914398\n",
      "loss at step 59040: 2.7511783957481386\n",
      "loss at step 59060: 2.7861436009407043\n",
      "loss at step 59080: 2.8019799113273622\n",
      "loss at step 59100: 2.7580455899238587\n",
      "loss at step 59120: 2.761945164203644\n",
      "loss at step 59140: 2.6689597368240356\n",
      "loss at step 59160: 2.7372775077819824\n",
      "loss at step 59180: 2.6581828474998472\n",
      "loss at step 59200: 2.690148186683655\n",
      "validation loss: 2.8206425857543946\n",
      "validation accuracy: 15.139816360601003%\n",
      "loss at step 59220: 2.667669880390167\n",
      "loss at step 59240: 2.6843892335891724\n",
      "loss at step 59260: 2.641413700580597\n",
      "loss at step 59280: 2.775593912601471\n",
      "loss at step 59300: 2.819706082344055\n",
      "loss at step 59320: 2.725297200679779\n",
      "loss at step 59340: 2.752075266838074\n",
      "loss at step 59360: 2.8149685621261598\n",
      "loss at step 59380: 2.77445502281189\n",
      "loss at step 59400: 2.676307702064514\n",
      "loss at step 59420: 2.7134920358657837\n",
      "loss at step 59440: 2.6874927043914796\n",
      "loss at step 59460: 2.739079701900482\n",
      "loss at step 59480: 2.756036174297333\n",
      "loss at step 59500: 2.726658308506012\n",
      "loss at step 59520: 2.7300818800926208\n",
      "loss at step 59540: 2.726129961013794\n",
      "loss at step 59560: 2.7406591176986694\n",
      "loss at step 59580: 2.748223567008972\n",
      "loss at step 59600: 2.7046339392662047\n",
      "validation loss: 2.833256597518921\n",
      "validation accuracy: 14.560726210350584%\n",
      "loss at step 59620: 2.713576316833496\n",
      "loss at step 59640: 2.7604331254959105\n",
      "loss at step 59660: 2.75614333152771\n",
      "loss at step 59680: 2.7341686844825746\n",
      "loss at step 59700: 2.7613430857658385\n",
      "loss at step 59720: 2.7680497646331785\n",
      "loss at step 59740: 2.6682434558868406\n",
      "loss at step 59760: 2.7342835783958437\n",
      "loss at step 59780: 2.8348535895347595\n",
      "loss at step 59800: 2.7855143904685975\n",
      "loss at step 59820: 2.8058664560317994\n",
      "loss at step 59840: 2.7190444827079774\n",
      "loss at step 59860: 2.7101776719093325\n",
      "loss at step 59880: 2.758633589744568\n",
      "loss at step 59900: 2.7524054527282713\n",
      "loss at step 59920: 2.754629611968994\n",
      "loss at step 59940: 2.841113865375519\n",
      "loss at step 59960: 2.6817373275756835\n",
      "loss at step 59980: 2.710284245014191\n",
      "loss at step 60000: 2.6969714403152465\n",
      "validation loss: 2.8248991044362386\n",
      "validation accuracy: 14.362479131886477%\n",
      "loss at step 60020: 2.7287256717681885\n",
      "loss at step 60040: 2.6955100297927856\n",
      "loss at step 60060: 2.750535798072815\n",
      "loss at step 60080: 2.784293603897095\n",
      "loss at step 60100: 2.7094844937324525\n",
      "loss at step 60120: 2.6845569372177125\n",
      "loss at step 60140: 2.730234593153\n",
      "loss at step 60160: 2.7624176263809206\n",
      "loss at step 60180: 2.7787723779678344\n",
      "loss at step 60200: 2.777750277519226\n",
      "loss at step 60220: 2.732457363605499\n",
      "loss at step 60240: 2.7885329008102415\n",
      "loss at step 60260: 2.7189873099327087\n",
      "loss at step 60280: 2.811330592632294\n",
      "loss at step 60300: 2.722738170623779\n",
      "loss at step 60320: 2.7116615176200867\n",
      "loss at step 60340: 2.7235844016075133\n",
      "loss at step 60360: 2.7656951189041137\n",
      "loss at step 60380: 2.7449862241744993\n",
      "loss at step 60400: 2.737171506881714\n",
      "validation loss: 2.830479014714559\n",
      "validation accuracy: 14.419866444073456%\n",
      "loss at step 60420: 2.7429192662239075\n",
      "loss at step 60440: 2.761680281162262\n",
      "loss at step 60460: 2.743548309803009\n",
      "loss at step 60480: 2.7597158193588256\n",
      "loss at step 60500: 2.7948108911514282\n",
      "loss at step 60520: 2.652437710762024\n",
      "loss at step 60540: 2.7300384402275086\n",
      "loss at step 60560: 2.8024951815605164\n",
      "loss at step 60580: 2.751203179359436\n",
      "loss at step 60600: 2.784005331993103\n",
      "loss at step 60620: 2.756696009635925\n",
      "loss at step 60640: 2.6768147110939027\n",
      "loss at step 60660: 2.7267236948013305\n",
      "loss at step 60680: 2.7660258054733275\n",
      "loss at step 60700: 2.6917552947998047\n",
      "loss at step 60720: 2.7999170064926147\n",
      "loss at step 60740: 2.7395166873931887\n",
      "loss at step 60760: 2.6895463943481444\n",
      "loss at step 60780: 2.7869325160980223\n",
      "loss at step 60800: 2.7521311163902284\n",
      "validation loss: 2.817365665435791\n",
      "validation accuracy: 14.367696160267112%\n",
      "loss at step 60820: 2.7763319492340086\n",
      "loss at step 60840: 2.734352207183838\n",
      "loss at step 60860: 2.723279118537903\n",
      "loss at step 60880: 2.7558422446250916\n",
      "loss at step 60900: 2.705502915382385\n",
      "loss at step 60920: 2.7758856534957888\n",
      "loss at step 60940: 2.7335610032081603\n",
      "loss at step 60960: 2.6804319262504577\n",
      "loss at step 60980: 2.7188089728355407\n",
      "loss at step 61000: 2.6882316708564757\n",
      "loss at step 61020: 2.8398907661437987\n",
      "loss at step 61040: 2.761986720561981\n",
      "loss at step 61060: 2.7484160542488096\n",
      "loss at step 61080: 2.789764678478241\n",
      "loss at step 61100: 2.7434473276138305\n",
      "loss at step 61120: 2.7472942590713503\n",
      "loss at step 61140: 2.719540536403656\n",
      "loss at step 61160: 2.779629921913147\n",
      "loss at step 61180: 2.695520830154419\n",
      "loss at step 61200: 2.647941780090332\n",
      "validation loss: 2.80926207224528\n",
      "validation accuracy: 15.082429048414022%\n",
      "loss at step 61220: 2.656856369972229\n",
      "loss at step 61240: 2.7044404625892637\n",
      "loss at step 61260: 2.7832555294036867\n",
      "loss at step 61280: 2.7228973507881165\n",
      "loss at step 61300: 2.712239670753479\n",
      "loss at step 61320: 2.7128596544265746\n",
      "loss at step 61340: 2.7475035548210145\n",
      "loss at step 61360: 2.767698860168457\n",
      "loss at step 61380: 2.813247191905975\n",
      "loss at step 61400: 2.7657058358192446\n",
      "loss at step 61420: 2.770466375350952\n",
      "loss at step 61440: 2.768113434314728\n",
      "loss at step 61460: 2.7423234581947327\n",
      "loss at step 61480: 2.6866006016731263\n",
      "loss at step 61500: 2.7534515380859377\n",
      "loss at step 61520: 2.720389175415039\n",
      "loss at step 61540: 2.7087087988853455\n",
      "loss at step 61560: 2.708342802524567\n",
      "loss at step 61580: 2.7300737738609313\n",
      "loss at step 61600: 2.675576412677765\n",
      "validation loss: 2.8182134342193605\n",
      "validation accuracy: 14.571160267111852%\n",
      "loss at step 61620: 2.695659041404724\n",
      "loss at step 61640: 2.761738669872284\n",
      "loss at step 61660: 2.759648084640503\n",
      "loss at step 61680: 2.699258875846863\n",
      "loss at step 61700: 2.7373464465141297\n",
      "loss at step 61720: 2.6952003240585327\n",
      "loss at step 61740: 2.7051427602767943\n",
      "loss at step 61760: 2.8046947479248048\n",
      "loss at step 61780: 2.758520007133484\n",
      "loss at step 61800: 2.7298522710800173\n",
      "loss at step 61820: 2.730101156234741\n",
      "loss at step 61840: 2.789731192588806\n",
      "loss at step 61860: 2.715077781677246\n",
      "loss at step 61880: 2.750671851634979\n",
      "loss at step 61900: 2.771495056152344\n",
      "loss at step 61920: 2.7267683267593386\n",
      "loss at step 61940: 2.798010802268982\n",
      "loss at step 61960: 2.776460337638855\n",
      "loss at step 61980: 2.7364853620529175\n",
      "loss at step 62000: 2.732710266113281\n",
      "validation loss: 2.804771760304769\n",
      "validation accuracy: 14.952003338898164%\n",
      "loss at step 62020: 2.7632681131362915\n",
      "loss at step 62040: 2.7252150416374206\n",
      "loss at step 62060: 2.7672180414199827\n",
      "loss at step 62080: 2.7405285954475405\n",
      "loss at step 62100: 2.717947256565094\n",
      "loss at step 62120: 2.7355067253112795\n",
      "loss at step 62140: 2.699159491062164\n",
      "loss at step 62160: 2.7736093401908875\n",
      "loss at step 62180: 2.781546747684479\n",
      "loss at step 62200: 2.7666508078575136\n",
      "loss at step 62220: 2.7914038419723513\n",
      "loss at step 62240: 2.701442813873291\n",
      "loss at step 62260: 2.751586544513702\n",
      "loss at step 62280: 2.7031676173210144\n",
      "loss at step 62300: 2.6968037247657777\n",
      "loss at step 62320: 2.6997698545455933\n",
      "loss at step 62340: 2.707191693782806\n",
      "loss at step 62360: 2.753262090682983\n",
      "loss at step 62380: 2.709843337535858\n",
      "loss at step 62400: 2.724268686771393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 2.8294899702072143\n",
      "validation accuracy: 15.071994991652755%\n",
      "loss at step 62420: 2.8431493520736693\n",
      "loss at step 62440: 2.6620311379432677\n",
      "loss at step 62460: 2.7294252276420594\n",
      "loss at step 62480: 2.639150249958038\n",
      "loss at step 62500: 2.7677031993865966\n",
      "loss at step 62520: 2.752448832988739\n",
      "loss at step 62540: 2.7343505144119264\n",
      "loss at step 62560: 2.8077401518821716\n",
      "loss at step 62580: 2.6673295855522157\n",
      "loss at step 62600: 2.6994274139404295\n",
      "loss at step 62620: 2.7986891627311707\n",
      "loss at step 62640: 2.714406096935272\n",
      "loss at step 62660: 2.7308116436004637\n",
      "loss at step 62680: 2.758282554149628\n",
      "loss at step 62700: 2.705794095993042\n",
      "loss at step 62720: 2.808923804759979\n",
      "loss at step 62740: 2.829856288433075\n",
      "loss at step 62760: 2.7670055747032167\n",
      "loss at step 62780: 2.7374179124832154\n",
      "loss at step 62800: 2.7568812370300293\n",
      "validation loss: 2.850922118028005\n",
      "validation accuracy: 14.085976627712855%\n",
      "loss at step 62820: 2.6964955925941467\n",
      "loss at step 62840: 2.7683202505111693\n",
      "loss at step 62860: 2.7616482019424438\n",
      "loss at step 62880: 2.656565713882446\n",
      "loss at step 62900: 2.722453033924103\n",
      "loss at step 62920: 2.665605914592743\n",
      "loss at step 62940: 2.7481180667877196\n",
      "loss at step 62960: 2.7400090217590334\n",
      "loss at step 62980: 2.7640634775161743\n",
      "loss at step 63000: 2.7238165736198425\n",
      "loss at step 63020: 2.7878355383872986\n",
      "loss at step 63040: 2.7051513671875\n",
      "loss at step 63060: 2.6726468086242674\n",
      "loss at step 63080: 2.702872431278229\n",
      "loss at step 63100: 2.76166375875473\n",
      "loss at step 63120: 2.6844433903694154\n",
      "loss at step 63140: 2.7308276772499083\n",
      "loss at step 63160: 2.6395984053611756\n",
      "loss at step 63180: 2.833331882953644\n",
      "loss at step 63200: 2.7721664905548096\n",
      "validation loss: 2.835805810292562\n",
      "validation accuracy: 14.221619365609348%\n",
      "loss at step 63220: 2.7729517579078675\n",
      "loss at step 63240: 2.726175367832184\n",
      "loss at step 63260: 2.7101088643074034\n",
      "loss at step 63280: 2.725144600868225\n",
      "loss at step 63300: 2.7465566396713257\n",
      "loss at step 63320: 2.7036827564239503\n",
      "loss at step 63340: 2.8094403982162475\n",
      "loss at step 63360: 2.753007245063782\n",
      "loss at step 63380: 2.7350482821464537\n",
      "loss at step 63400: 2.761919856071472\n",
      "loss at step 63420: 2.746679449081421\n",
      "loss at step 63440: 2.729883944988251\n",
      "loss at step 63460: 2.7319177746772767\n",
      "loss at step 63480: 2.812592661380768\n",
      "loss at step 63500: 2.7646700739860535\n",
      "loss at step 63520: 2.7680829644203184\n",
      "loss at step 63540: 2.7592925429344177\n",
      "loss at step 63560: 2.7000255227088927\n",
      "loss at step 63580: 2.677380990982056\n",
      "loss at step 63600: 2.7703784942626952\n",
      "validation loss: 2.8321474027633666\n",
      "validation accuracy: 14.665066777963274%\n",
      "loss at step 63620: 2.7371791124343874\n",
      "loss at step 63640: 2.7877628564834596\n",
      "loss at step 63660: 2.630649518966675\n",
      "loss at step 63680: 2.656884229183197\n",
      "loss at step 63700: 2.74843293428421\n",
      "loss at step 63720: 2.800026845932007\n",
      "loss at step 63740: 2.5882640719413756\n",
      "loss at step 63760: 2.7239402651786806\n",
      "loss at step 63780: 2.6881356477737426\n",
      "loss at step 63800: 2.8202796578407288\n",
      "loss at step 63820: 2.627278172969818\n",
      "loss at step 63840: 2.652906835079193\n",
      "loss at step 63860: 2.6747175097465514\n",
      "loss at step 63880: 2.792191278934479\n",
      "loss at step 63900: 2.7861147284507752\n",
      "loss at step 63920: 2.6703621983528136\n",
      "loss at step 63940: 2.6591576099395753\n",
      "loss at step 63960: 2.837064099311829\n",
      "loss at step 63980: 2.7622549057006838\n",
      "loss at step 64000: 2.7670838475227355\n",
      "validation loss: 2.829767033259074\n",
      "validation accuracy: 14.753756260434056%\n",
      "loss at step 64020: 2.7515332460403443\n",
      "loss at step 64040: 2.6530880093574525\n",
      "loss at step 64060: 2.675208842754364\n",
      "loss at step 64080: 2.68654625415802\n",
      "loss at step 64100: 2.7227959632873535\n",
      "loss at step 64120: 2.7964603424072267\n",
      "loss at step 64140: 2.778374266624451\n",
      "loss at step 64160: 2.757517623901367\n",
      "loss at step 64180: 2.7890843987464904\n",
      "loss at step 64200: 2.7050031423568726\n",
      "loss at step 64220: 2.6872651100158693\n",
      "loss at step 64240: 2.763877034187317\n",
      "loss at step 64260: 2.7489432454109193\n",
      "loss at step 64280: 2.6922922253608705\n",
      "loss at step 64300: 2.753292989730835\n",
      "loss at step 64320: 2.7836114048957823\n",
      "loss at step 64340: 2.70882853269577\n",
      "loss at step 64360: 2.732341241836548\n",
      "loss at step 64380: 2.7019787430763245\n",
      "loss at step 64400: 2.723372554779053\n",
      "validation loss: 2.8488121175765992\n",
      "validation accuracy: 14.153797996661103%\n",
      "loss at step 64420: 2.837825298309326\n",
      "loss at step 64440: 2.774257242679596\n",
      "loss at step 64460: 2.7973370432853697\n",
      "loss at step 64480: 2.721824622154236\n",
      "loss at step 64500: 2.755164909362793\n",
      "loss at step 64520: 2.735126864910126\n",
      "loss at step 64540: 2.7337063193321227\n",
      "loss at step 64560: 2.7550255060195923\n",
      "loss at step 64580: 2.740941107273102\n",
      "loss at step 64600: 2.7342422127723696\n",
      "loss at step 64620: 2.724265396595001\n",
      "loss at step 64640: 2.682419788837433\n",
      "loss at step 64660: 2.7195308208465576\n",
      "loss at step 64680: 2.7993597865104674\n",
      "loss at step 64700: 2.7307612895965576\n",
      "loss at step 64720: 2.8013831853866575\n",
      "loss at step 64740: 2.735623562335968\n",
      "loss at step 64760: 2.7466700792312624\n",
      "loss at step 64780: 2.712808978557587\n",
      "loss at step 64800: 2.7339067220687867\n",
      "validation loss: 2.840482134819031\n",
      "validation accuracy: 13.762520868113523%\n",
      "loss at step 64820: 2.7473950028419494\n",
      "loss at step 64840: 2.7167381763458254\n",
      "loss at step 64860: 2.7834485173225403\n",
      "loss at step 64880: 2.6817952036857604\n",
      "loss at step 64900: 2.7567753553390504\n",
      "loss at step 64920: 2.7083379864692687\n",
      "loss at step 64940: 2.75054726600647\n",
      "loss at step 64960: 2.738910412788391\n",
      "loss at step 64980: 2.7127206087112428\n",
      "loss at step 65000: 2.7073665142059324\n",
      "loss at step 65020: 2.651435208320618\n",
      "loss at step 65040: 2.782267129421234\n",
      "loss at step 65060: 2.7001515865325927\n",
      "loss at step 65080: 2.760667014122009\n",
      "loss at step 65100: 2.7210395693778993\n",
      "loss at step 65120: 2.751566207408905\n",
      "loss at step 65140: 2.591065061092377\n",
      "loss at step 65160: 2.719625008106232\n",
      "loss at step 65180: 2.6802815079689024\n",
      "loss at step 65200: 2.768242561817169\n",
      "validation loss: 2.795767520268758\n",
      "validation accuracy: 15.176335559265441%\n",
      "loss at step 65220: 2.7042876958847044\n",
      "loss at step 65240: 2.7774450898170473\n",
      "loss at step 65260: 2.7239551663398744\n",
      "loss at step 65280: 2.7349403738975524\n",
      "loss at step 65300: 2.7532392024993895\n",
      "loss at step 65320: 2.7727814078330995\n",
      "loss at step 65340: 2.7309888362884522\n",
      "loss at step 65360: 2.718679356575012\n",
      "loss at step 65380: 2.74583740234375\n",
      "loss at step 65400: 2.702523350715637\n",
      "loss at step 65420: 2.8031538844108583\n",
      "loss at step 65440: 2.74037903547287\n",
      "loss at step 65460: 2.733539617061615\n",
      "loss at step 65480: 2.686292600631714\n",
      "loss at step 65500: 2.7537785887718202\n",
      "loss at step 65520: 2.7715170621871947\n",
      "loss at step 65540: 2.7397141814231873\n",
      "loss at step 65560: 2.6135749578475953\n",
      "loss at step 65580: 2.7922125577926638\n",
      "loss at step 65600: 2.7244834661483766\n",
      "validation loss: 2.8066953786214195\n",
      "validation accuracy: 15.066777963272122%\n",
      "loss at step 65620: 2.7031113386154173\n",
      "loss at step 65640: 2.731770956516266\n",
      "loss at step 65660: 2.7300551652908327\n",
      "loss at step 65680: 2.7550791263580323\n",
      "loss at step 65700: 2.711174201965332\n",
      "loss at step 65720: 2.798218870162964\n",
      "loss at step 65740: 2.700855624675751\n",
      "loss at step 65760: 2.6911643147468567\n",
      "loss at step 65780: 2.6903804659843447\n",
      "loss at step 65800: 2.689565122127533\n",
      "loss at step 65820: 2.827622926235199\n",
      "loss at step 65840: 2.7295417904853823\n",
      "loss at step 65860: 2.730902397632599\n",
      "loss at step 65880: 2.7062655687332153\n",
      "loss at step 65900: 2.7337515592575072\n",
      "loss at step 65920: 2.7380577683448792\n",
      "loss at step 65940: 2.728805589675903\n",
      "loss at step 65960: 2.699814295768738\n",
      "loss at step 65980: 2.7549678802490236\n",
      "loss at step 66000: 2.6870266675949095\n",
      "validation loss: 2.806535998980204\n",
      "validation accuracy: 14.899833055091818%\n",
      "loss at step 66020: 2.683998775482178\n",
      "loss at step 66040: 2.6937106013298036\n",
      "loss at step 66060: 2.7884817481040955\n",
      "loss at step 66080: 2.736304211616516\n",
      "loss at step 66100: 2.7408923745155334\n",
      "loss at step 66120: 2.854982388019562\n",
      "loss at step 66140: 2.764731526374817\n",
      "loss at step 66160: 2.6536847352981567\n",
      "loss at step 66180: 2.7754685997962953\n",
      "loss at step 66200: 2.6816965341567993\n",
      "loss at step 66220: 2.684537708759308\n",
      "loss at step 66240: 2.6938812494277955\n",
      "loss at step 66260: 2.742537426948547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at step 66280: 2.683622121810913\n",
      "loss at step 66300: 2.745735454559326\n",
      "loss at step 66320: 2.719000542163849\n",
      "loss at step 66340: 2.6693244218826293\n",
      "loss at step 66360: 2.715834951400757\n",
      "loss at step 66380: 2.7228469014167787\n",
      "loss at step 66400: 2.6910349249839784\n",
      "validation loss: 2.8239078839619953\n",
      "validation accuracy: 14.456385642737896%\n",
      "loss at step 66420: 2.8501644372940063\n",
      "loss at step 66440: 2.7390034914016725\n",
      "loss at step 66460: 2.7048932790756224\n",
      "loss at step 66480: 2.736556112766266\n",
      "loss at step 66500: 2.6898212313652037\n",
      "loss at step 66520: 2.615202176570892\n",
      "loss at step 66540: 2.760747218132019\n",
      "loss at step 66560: 2.7362848043441774\n",
      "loss at step 66580: 2.761297607421875\n",
      "loss at step 66600: 2.7430568814277647\n",
      "loss at step 66620: 2.673439145088196\n",
      "loss at step 66640: 2.658943462371826\n",
      "loss at step 66660: 2.8573553562164307\n",
      "loss at step 66680: 2.6952586650848387\n",
      "loss at step 66700: 2.7244382739067077\n",
      "loss at step 66720: 2.7591853499412538\n",
      "loss at step 66740: 2.715910542011261\n",
      "loss at step 66760: 2.7265052914619448\n",
      "loss at step 66780: 2.723500430583954\n",
      "loss at step 66800: 2.766363334655762\n",
      "validation loss: 2.8214211599032084\n",
      "validation accuracy: 14.425083472454089%\n",
      "loss at step 66820: 2.711891937255859\n",
      "loss at step 66840: 2.784374952316284\n",
      "loss at step 66860: 2.7740045189857483\n",
      "loss at step 66880: 2.7807467222213744\n",
      "loss at step 66900: 2.7168120980262755\n",
      "loss at step 66920: 2.7063664317131044\n",
      "loss at step 66940: 2.6956045150756838\n",
      "loss at step 66960: 2.7982022881507875\n",
      "loss at step 66980: 2.682286822795868\n",
      "loss at step 67000: 2.697176623344421\n",
      "loss at step 67020: 2.7380019187927247\n",
      "loss at step 67040: 2.730949914455414\n",
      "loss at step 67060: 2.747335064411163\n",
      "loss at step 67080: 2.729193425178528\n",
      "loss at step 67100: 2.7294283390045164\n",
      "loss at step 67120: 2.6613389611244203\n",
      "loss at step 67140: 2.692430651187897\n",
      "loss at step 67160: 2.739078903198242\n",
      "loss at step 67180: 2.6884921073913572\n",
      "loss at step 67200: 2.7266295552253723\n",
      "validation loss: 2.78822891553243\n",
      "validation accuracy: 14.894616026711185%\n",
      "loss at step 67220: 2.7102179527282715\n",
      "loss at step 67240: 2.6926488041877747\n",
      "loss at step 67260: 2.732034695148468\n",
      "loss at step 67280: 2.6818748831748964\n",
      "loss at step 67300: 2.664404881000519\n",
      "loss at step 67320: 2.7742757320404055\n",
      "loss at step 67340: 2.843253564834595\n",
      "loss at step 67360: 2.815495324134827\n",
      "loss at step 67380: 2.6873478651046754\n",
      "loss at step 67400: 2.825403130054474\n",
      "loss at step 67420: 2.696922075748444\n",
      "loss at step 67440: 2.6856154561042787\n",
      "loss at step 67460: 2.72739520072937\n",
      "loss at step 67480: 2.6997287034988404\n",
      "loss at step 67500: 2.7168305277824403\n",
      "loss at step 67520: 2.747554099559784\n",
      "loss at step 67540: 2.729821300506592\n",
      "loss at step 67560: 2.706329572200775\n",
      "loss at step 67580: 2.747048223018646\n",
      "loss at step 67600: 2.762600529193878\n",
      "validation loss: 2.8105348110198975\n",
      "validation accuracy: 14.513772954924875%\n",
      "loss at step 67620: 2.7109211921691894\n",
      "loss at step 67640: 2.5910220623016356\n",
      "loss at step 67660: 2.6272969245910645\n",
      "loss at step 67680: 2.7599954485893248\n",
      "loss at step 67700: 2.651369273662567\n",
      "loss at step 67720: 2.649281072616577\n",
      "loss at step 67740: 2.7283363938331604\n",
      "loss at step 67760: 2.79845906496048\n",
      "loss at step 67780: 2.8184451699256896\n",
      "loss at step 67800: 2.735468256473541\n",
      "loss at step 67820: 2.7120744585990906\n",
      "loss at step 67840: 2.785115969181061\n",
      "loss at step 67860: 2.7340809106826782\n",
      "loss at step 67880: 2.716308856010437\n",
      "loss at step 67900: 2.6706472754478456\n",
      "loss at step 67920: 2.7531341552734374\n",
      "loss at step 67940: 2.738097333908081\n",
      "loss at step 67960: 2.6520194172859193\n",
      "loss at step 67980: 2.678645575046539\n",
      "loss at step 68000: 2.789751350879669\n",
      "validation loss: 2.811989903450012\n",
      "validation accuracy: 14.62854757929883%\n",
      "loss at step 68020: 2.6568779826164244\n",
      "loss at step 68040: 2.6748347997665407\n",
      "loss at step 68060: 2.696945309638977\n",
      "loss at step 68080: 2.727567362785339\n",
      "loss at step 68100: 2.8306736946105957\n",
      "loss at step 68120: 2.7418299436569216\n",
      "loss at step 68140: 2.737257242202759\n",
      "loss at step 68160: 2.72562837600708\n",
      "loss at step 68180: 2.68207243680954\n",
      "loss at step 68200: 2.714480924606323\n",
      "loss at step 68220: 2.702861285209656\n",
      "loss at step 68240: 2.75667427778244\n",
      "loss at step 68260: 2.6892066836357116\n",
      "loss at step 68280: 2.788697636127472\n",
      "loss at step 68300: 2.6618424534797667\n",
      "loss at step 68320: 2.748570907115936\n",
      "loss at step 68340: 2.7818229675292967\n",
      "loss at step 68360: 2.709855115413666\n",
      "loss at step 68380: 2.6557513952255247\n",
      "loss at step 68400: 2.73877534866333\n",
      "validation loss: 2.8081572834650674\n",
      "validation accuracy: 14.518989983305508%\n",
      "loss at step 68420: 2.619810390472412\n",
      "loss at step 68440: 2.7947202205657957\n",
      "loss at step 68460: 2.73217134475708\n",
      "loss at step 68480: 2.6268277049064634\n",
      "loss at step 68500: 2.647812879085541\n",
      "loss at step 68520: 2.6761611223220827\n",
      "loss at step 68540: 2.7693350672721864\n",
      "loss at step 68560: 2.783094561100006\n",
      "loss at step 68580: 2.6578449249267577\n",
      "loss at step 68600: 2.6983231782913206\n",
      "loss at step 68620: 2.7392595648765563\n",
      "loss at step 68640: 2.7383506059646607\n",
      "loss at step 68660: 2.6831447958946226\n",
      "loss at step 68680: 2.7406521320343016\n",
      "loss at step 68700: 2.658949315547943\n",
      "loss at step 68720: 2.7033642411231993\n",
      "loss at step 68740: 2.7098929047584535\n",
      "loss at step 68760: 2.679269325733185\n",
      "loss at step 68780: 2.6685161352157594\n",
      "loss at step 68800: 2.7691985607147216\n",
      "validation loss: 2.8349174801508585\n",
      "validation accuracy: 14.889398998330549%\n",
      "loss at step 68820: 2.6674423456192016\n",
      "loss at step 68840: 2.654093098640442\n",
      "loss at step 68860: 2.6549272775650024\n",
      "loss at step 68880: 2.726705586910248\n",
      "loss at step 68900: 2.75122754573822\n",
      "loss at step 68920: 2.699980008602142\n",
      "loss at step 68940: 2.7618501782417297\n",
      "loss at step 68960: 2.6699419617652893\n",
      "loss at step 68980: 2.757819926738739\n",
      "loss at step 69000: 2.722048509120941\n",
      "loss at step 69020: 2.71308810710907\n",
      "loss at step 69040: 2.6626145005226136\n",
      "loss at step 69060: 2.697283375263214\n",
      "loss at step 69080: 2.6803089499473574\n",
      "loss at step 69100: 2.741413927078247\n",
      "loss at step 69120: 2.72788553237915\n",
      "loss at step 69140: 2.7292433619499206\n",
      "loss at step 69160: 2.7493502497673035\n",
      "loss at step 69180: 2.728806662559509\n",
      "loss at step 69200: 2.74229474067688\n",
      "validation loss: 2.8136796673138935\n",
      "validation accuracy: 14.46160267111853%\n",
      "loss at step 69220: 2.6986851930618285\n",
      "loss at step 69240: 2.703294241428375\n",
      "loss at step 69260: 2.7008259534835815\n",
      "loss at step 69280: 2.6460787534713743\n",
      "loss at step 69300: 2.7458715200424195\n",
      "loss at step 69320: 2.7804275155067444\n",
      "loss at step 69340: 2.6814660310745237\n",
      "loss at step 69360: 2.7125344395637514\n",
      "loss at step 69380: 2.707428455352783\n",
      "loss at step 69400: 2.733753037452698\n",
      "loss at step 69420: 2.7442386150360107\n",
      "loss at step 69440: 2.652943468093872\n",
      "loss at step 69460: 2.7473795890808104\n",
      "loss at step 69480: 2.710187923908234\n",
      "loss at step 69500: 2.6475529432296754\n",
      "loss at step 69520: 2.6855668902397154\n",
      "loss at step 69540: 2.733658158779144\n",
      "loss at step 69560: 2.7235989212989806\n",
      "loss at step 69580: 2.632349705696106\n",
      "loss at step 69600: 2.7431633830070496\n",
      "validation loss: 2.8100482654571532\n",
      "validation accuracy: 14.910267111853088%\n",
      "loss at step 69620: 2.7411988735198975\n",
      "loss at step 69640: 2.7476146936416628\n",
      "loss at step 69660: 2.630417013168335\n",
      "loss at step 69680: 2.781264090538025\n",
      "loss at step 69700: 2.786075675487518\n",
      "loss at step 69720: 2.7636484503746033\n",
      "loss at step 69740: 2.7805830001831056\n",
      "loss at step 69760: 2.7054759740829466\n",
      "loss at step 69780: 2.6983456373214723\n",
      "loss at step 69800: 2.589800465106964\n",
      "loss at step 69820: 2.716184175014496\n",
      "loss at step 69840: 2.6714547157287596\n",
      "loss at step 69860: 2.7469990611076356\n",
      "loss at step 69880: 2.7405641913414\n",
      "loss at step 69900: 2.7552187561988832\n",
      "loss at step 69920: 2.6943319916725157\n",
      "loss at step 69940: 2.7281771063804627\n",
      "loss at step 69960: 2.7468153834342957\n",
      "loss at step 69980: 2.683527910709381\n",
      "loss at step 70000: 2.6740485548973085\n",
      "validation loss: 2.799098346233368\n",
      "validation accuracy: 14.680717863105174%\n",
      "loss at step 70020: 2.7449235200881956\n",
      "loss at step 70040: 2.8180225491523743\n",
      "loss at step 70060: 2.704353165626526\n",
      "loss at step 70080: 2.675236427783966\n",
      "loss at step 70100: 2.7539702653884888\n",
      "loss at step 70120: 2.775426506996155\n",
      "loss at step 70140: 2.673861837387085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at step 70160: 2.66344039440155\n",
      "loss at step 70180: 2.68296355009079\n",
      "loss at step 70200: 2.641629385948181\n",
      "loss at step 70220: 2.7020328521728514\n",
      "loss at step 70240: 2.7160440325737\n",
      "loss at step 70260: 2.755428671836853\n",
      "loss at step 70280: 2.6769171595573424\n",
      "loss at step 70300: 2.6899892687797546\n",
      "loss at step 70320: 2.753960978984833\n",
      "loss at step 70340: 2.660026800632477\n",
      "loss at step 70360: 2.5964502811431887\n",
      "loss at step 70380: 2.741564762592316\n",
      "loss at step 70400: 2.699671483039856\n",
      "validation loss: 2.785056522687276\n",
      "validation accuracy: 15.087646076794659%\n",
      "loss at step 70420: 2.7176315069198607\n",
      "loss at step 70440: 2.742523527145386\n",
      "loss at step 70460: 2.6984883904457093\n",
      "loss at step 70480: 2.7235883474349976\n",
      "loss at step 70500: 2.734596681594849\n",
      "loss at step 70520: 2.7392531275749206\n",
      "loss at step 70540: 2.709845983982086\n",
      "loss at step 70560: 2.7243436217308044\n",
      "loss at step 70580: 2.7171441435813906\n",
      "loss at step 70600: 2.7232242226600647\n",
      "loss at step 70620: 2.647429716587067\n",
      "loss at step 70640: 2.776351714134216\n",
      "loss at step 70660: 2.7474828720092774\n",
      "loss at step 70680: 2.73893027305603\n",
      "loss at step 70700: 2.680235576629639\n",
      "loss at step 70720: 2.7493783950805666\n",
      "loss at step 70740: 2.774401092529297\n",
      "loss at step 70760: 2.663031077384949\n",
      "loss at step 70780: 2.7396947383880614\n",
      "loss at step 70800: 2.651890742778778\n",
      "validation loss: 2.81328032652537\n",
      "validation accuracy: 14.64941569282137%\n",
      "loss at step 70820: 2.7546178698539734\n",
      "loss at step 70840: 2.7724278450012205\n",
      "loss at step 70860: 2.7914867401123047\n",
      "loss at step 70880: 2.7593004941940307\n",
      "loss at step 70900: 2.7506704926490784\n",
      "loss at step 70920: 2.6368528962135316\n",
      "loss at step 70940: 2.696850764751434\n",
      "loss at step 70960: 2.73393828868866\n",
      "loss at step 70980: 2.7493889212608336\n",
      "loss at step 71000: 2.7569407343864443\n",
      "loss at step 71020: 2.7560463309288026\n",
      "loss at step 71040: 2.6786917567253115\n",
      "loss at step 71060: 2.7363911271095276\n",
      "loss at step 71080: 2.661274456977844\n",
      "loss at step 71100: 2.7427258014678957\n",
      "loss at step 71120: 2.6486937761306764\n",
      "loss at step 71140: 2.6523961544036867\n",
      "loss at step 71160: 2.661572074890137\n",
      "loss at step 71180: 2.8137300610542297\n",
      "loss at step 71200: 2.7094601154327393\n",
      "validation loss: 2.782202881971995\n",
      "validation accuracy: 14.94678631051753%\n",
      "loss at step 71220: 2.687570834159851\n",
      "loss at step 71240: 2.826192331314087\n",
      "loss at step 71260: 2.7603504776954653\n",
      "loss at step 71280: 2.7262362003326417\n",
      "loss at step 71300: 2.6811731576919557\n",
      "loss at step 71320: 2.7241868495941164\n",
      "loss at step 71340: 2.6893272161483766\n",
      "loss at step 71360: 2.8004260659217834\n",
      "loss at step 71380: 2.682280158996582\n",
      "loss at step 71400: 2.6508466005325317\n",
      "loss at step 71420: 2.7213533401489256\n",
      "loss at step 71440: 2.7305660963058473\n",
      "loss at step 71460: 2.7461105942726136\n",
      "loss at step 71480: 2.7262118816375733\n",
      "loss at step 71500: 2.6357888102531435\n",
      "loss at step 71520: 2.774958860874176\n",
      "loss at step 71540: 2.680686855316162\n",
      "loss at step 71560: 2.7163615584373475\n",
      "loss at step 71580: 2.674128520488739\n",
      "loss at step 71600: 2.7649348974227905\n",
      "validation loss: 2.7700387700398763\n",
      "validation accuracy: 15.108514190317196%\n",
      "loss at step 71620: 2.689327561855316\n",
      "loss at step 71640: 2.7521745920181275\n",
      "loss at step 71660: 2.7619568705558777\n",
      "loss at step 71680: 2.643771839141846\n",
      "loss at step 71700: 2.6893676638603212\n",
      "loss at step 71720: 2.6776324272155763\n",
      "loss at step 71740: 2.71091468334198\n",
      "loss at step 71760: 2.7223400354385374\n",
      "loss at step 71780: 2.7558814764022825\n",
      "loss at step 71800: 2.678193008899689\n",
      "loss at step 71820: 2.7170727610588075\n",
      "loss at step 71840: 2.694865846633911\n",
      "loss at step 71860: 2.760711681842804\n",
      "loss at step 71880: 2.7108875274658204\n",
      "loss at step 71900: 2.642151427268982\n",
      "loss at step 71920: 2.6508891224861144\n",
      "loss at step 71940: 2.6749499440193176\n",
      "loss at step 71960: 2.6139360785484316\n",
      "loss at step 71980: 2.7474159479141234\n",
      "loss at step 72000: 2.6452192425727845\n",
      "validation loss: 2.788507817586263\n",
      "validation accuracy: 14.81636060100167%\n",
      "loss at step 72020: 2.6739784359931944\n",
      "loss at step 72040: 2.753241741657257\n",
      "loss at step 72060: 2.74020231962204\n",
      "loss at step 72080: 2.737259364128113\n",
      "loss at step 72100: 2.73312486410141\n",
      "loss at step 72120: 2.740959179401398\n",
      "loss at step 72140: 2.775155794620514\n",
      "loss at step 72160: 2.764151191711426\n",
      "loss at step 72180: 2.672412395477295\n",
      "loss at step 72200: 2.6298654079437256\n",
      "loss at step 72220: 2.7096765995025636\n",
      "loss at step 72240: 2.7740439891815187\n",
      "loss at step 72260: 2.695710813999176\n",
      "loss at step 72280: 2.6414540648460387\n",
      "loss at step 72300: 2.7037740111351014\n",
      "loss at step 72320: 2.7123819947242738\n",
      "loss at step 72340: 2.618135726451874\n",
      "loss at step 72360: 2.745452094078064\n",
      "loss at step 72380: 2.741134595870972\n",
      "loss at step 72400: 2.617374134063721\n",
      "validation loss: 2.7872269439697264\n",
      "validation accuracy: 14.983305509181971%\n",
      "loss at step 72420: 2.666411316394806\n",
      "loss at step 72440: 2.859536337852478\n",
      "loss at step 72460: 2.756929135322571\n",
      "loss at step 72480: 2.6953552842140196\n",
      "loss at step 72500: 2.6889493346214293\n",
      "loss at step 72520: 2.697362208366394\n",
      "loss at step 72540: 2.675302231311798\n",
      "loss at step 72560: 2.687197756767273\n",
      "loss at step 72580: 2.7449684023857115\n",
      "loss at step 72600: 2.669792079925537\n",
      "loss at step 72620: 2.652315640449524\n",
      "loss at step 72640: 2.707306241989136\n",
      "loss at step 72660: 2.6980511426925657\n",
      "loss at step 72680: 2.7242639541625975\n",
      "loss at step 72700: 2.7190401434898375\n",
      "loss at step 72720: 2.7493410110473633\n",
      "loss at step 72740: 2.7408867716789245\n",
      "loss at step 72760: 2.729003882408142\n",
      "loss at step 72780: 2.6984182596206665\n",
      "loss at step 72800: 2.757373344898224\n",
      "validation loss: 2.8153963152567547\n",
      "validation accuracy: 14.701585976627712%\n",
      "loss at step 72820: 2.73688542842865\n",
      "loss at step 72840: 2.7522074341773988\n",
      "loss at step 72860: 2.670569384098053\n",
      "loss at step 72880: 2.7060422778129576\n",
      "loss at step 72900: 2.701774740219116\n",
      "loss at step 72920: 2.825478982925415\n",
      "loss at step 72940: 2.776409995555878\n",
      "loss at step 72960: 2.7256421089172362\n",
      "loss at step 72980: 2.692459213733673\n",
      "loss at step 73000: 2.6669257402420046\n",
      "loss at step 73020: 2.7189624309539795\n",
      "loss at step 73040: 2.7211177825927733\n",
      "loss at step 73060: 2.7208883285522463\n",
      "loss at step 73080: 2.6929322481155396\n",
      "loss at step 73100: 2.7467883706092833\n",
      "loss at step 73120: 2.6380192637443542\n",
      "loss at step 73140: 2.7780810832977294\n",
      "loss at step 73160: 2.753740072250366\n",
      "loss at step 73180: 2.679991805553436\n",
      "loss at step 73200: 2.6665103316307066\n",
      "validation loss: 2.8433294614156086\n",
      "validation accuracy: 14.09119365609349%\n",
      "loss at step 73220: 2.699060380458832\n",
      "loss at step 73240: 2.721734309196472\n",
      "loss at step 73260: 2.768301320075989\n",
      "loss at step 73280: 2.7057411789894106\n",
      "loss at step 73300: 2.6854750752449035\n",
      "loss at step 73320: 2.7207929253578187\n",
      "loss at step 73340: 2.6510472893714905\n",
      "loss at step 73360: 2.7729928970336912\n",
      "loss at step 73380: 2.741111898422241\n",
      "loss at step 73400: 2.7794140696525576\n",
      "loss at step 73420: 2.7636398911476134\n",
      "loss at step 73440: 2.6798375844955444\n",
      "loss at step 73460: 2.656494069099426\n",
      "loss at step 73480: 2.6795739769935607\n",
      "loss at step 73500: 2.8347069382667542\n",
      "loss at step 73520: 2.72997190952301\n",
      "loss at step 73540: 2.6735458731651307\n",
      "loss at step 73560: 2.729005777835846\n",
      "loss at step 73580: 2.6987605571746824\n",
      "loss at step 73600: 2.7051268815994263\n",
      "validation loss: 2.808657778104146\n",
      "validation accuracy: 14.72245409015025%\n",
      "loss at step 73620: 2.6846657395362854\n",
      "loss at step 73640: 2.685099995136261\n",
      "loss at step 73660: 2.6255541026592253\n",
      "loss at step 73680: 2.686852216720581\n",
      "loss at step 73700: 2.721357262134552\n",
      "loss at step 73720: 2.6172260880470275\n",
      "loss at step 73740: 2.633565366268158\n",
      "loss at step 73760: 2.691048800945282\n",
      "loss at step 73780: 2.6972538948059084\n",
      "loss at step 73800: 2.7702234625816344\n",
      "loss at step 73820: 2.6890961050987245\n",
      "loss at step 73840: 2.699773037433624\n",
      "loss at step 73860: 2.691024422645569\n",
      "loss at step 73880: 2.6712097048759462\n",
      "loss at step 73900: 2.662577986717224\n",
      "loss at step 73920: 2.697163701057434\n",
      "loss at step 73940: 2.7167441964149477\n",
      "loss at step 73960: 2.661317324638367\n",
      "loss at step 73980: 2.7297333002090456\n",
      "loss at step 74000: 2.707938075065613\n",
      "validation loss: 2.819104126294454\n",
      "validation accuracy: 14.33117696160267%\n",
      "loss at step 74020: 2.7280502676963807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at step 74040: 2.7906698107719423\n",
      "loss at step 74060: 2.693540632724762\n",
      "loss at step 74080: 2.7451900601387025\n",
      "loss at step 74100: 2.669507610797882\n",
      "loss at step 74120: 2.732143783569336\n",
      "loss at step 74140: 2.7503705620765686\n",
      "loss at step 74160: 2.63346072435379\n",
      "loss at step 74180: 2.743454360961914\n",
      "loss at step 74200: 2.661134052276611\n",
      "loss at step 74220: 2.6852021336555483\n",
      "loss at step 74240: 2.7577125906944273\n",
      "loss at step 74260: 2.7353310227394103\n",
      "loss at step 74280: 2.7367393136024476\n",
      "loss at step 74300: 2.7299183011054993\n",
      "loss at step 74320: 2.7396024227142335\n",
      "loss at step 74340: 2.7395118713378905\n",
      "loss at step 74360: 2.6789526104927064\n",
      "loss at step 74380: 2.748567795753479\n",
      "loss at step 74400: 2.7364168524742127\n",
      "validation loss: 2.7966662502288817\n",
      "validation accuracy: 15.27024207011686%\n",
      "loss at step 74420: 2.7225094079971313\n",
      "loss at step 74440: 2.8276709079742433\n",
      "loss at step 74460: 2.6744786381721495\n",
      "loss at step 74480: 2.761888802051544\n",
      "loss at step 74500: 2.7836359977722167\n",
      "loss at step 74520: 2.6697599053382874\n",
      "loss at step 74540: 2.77215895652771\n",
      "loss at step 74560: 2.763933074474335\n",
      "loss at step 74580: 2.7300875067710875\n",
      "loss at step 74600: 2.6419604897499083\n",
      "loss at step 74620: 2.747483456134796\n",
      "loss at step 74640: 2.7306474566459658\n",
      "loss at step 74660: 2.6877929091453554\n",
      "loss at step 74680: 2.8077083826065063\n",
      "loss at step 74700: 2.7339348077774046\n",
      "loss at step 74720: 2.719842529296875\n",
      "loss at step 74740: 2.7542720556259157\n",
      "loss at step 74760: 2.7264438390731813\n",
      "loss at step 74780: 2.689163553714752\n",
      "epoch 1\n",
      "loss at step 74800: 2.717601144313812\n",
      "validation loss: 2.779882530371348\n",
      "validation accuracy: 14.941569282136896%\n",
      "loss at step 74820: 2.6725493907928466\n",
      "loss at step 74840: 2.7117486953735352\n",
      "loss at step 74860: 2.65308563709259\n",
      "loss at step 74880: 2.6743906021118162\n",
      "loss at step 74900: 2.704950547218323\n",
      "loss at step 74920: 2.763411259651184\n",
      "loss at step 74940: 2.667123019695282\n",
      "loss at step 74960: 2.708503246307373\n",
      "loss at step 74980: 2.561859667301178\n",
      "loss at step 75000: 2.7180677890777587\n",
      "loss at step 75020: 2.7181761384010317\n",
      "loss at step 75040: 2.626705503463745\n",
      "loss at step 75060: 2.6982664942741392\n",
      "loss at step 75080: 2.7251550912857057\n",
      "loss at step 75100: 2.6703065991401673\n",
      "loss at step 75120: 2.703950297832489\n",
      "loss at step 75140: 2.6882933735847474\n",
      "loss at step 75160: 2.651162266731262\n",
      "loss at step 75180: 2.803410220146179\n",
      "loss at step 75200: 2.661754882335663\n",
      "validation loss: 2.778109974861145\n",
      "validation accuracy: 15.171118530884808%\n",
      "loss at step 75220: 2.650687909126282\n",
      "loss at step 75240: 2.711516773700714\n",
      "loss at step 75260: 2.6468170642852784\n",
      "loss at step 75280: 2.6536317110061645\n",
      "loss at step 75300: 2.699068915843964\n",
      "loss at step 75320: 2.650407862663269\n",
      "loss at step 75340: 2.6477598309516908\n",
      "loss at step 75360: 2.7567867994308473\n",
      "loss at step 75380: 2.726099359989166\n",
      "loss at step 75400: 2.7264657974243165\n",
      "loss at step 75420: 2.719506824016571\n",
      "loss at step 75440: 2.6624986052513124\n",
      "loss at step 75460: 2.674836003780365\n",
      "loss at step 75480: 2.652404987812042\n",
      "loss at step 75500: 2.666016924381256\n",
      "loss at step 75520: 2.655219924449921\n",
      "loss at step 75540: 2.6334923028945925\n",
      "loss at step 75560: 2.7224663615226747\n",
      "loss at step 75580: 2.75281343460083\n",
      "loss at step 75600: 2.6470543503761292\n",
      "validation loss: 2.811325658162435\n",
      "validation accuracy: 15.18676961602671%\n",
      "loss at step 75620: 2.7306814074516295\n",
      "loss at step 75640: 2.7445897459983826\n",
      "loss at step 75660: 2.689032793045044\n",
      "loss at step 75680: 2.7576904296875\n",
      "loss at step 75700: 2.7145540595054625\n",
      "loss at step 75720: 2.694617784023285\n",
      "loss at step 75740: 2.7233695507049562\n",
      "loss at step 75760: 2.7529709815979\n",
      "loss at step 75780: 2.733163797855377\n",
      "loss at step 75800: 2.6340455651283263\n",
      "loss at step 75820: 2.726991260051727\n",
      "loss at step 75840: 2.80302654504776\n",
      "loss at step 75860: 2.6565532445907594\n",
      "loss at step 75880: 2.6703348040580748\n",
      "loss at step 75900: 2.6523645758628844\n",
      "loss at step 75920: 2.7557074666023254\n",
      "loss at step 75940: 2.7308491230010987\n",
      "loss at step 75960: 2.66963152885437\n",
      "loss at step 75980: 2.6297442436218263\n",
      "loss at step 76000: 2.760279130935669\n",
      "validation loss: 2.8062662076950073\n",
      "validation accuracy: 14.492904841402337%\n",
      "loss at step 76020: 2.712662100791931\n",
      "loss at step 76040: 2.7900035858154295\n",
      "loss at step 76060: 2.733705699443817\n",
      "loss at step 76080: 2.6036380887031556\n",
      "loss at step 76100: 2.6845564246177673\n",
      "loss at step 76120: 2.7277752995491027\n",
      "loss at step 76140: 2.668500065803528\n",
      "loss at step 76160: 2.7501678705215453\n",
      "loss at step 76180: 2.6913180589675902\n",
      "loss at step 76200: 2.719146120548248\n",
      "loss at step 76220: 2.6484255194664\n",
      "loss at step 76240: 2.655258810520172\n",
      "loss at step 76260: 2.743640434741974\n",
      "loss at step 76280: 2.7409263610839845\n",
      "loss at step 76300: 2.786471724510193\n",
      "loss at step 76320: 2.7134909510612486\n",
      "loss at step 76340: 2.708077049255371\n",
      "loss at step 76360: 2.6664568424224853\n",
      "loss at step 76380: 2.7203514695167543\n",
      "loss at step 76400: 2.7229064106941223\n",
      "validation loss: 2.8306847103436787\n",
      "validation accuracy: 14.018155258764608%\n",
      "loss at step 76420: 2.675506126880646\n",
      "loss at step 76440: 2.6980037450790406\n",
      "loss at step 76460: 2.726436460018158\n",
      "loss at step 76480: 2.7415217399597167\n",
      "loss at step 76500: 2.7601223826408385\n",
      "loss at step 76520: 2.764651727676392\n",
      "loss at step 76540: 2.6615313053131104\n",
      "loss at step 76560: 2.697487449645996\n",
      "loss at step 76580: 2.6893436551094054\n",
      "loss at step 76600: 2.79491366147995\n",
      "loss at step 76620: 2.711420547962189\n",
      "loss at step 76640: 2.677944779396057\n",
      "loss at step 76660: 2.6888072013854982\n",
      "loss at step 76680: 2.6125768065452575\n",
      "loss at step 76700: 2.678019344806671\n",
      "loss at step 76720: 2.7593199968338014\n",
      "loss at step 76740: 2.6986128091812134\n",
      "loss at step 76760: 2.708851170539856\n",
      "loss at step 76780: 2.661170184612274\n",
      "loss at step 76800: 2.651817727088928\n",
      "validation loss: 2.80668962319692\n",
      "validation accuracy: 14.6389816360601%\n",
      "loss at step 76820: 2.63119136095047\n",
      "loss at step 76840: 2.678864765167236\n",
      "loss at step 76860: 2.6569074988365173\n",
      "loss at step 76880: 2.704550802707672\n",
      "loss at step 76900: 2.7489016175270082\n",
      "loss at step 76920: 2.6707290172576905\n",
      "loss at step 76940: 2.715006422996521\n",
      "loss at step 76960: 2.728745687007904\n",
      "loss at step 76980: 2.68060462474823\n",
      "loss at step 77000: 2.674658727645874\n",
      "loss at step 77020: 2.608906054496765\n",
      "loss at step 77040: 2.6707627892494203\n",
      "loss at step 77060: 2.8187260031700134\n",
      "loss at step 77080: 2.6532682180404663\n",
      "loss at step 77100: 2.718753981590271\n",
      "loss at step 77120: 2.6384516596794128\n",
      "loss at step 77140: 2.7167680859565735\n",
      "loss at step 77160: 2.732557940483093\n",
      "loss at step 77180: 2.7604321479797362\n",
      "loss at step 77200: 2.7075845122337343\n",
      "validation loss: 2.778334678808848\n",
      "validation accuracy: 14.962437395659434%\n",
      "loss at step 77220: 2.6632995128631594\n",
      "loss at step 77240: 2.7059087991714477\n",
      "loss at step 77260: 2.6870299935340882\n",
      "loss at step 77280: 2.6745537519454956\n",
      "loss at step 77300: 2.6866878867149353\n",
      "loss at step 77320: 2.691093111038208\n",
      "loss at step 77340: 2.6267136693000794\n",
      "loss at step 77360: 2.715394127368927\n",
      "loss at step 77380: 2.7854729652404786\n",
      "loss at step 77400: 2.7394907474517822\n",
      "loss at step 77420: 2.7075889825820925\n",
      "loss at step 77440: 2.705878531932831\n",
      "loss at step 77460: 2.6772257685661316\n",
      "loss at step 77480: 2.727345311641693\n",
      "loss at step 77500: 2.720041310787201\n",
      "loss at step 77520: 2.700018525123596\n",
      "loss at step 77540: 2.67971647977829\n",
      "loss at step 77560: 2.7028462648391725\n",
      "loss at step 77580: 2.6858765482902527\n",
      "loss at step 77600: 2.702770972251892\n",
      "validation loss: 2.781599842707316\n",
      "validation accuracy: 15.18676961602671%\n",
      "loss at step 77620: 2.7108356595039367\n",
      "loss at step 77640: 2.699186098575592\n",
      "loss at step 77660: 2.660201668739319\n",
      "loss at step 77680: 2.668845272064209\n",
      "loss at step 77700: 2.681292474269867\n",
      "loss at step 77720: 2.6919917464256287\n",
      "loss at step 77740: 2.6782934784889223\n",
      "loss at step 77760: 2.7081195950508117\n",
      "loss at step 77780: 2.6510156869888304\n",
      "loss at step 77800: 2.7209216594696044\n",
      "loss at step 77820: 2.676776444911957\n",
      "loss at step 77840: 2.6077096819877625\n",
      "loss at step 77860: 2.6382077813148497\n",
      "loss at step 77880: 2.7175253868103026\n",
      "loss at step 77900: 2.6831315875053408\n",
      "loss at step 77920: 2.7303302764892576\n",
      "loss at step 77940: 2.7009657502174376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at step 77960: 2.6565163016319273\n",
      "loss at step 77980: 2.6262449383735658\n",
      "loss at step 78000: 2.64651243686676\n",
      "validation loss: 2.8214937893549603\n",
      "validation accuracy: 14.560726210350584%\n",
      "loss at step 78020: 2.6724722504615785\n",
      "loss at step 78040: 2.6888559103012084\n",
      "loss at step 78060: 2.7622055768966676\n",
      "loss at step 78080: 2.6918702244758608\n",
      "loss at step 78100: 2.727749490737915\n",
      "loss at step 78120: 2.6655391693115233\n",
      "loss at step 78140: 2.758989930152893\n",
      "loss at step 78160: 2.715466928482056\n",
      "loss at step 78180: 2.7344421267509462\n",
      "loss at step 78200: 2.7127715587615966\n",
      "loss at step 78220: 2.706951367855072\n",
      "loss at step 78240: 2.6767958283424376\n",
      "loss at step 78260: 2.642867422103882\n",
      "loss at step 78280: 2.6565951466560365\n",
      "loss at step 78300: 2.664384496212006\n",
      "loss at step 78320: 2.6224735140800477\n",
      "loss at step 78340: 2.6470692634582518\n",
      "loss at step 78360: 2.6815388202667236\n",
      "loss at step 78380: 2.7179011344909667\n",
      "loss at step 78400: 2.704615664482117\n",
      "validation loss: 2.797864165306091\n",
      "validation accuracy: 14.952003338898164%\n",
      "loss at step 78420: 2.6929054379463198\n",
      "loss at step 78440: 2.6336499094963073\n",
      "loss at step 78460: 2.7539429903030395\n",
      "loss at step 78480: 2.6975986838340758\n",
      "loss at step 78500: 2.702742350101471\n",
      "loss at step 78520: 2.76746084690094\n",
      "loss at step 78540: 2.678136098384857\n",
      "loss at step 78560: 2.679794228076935\n",
      "loss at step 78580: 2.776941108703613\n",
      "loss at step 78600: 2.6602694988250732\n",
      "loss at step 78620: 2.6252403140068052\n",
      "loss at step 78640: 2.6791818499565125\n",
      "loss at step 78660: 2.681412708759308\n",
      "loss at step 78680: 2.7079811096191406\n",
      "loss at step 78700: 2.735007178783417\n",
      "loss at step 78720: 2.695323979854584\n",
      "loss at step 78740: 2.701763868331909\n",
      "loss at step 78760: 2.7406147718429565\n",
      "loss at step 78780: 2.6338738083839415\n",
      "loss at step 78800: 2.761175262928009\n",
      "validation loss: 2.8007526954015094\n",
      "validation accuracy: 14.884181969949918%\n",
      "loss at step 78820: 2.7292514562606813\n",
      "loss at step 78840: 2.7135875821113586\n",
      "loss at step 78860: 2.6861795663833616\n",
      "loss at step 78880: 2.7869274616241455\n",
      "loss at step 78900: 2.783119428157806\n",
      "loss at step 78920: 2.6988548398017884\n",
      "loss at step 78940: 2.6781033515930175\n",
      "loss at step 78960: 2.735380256175995\n",
      "loss at step 78980: 2.6196898818016052\n",
      "loss at step 79000: 2.815752649307251\n",
      "loss at step 79020: 2.630457329750061\n",
      "loss at step 79040: 2.7145890831947326\n",
      "loss at step 79060: 2.723092997074127\n",
      "loss at step 79080: 2.638672924041748\n",
      "loss at step 79100: 2.721641647815704\n",
      "loss at step 79120: 2.6912254095077515\n",
      "loss at step 79140: 2.77025226354599\n",
      "loss at step 79160: 2.681784451007843\n",
      "loss at step 79180: 2.6947945833206175\n",
      "loss at step 79200: 2.781958794593811\n",
      "validation loss: 2.817050223350525\n",
      "validation accuracy: 14.878964941569281%\n",
      "loss at step 79220: 2.7043105483055117\n",
      "loss at step 79240: 2.7323100447654722\n",
      "loss at step 79260: 2.690548539161682\n",
      "loss at step 79280: 2.747200608253479\n",
      "loss at step 79300: 2.7843506693840028\n",
      "loss at step 79320: 2.64796404838562\n",
      "loss at step 79340: 2.627648723125458\n",
      "loss at step 79360: 2.729673111438751\n",
      "loss at step 79380: 2.672525608539581\n",
      "loss at step 79400: 2.671349060535431\n",
      "loss at step 79420: 2.671676993370056\n",
      "loss at step 79440: 2.6960679411888124\n",
      "loss at step 79460: 2.6891843676567078\n",
      "loss at step 79480: 2.6803372740745544\n",
      "loss at step 79500: 2.684150791168213\n",
      "loss at step 79520: 2.630903124809265\n",
      "loss at step 79540: 2.629573333263397\n",
      "loss at step 79560: 2.6438468217849733\n",
      "loss at step 79580: 2.774013912677765\n",
      "loss at step 79600: 2.7324675917625427\n",
      "validation loss: 2.8015556597709654\n",
      "validation accuracy: 14.581594323873121%\n",
      "loss at step 79620: 2.6659318089485167\n",
      "loss at step 79640: 2.6489683985710144\n",
      "loss at step 79660: 2.7084529519081117\n",
      "loss at step 79680: 2.6781418323516846\n",
      "loss at step 79700: 2.675870645046234\n",
      "loss at step 79720: 2.659687602519989\n",
      "loss at step 79740: 2.677005076408386\n",
      "loss at step 79760: 2.7517142415046694\n",
      "loss at step 79780: 2.6714693307876587\n",
      "loss at step 79800: 2.7030600547790526\n",
      "loss at step 79820: 2.7150283098220824\n",
      "loss at step 79840: 2.6689265847206114\n",
      "loss at step 79860: 2.652647006511688\n",
      "loss at step 79880: 2.715117621421814\n",
      "loss at step 79900: 2.690624415874481\n",
      "loss at step 79920: 2.687827003002167\n",
      "loss at step 79940: 2.7502168297767637\n",
      "loss at step 79960: 2.7764784932136535\n",
      "loss at step 79980: 2.7039381146430967\n",
      "loss at step 80000: 2.728611648082733\n",
      "validation loss: 2.8008605662981667\n",
      "validation accuracy: 14.972871452420703%\n",
      "loss at step 80020: 2.7436736702919005\n",
      "loss at step 80040: 2.6469200432300566\n",
      "loss at step 80060: 2.7090911507606505\n",
      "loss at step 80080: 2.718386709690094\n",
      "loss at step 80100: 2.7017748236656187\n",
      "loss at step 80120: 2.69730384349823\n",
      "loss at step 80140: 2.69125736951828\n",
      "loss at step 80160: 2.6662803173065184\n",
      "loss at step 80180: 2.685043454170227\n",
      "loss at step 80200: 2.6907555937767027\n",
      "loss at step 80220: 2.7266381978988647\n",
      "loss at step 80240: 2.689975893497467\n",
      "loss at step 80260: 2.6585928797721863\n",
      "loss at step 80280: 2.6494309425354006\n",
      "loss at step 80300: 2.70752534866333\n",
      "loss at step 80320: 2.6966134786605833\n",
      "loss at step 80340: 2.775218117237091\n",
      "loss at step 80360: 2.683944857120514\n",
      "loss at step 80380: 2.697862470149994\n",
      "loss at step 80400: 2.757220983505249\n",
      "validation loss: 2.783663431008657\n",
      "validation accuracy: 15.37458263772955%\n",
      "loss at step 80420: 2.69452565908432\n",
      "loss at step 80440: 2.707898223400116\n",
      "loss at step 80460: 2.6955288648605347\n",
      "loss at step 80480: 2.6809449672698973\n",
      "loss at step 80500: 2.7274681270122527\n",
      "loss at step 80520: 2.6349833846092223\n",
      "loss at step 80540: 2.7178542375564576\n",
      "loss at step 80560: 2.765447175502777\n",
      "loss at step 80580: 2.6912487149238586\n",
      "loss at step 80600: 2.699801981449127\n",
      "loss at step 80620: 2.7611129760742186\n",
      "loss at step 80640: 2.6132293462753298\n",
      "loss at step 80660: 2.7195013403892516\n",
      "loss at step 80680: 2.739079678058624\n",
      "loss at step 80700: 2.758632814884186\n",
      "loss at step 80720: 2.6757823824882507\n",
      "loss at step 80740: 2.6525244235992433\n",
      "loss at step 80760: 2.6480855107307435\n",
      "loss at step 80780: 2.6877729892730713\n",
      "loss at step 80800: 2.7203595638275146\n",
      "validation loss: 2.8216961749394733\n",
      "validation accuracy: 14.607679465776293%\n",
      "loss at step 80820: 2.728468120098114\n",
      "loss at step 80840: 2.718339812755585\n",
      "loss at step 80860: 2.6791351675987243\n",
      "loss at step 80880: 2.6393236756324767\n",
      "loss at step 80900: 2.704667127132416\n",
      "loss at step 80920: 2.66858423948288\n",
      "loss at step 80940: 2.6314347267150877\n",
      "loss at step 80960: 2.634696161746979\n",
      "loss at step 80980: 2.736067295074463\n",
      "loss at step 81000: 2.7220560789108275\n",
      "loss at step 81020: 2.7260526180267335\n",
      "loss at step 81040: 2.683088231086731\n",
      "loss at step 81060: 2.7025532841682436\n",
      "loss at step 81080: 2.6671977162361147\n",
      "loss at step 81100: 2.7214781284332275\n",
      "loss at step 81120: 2.6746034622192383\n",
      "loss at step 81140: 2.7124991178512574\n",
      "loss at step 81160: 2.756414794921875\n",
      "loss at step 81180: 2.708883798122406\n",
      "loss at step 81200: 2.685109007358551\n",
      "validation loss: 2.81871866385142\n",
      "validation accuracy: 14.48247078464107%\n",
      "loss at step 81220: 2.7427082777023317\n",
      "loss at step 81240: 2.6797254920005797\n",
      "loss at step 81260: 2.7595712423324583\n",
      "loss at step 81280: 2.7209259152412413\n",
      "loss at step 81300: 2.7870478987693788\n",
      "loss at step 81320: 2.6354302287101747\n",
      "loss at step 81340: 2.815893805027008\n",
      "loss at step 81360: 2.65951207280159\n",
      "loss at step 81380: 2.739198076725006\n",
      "loss at step 81400: 2.7269648790359495\n",
      "loss at step 81420: 2.714137947559357\n",
      "loss at step 81440: 2.750328814983368\n",
      "loss at step 81460: 2.710718011856079\n",
      "loss at step 81480: 2.711262917518616\n",
      "loss at step 81500: 2.725036084651947\n",
      "loss at step 81520: 2.6405984997749328\n",
      "loss at step 81540: 2.6524359226226806\n",
      "loss at step 81560: 2.625990295410156\n",
      "loss at step 81580: 2.717617559432983\n",
      "loss at step 81600: 2.7228781461715696\n",
      "validation loss: 2.786348072687785\n",
      "validation accuracy: 15.37458263772955%\n",
      "loss at step 81620: 2.65999356508255\n",
      "loss at step 81640: 2.7587764978408815\n",
      "loss at step 81660: 2.7140395998954774\n",
      "loss at step 81680: 2.7065682888031004\n",
      "loss at step 81700: 2.7634051203727723\n",
      "loss at step 81720: 2.7806933403015135\n",
      "loss at step 81740: 2.7545941948890684\n",
      "loss at step 81760: 2.638569748401642\n",
      "loss at step 81780: 2.6896318674087523\n",
      "loss at step 81800: 2.6986054182052612\n",
      "loss at step 81820: 2.737830948829651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at step 81840: 2.6102961778640745\n",
      "loss at step 81860: 2.7354508876800536\n",
      "loss at step 81880: 2.693737292289734\n",
      "loss at step 81900: 2.6816327452659605\n",
      "loss at step 81920: 2.7075544357299806\n",
      "loss at step 81940: 2.7244885206222533\n",
      "loss at step 81960: 2.626539480686188\n",
      "loss at step 81980: 2.7327697396278383\n",
      "loss at step 82000: 2.6473228573799132\n",
      "validation loss: 2.7735563770929974\n",
      "validation accuracy: 14.832011686143574%\n",
      "loss at step 82020: 2.684140205383301\n",
      "loss at step 82040: 2.741627836227417\n",
      "loss at step 82060: 2.7919574618339538\n",
      "loss at step 82080: 2.6352982878685\n",
      "loss at step 82100: 2.6950711488723753\n",
      "loss at step 82120: 2.76212557554245\n",
      "loss at step 82140: 2.7342135667800904\n",
      "loss at step 82160: 2.693541634082794\n",
      "loss at step 82180: 2.6095203399658202\n",
      "loss at step 82200: 2.6525866389274597\n",
      "loss at step 82220: 2.7564761877059936\n",
      "loss at step 82240: 2.704667329788208\n",
      "loss at step 82260: 2.7064175486564634\n",
      "loss at step 82280: 2.750186729431152\n",
      "loss at step 82300: 2.717523765563965\n",
      "loss at step 82320: 2.7351428389549257\n",
      "loss at step 82340: 2.6651676535606383\n",
      "loss at step 82360: 2.630889093875885\n",
      "loss at step 82380: 2.7208399891853334\n",
      "loss at step 82400: 2.7104833722114563\n",
      "validation loss: 2.7819920110702516\n",
      "validation accuracy: 15.238939899833055%\n",
      "loss at step 82420: 2.6472492575645448\n",
      "loss at step 82440: 2.7137415528297426\n",
      "loss at step 82460: 2.672234845161438\n",
      "loss at step 82480: 2.61841481924057\n",
      "loss at step 82500: 2.6246443152427674\n",
      "loss at step 82520: 2.7072115302085877\n",
      "loss at step 82540: 2.696146297454834\n",
      "loss at step 82560: 2.705963695049286\n",
      "loss at step 82580: 2.7096083879470827\n",
      "loss at step 82600: 2.6844484210014343\n",
      "loss at step 82620: 2.7187068939208983\n",
      "loss at step 82640: 2.6918750524520876\n",
      "loss at step 82660: 2.712571716308594\n",
      "loss at step 82680: 2.7015435695648193\n",
      "loss at step 82700: 2.66736661195755\n",
      "loss at step 82720: 2.7009414553642275\n",
      "loss at step 82740: 2.6652451872825624\n",
      "loss at step 82760: 2.699659585952759\n",
      "loss at step 82780: 2.700258529186249\n",
      "loss at step 82800: 2.725758898258209\n",
      "validation loss: 2.784721791744232\n",
      "validation accuracy: 15.296327212020033%\n",
      "loss at step 82820: 2.704681706428528\n",
      "loss at step 82840: 2.6376534342765807\n",
      "loss at step 82860: 2.698432517051697\n",
      "loss at step 82880: 2.6883230805397034\n",
      "loss at step 82900: 2.7297474145889282\n",
      "loss at step 82920: 2.7350465297698974\n",
      "loss at step 82940: 2.6967154502868653\n",
      "loss at step 82960: 2.6561957120895388\n",
      "loss at step 82980: 2.704329013824463\n",
      "loss at step 83000: 2.718402695655823\n",
      "loss at step 83020: 2.7037220358848573\n",
      "loss at step 83040: 2.683102011680603\n",
      "loss at step 83060: 2.704652428627014\n",
      "loss at step 83080: 2.6847881197929384\n",
      "loss at step 83100: 2.6183116316795347\n",
      "loss at step 83120: 2.6452651500701903\n",
      "loss at step 83140: 2.713108253479004\n",
      "loss at step 83160: 2.7401853442192077\n",
      "loss at step 83180: 2.784574496746063\n",
      "loss at step 83200: 2.7333423018455507\n",
      "validation loss: 2.8127304792404173\n",
      "validation accuracy: 15.202420701168615%\n",
      "loss at step 83220: 2.6710313320159913\n",
      "loss at step 83240: 2.665433371067047\n",
      "loss at step 83260: 2.6410672068595886\n",
      "loss at step 83280: 2.668292450904846\n",
      "loss at step 83300: 2.728942370414734\n",
      "loss at step 83320: 2.7550724029541014\n",
      "loss at step 83340: 2.660590875148773\n",
      "loss at step 83360: 2.658318305015564\n",
      "loss at step 83380: 2.7037853002548218\n",
      "loss at step 83400: 2.562709057331085\n",
      "loss at step 83420: 2.753738617897034\n",
      "loss at step 83440: 2.756484401226044\n",
      "loss at step 83460: 2.6180808901786805\n",
      "loss at step 83480: 2.673630428314209\n",
      "loss at step 83500: 2.744029676914215\n",
      "loss at step 83520: 2.7935687780380247\n",
      "loss at step 83540: 2.7028240442276\n",
      "loss at step 83560: 2.7058424830436705\n",
      "loss at step 83580: 2.6938403606414796\n",
      "loss at step 83600: 2.7381949067115783\n",
      "validation loss: 2.78110852877299\n",
      "validation accuracy: 14.873747913188648%\n",
      "loss at step 83620: 2.6626582026481627\n",
      "loss at step 83640: 2.6939700841903687\n",
      "loss at step 83660: 2.6837144613265993\n",
      "loss at step 83680: 2.5585769057273864\n",
      "loss at step 83700: 2.7670990586280824\n",
      "loss at step 83720: 2.7738258481025695\n",
      "loss at step 83740: 2.7021586179733275\n",
      "loss at step 83760: 2.7233897924423216\n",
      "loss at step 83780: 2.7240025401115417\n",
      "loss at step 83800: 2.6704438805580137\n",
      "loss at step 83820: 2.683555340766907\n",
      "loss at step 83840: 2.693645989894867\n",
      "loss at step 83860: 2.6533904552459715\n",
      "loss at step 83880: 2.691108775138855\n",
      "loss at step 83900: 2.6804361224174498\n",
      "loss at step 83920: 2.700001895427704\n",
      "loss at step 83940: 2.7759615421295165\n",
      "loss at step 83960: 2.739515554904938\n",
      "loss at step 83980: 2.668385410308838\n",
      "loss at step 84000: 2.7357685685157778\n",
      "validation loss: 2.759327940940857\n",
      "validation accuracy: 15.025041736227045%\n",
      "loss at step 84020: 2.7301337242126467\n",
      "loss at step 84040: 2.717559027671814\n",
      "loss at step 84060: 2.7608878135681154\n",
      "loss at step 84080: 2.6844882726669312\n",
      "loss at step 84100: 2.7466896414756774\n",
      "loss at step 84120: 2.7269214153289796\n",
      "loss at step 84140: 2.6410309314727782\n",
      "loss at step 84160: 2.690724802017212\n",
      "loss at step 84180: 2.6934011459350584\n",
      "loss at step 84200: 2.7213307857513427\n",
      "loss at step 84220: 2.751410663127899\n",
      "loss at step 84240: 2.746451425552368\n",
      "loss at step 84260: 2.80840117931366\n",
      "loss at step 84280: 2.686433255672455\n",
      "loss at step 84300: 2.662002444267273\n",
      "loss at step 84320: 2.705274295806885\n",
      "loss at step 84340: 2.7306841731071474\n",
      "loss at step 84360: 2.7212503790855407\n",
      "loss at step 84380: 2.671044719219208\n",
      "loss at step 84400: 2.671065068244934\n",
      "validation loss: 2.775654187202454\n",
      "validation accuracy: 15.562395659432388%\n",
      "loss at step 84420: 2.6924856543540954\n",
      "loss at step 84440: 2.7717089295387267\n",
      "loss at step 84460: 2.729798436164856\n",
      "loss at step 84480: 2.6246172547340394\n",
      "loss at step 84500: 2.7308830857276916\n",
      "loss at step 84520: 2.7272828698158262\n",
      "loss at step 84540: 2.7196722984313966\n",
      "loss at step 84560: 2.700476920604706\n",
      "loss at step 84580: 2.6273104310035706\n",
      "loss at step 84600: 2.767555570602417\n",
      "loss at step 84620: 2.634964442253113\n",
      "loss at step 84640: 2.6513736605644227\n",
      "loss at step 84660: 2.700332021713257\n",
      "loss at step 84680: 2.6724491238594057\n",
      "loss at step 84700: 2.649760663509369\n",
      "loss at step 84720: 2.721672308444977\n",
      "loss at step 84740: 2.604845106601715\n",
      "loss at step 84760: 2.6454955339431763\n",
      "loss at step 84780: 2.6905566453933716\n",
      "loss at step 84800: 2.67888343334198\n",
      "validation loss: 2.7620285177230834\n",
      "validation accuracy: 15.551961602671119%\n",
      "loss at step 84820: 2.741531181335449\n",
      "loss at step 84840: 2.6766789317131043\n",
      "loss at step 84860: 2.734989881515503\n",
      "loss at step 84880: 2.661625063419342\n",
      "loss at step 84900: 2.672171139717102\n",
      "loss at step 84920: 2.668528640270233\n",
      "loss at step 84940: 2.7312439918518066\n",
      "loss at step 84960: 2.7282403230667116\n",
      "loss at step 84980: 2.704512929916382\n",
      "loss at step 85000: 2.7006723880767822\n",
      "loss at step 85020: 2.711165416240692\n",
      "loss at step 85040: 2.792835462093353\n",
      "loss at step 85060: 2.5949041843414307\n",
      "loss at step 85080: 2.617125964164734\n",
      "loss at step 85100: 2.7329596161842344\n",
      "loss at step 85120: 2.8077152013778686\n",
      "loss at step 85140: 2.7328431963920594\n",
      "loss at step 85160: 2.7505903601646424\n",
      "loss at step 85180: 2.653518009185791\n",
      "loss at step 85200: 2.677085220813751\n",
      "validation loss: 2.7645567893981933\n",
      "validation accuracy: 15.385016694490819%\n",
      "loss at step 85220: 2.6385713696479796\n",
      "loss at step 85240: 2.609145092964172\n",
      "loss at step 85260: 2.6364448189735414\n",
      "loss at step 85280: 2.750034141540527\n",
      "loss at step 85300: 2.635925829410553\n",
      "loss at step 85320: 2.7368151903152467\n",
      "loss at step 85340: 2.770782935619354\n",
      "loss at step 85360: 2.654442620277405\n",
      "loss at step 85380: 2.6412420511245727\n",
      "loss at step 85400: 2.672366774082184\n",
      "loss at step 85420: 2.7045252561569213\n",
      "loss at step 85440: 2.7385974168777465\n",
      "loss at step 85460: 2.7052555441856385\n",
      "loss at step 85480: 2.6303585052490233\n",
      "loss at step 85500: 2.666605567932129\n",
      "loss at step 85520: 2.661859917640686\n",
      "loss at step 85540: 2.673335063457489\n",
      "loss at step 85560: 2.6272664308547973\n",
      "loss at step 85580: 2.7273430466651916\n",
      "loss at step 85600: 2.670826828479767\n",
      "validation loss: 2.789660037358602\n",
      "validation accuracy: 14.800709515859767%\n",
      "loss at step 85620: 2.705529975891113\n",
      "loss at step 85640: 2.7312993884086607\n",
      "loss at step 85660: 2.6708119750022887\n",
      "loss at step 85680: 2.6207871317863463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at step 85700: 2.7244537472724915\n",
      "loss at step 85720: 2.7095914006233217\n",
      "loss at step 85740: 2.6857770919799804\n",
      "loss at step 85760: 2.754808723926544\n",
      "loss at step 85780: 2.6399945616722107\n",
      "loss at step 85800: 2.6930785298347475\n",
      "loss at step 85820: 2.717161762714386\n",
      "loss at step 85840: 2.701657009124756\n",
      "loss at step 85860: 2.6453122735023498\n",
      "loss at step 85880: 2.6920691609382628\n",
      "loss at step 85900: 2.679655838012695\n",
      "loss at step 85920: 2.686196005344391\n",
      "loss at step 85940: 2.6726969122886657\n",
      "loss at step 85960: 2.6998019337654116\n",
      "loss at step 85980: 2.73535498380661\n",
      "loss at step 86000: 2.6932485580444334\n",
      "validation loss: 2.7636483828226726\n",
      "validation accuracy: 15.071994991652755%\n",
      "loss at step 86020: 2.5815449595451354\n",
      "loss at step 86040: 2.642139995098114\n",
      "loss at step 86060: 2.635317826271057\n",
      "loss at step 86080: 2.6790221452713014\n",
      "loss at step 86100: 2.6624902844429017\n",
      "loss at step 86120: 2.783533239364624\n",
      "loss at step 86140: 2.658139336109161\n",
      "loss at step 86160: 2.77686607837677\n",
      "loss at step 86180: 2.7036841988563536\n",
      "loss at step 86200: 2.739627170562744\n",
      "loss at step 86220: 2.591415321826935\n",
      "loss at step 86240: 2.700923466682434\n",
      "loss at step 86260: 2.760647165775299\n",
      "loss at step 86280: 2.6650776147842405\n",
      "loss at step 86300: 2.7014146566390993\n",
      "loss at step 86320: 2.7113689184188843\n",
      "loss at step 86340: 2.736666202545166\n",
      "loss at step 86360: 2.724335050582886\n",
      "loss at step 86380: 2.6626947045326235\n",
      "loss at step 86400: 2.723495364189148\n",
      "validation loss: 2.797357519467672\n",
      "validation accuracy: 14.675500834724541%\n",
      "loss at step 86420: 2.706667387485504\n",
      "loss at step 86440: 2.718887722492218\n",
      "loss at step 86460: 2.696415901184082\n",
      "loss at step 86480: 2.738763952255249\n",
      "loss at step 86500: 2.7192545771598815\n",
      "loss at step 86520: 2.581633818149567\n",
      "loss at step 86540: 2.723369061946869\n",
      "loss at step 86560: 2.7724344730377197\n",
      "loss at step 86580: 2.6862618923187256\n",
      "loss at step 86600: 2.643928575515747\n",
      "loss at step 86620: 2.7132492542266844\n",
      "loss at step 86640: 2.7650229215621946\n",
      "loss at step 86660: 2.7169077038764953\n",
      "loss at step 86680: 2.7771806359291076\n",
      "loss at step 86700: 2.7344637870788575\n",
      "loss at step 86720: 2.734141993522644\n",
      "loss at step 86740: 2.6295500874519346\n",
      "loss at step 86760: 2.605235826969147\n",
      "loss at step 86780: 2.7122042179107666\n",
      "loss at step 86800: 2.6133120179176332\n",
      "validation loss: 2.7730489428838094\n",
      "validation accuracy: 15.191986644407345%\n",
      "loss at step 86820: 2.697503089904785\n",
      "loss at step 86840: 2.6303128838539123\n",
      "loss at step 86860: 2.7153488039970397\n",
      "loss at step 86880: 2.6897216200828553\n",
      "loss at step 86900: 2.704179418087006\n",
      "loss at step 86920: 2.7035043239593506\n",
      "loss at step 86940: 2.684467685222626\n",
      "loss at step 86960: 2.7610637307167054\n",
      "loss at step 86980: 2.6697705149650575\n",
      "loss at step 87000: 2.7780213832855223\n",
      "loss at step 87020: 2.7162460803985597\n",
      "loss at step 87040: 2.708818459510803\n",
      "loss at step 87060: 2.7248229146003724\n",
      "loss at step 87080: 2.6987546920776366\n",
      "loss at step 87100: 2.679868829250336\n",
      "loss at step 87120: 2.703120303153992\n",
      "loss at step 87140: 2.623874568939209\n",
      "loss at step 87160: 2.7484116911888123\n",
      "loss at step 87180: 2.796376919746399\n",
      "loss at step 87200: 2.6717766165733337\n",
      "validation loss: 2.775611270268758\n",
      "validation accuracy: 14.941569282136896%\n",
      "loss at step 87220: 2.640066397190094\n",
      "loss at step 87240: 2.6551433563232423\n",
      "loss at step 87260: 2.6498398423194884\n",
      "loss at step 87280: 2.703399884700775\n",
      "loss at step 87300: 2.65823575258255\n",
      "loss at step 87320: 2.6902134656906127\n",
      "loss at step 87340: 2.675199592113495\n",
      "loss at step 87360: 2.708581578731537\n",
      "loss at step 87380: 2.727207601070404\n",
      "loss at step 87400: 2.7131204485893248\n",
      "loss at step 87420: 2.667753529548645\n",
      "loss at step 87440: 2.644998013973236\n",
      "loss at step 87460: 2.6638779401779176\n",
      "loss at step 87480: 2.6580408930778505\n",
      "loss at step 87500: 2.718018889427185\n",
      "loss at step 87520: 2.658294880390167\n",
      "loss at step 87540: 2.641823923587799\n",
      "loss at step 87560: 2.712712264060974\n",
      "loss at step 87580: 2.6421687602996826\n",
      "loss at step 87600: 2.753431022167206\n",
      "validation loss: 2.7891926924387613\n",
      "validation accuracy: 14.998956594323873%\n",
      "loss at step 87620: 2.695697212219238\n",
      "loss at step 87640: 2.6377211928367617\n",
      "loss at step 87660: 2.762717294692993\n",
      "loss at step 87680: 2.5955389976501464\n",
      "loss at step 87700: 2.679193902015686\n",
      "loss at step 87720: 2.660453724861145\n",
      "loss at step 87740: 2.7107136130332945\n",
      "loss at step 87760: 2.669964623451233\n",
      "loss at step 87780: 2.7090535283088686\n",
      "loss at step 87800: 2.657961535453796\n",
      "loss at step 87820: 2.8210081815719605\n",
      "loss at step 87840: 2.7234408497810363\n",
      "loss at step 87860: 2.7491849184036257\n",
      "loss at step 87880: 2.628129208087921\n",
      "loss at step 87900: 2.666757118701935\n",
      "loss at step 87920: 2.652685225009918\n",
      "loss at step 87940: 2.623961699008942\n",
      "loss at step 87960: 2.6134193778038024\n",
      "loss at step 87980: 2.7598830342292784\n",
      "loss at step 88000: 2.7449981689453127\n",
      "validation loss: 2.7571923883756\n",
      "validation accuracy: 15.233722871452422%\n",
      "loss at step 88020: 2.708720028400421\n",
      "loss at step 88040: 2.698088562488556\n",
      "loss at step 88060: 2.6871481060981752\n",
      "loss at step 88080: 2.670688807964325\n",
      "loss at step 88100: 2.740435528755188\n",
      "loss at step 88120: 2.7211782574653625\n",
      "loss at step 88140: 2.710538077354431\n",
      "loss at step 88160: 2.7068933248519897\n",
      "loss at step 88180: 2.7745852947235106\n",
      "loss at step 88200: 2.7093729376792908\n",
      "loss at step 88220: 2.7199376463890075\n",
      "loss at step 88240: 2.623327612876892\n",
      "loss at step 88260: 2.714119386672974\n",
      "loss at step 88280: 2.6141133069992066\n",
      "loss at step 88300: 2.6899106025695803\n",
      "loss at step 88320: 2.689393150806427\n",
      "loss at step 88340: 2.6275681138038633\n",
      "loss at step 88360: 2.746113288402557\n",
      "loss at step 88380: 2.6737738490104674\n",
      "loss at step 88400: 2.6311599731445314\n",
      "validation loss: 2.7885253922144573\n",
      "validation accuracy: 14.64941569282137%\n",
      "loss at step 88420: 2.7169593453407286\n",
      "loss at step 88440: 2.659420096874237\n",
      "loss at step 88460: 2.6674914598464965\n",
      "loss at step 88480: 2.6695522546768187\n",
      "loss at step 88500: 2.718915271759033\n",
      "loss at step 88520: 2.75981981754303\n",
      "loss at step 88540: 2.717737090587616\n",
      "loss at step 88560: 2.728906977176666\n",
      "loss at step 88580: 2.639668321609497\n",
      "loss at step 88600: 2.7242722034454347\n",
      "loss at step 88620: 2.685935139656067\n",
      "loss at step 88640: 2.6672947287559508\n",
      "loss at step 88660: 2.7467900991439818\n",
      "loss at step 88680: 2.7454458713531493\n",
      "loss at step 88700: 2.6332404255867004\n",
      "loss at step 88720: 2.6649348020553587\n",
      "loss at step 88740: 2.7150402069091797\n",
      "loss at step 88760: 2.6833344101905823\n",
      "loss at step 88780: 2.747461760044098\n",
      "loss at step 88800: 2.74621764421463\n",
      "validation loss: 2.7532319140434267\n",
      "validation accuracy: 15.223288814691152%\n",
      "loss at step 88820: 2.6465076088905333\n",
      "loss at step 88840: 2.6655982971191405\n",
      "loss at step 88860: 2.6834511399269103\n",
      "loss at step 88880: 2.692149245738983\n",
      "loss at step 88900: 2.700578308105469\n",
      "loss at step 88920: 2.6942682385444643\n",
      "loss at step 88940: 2.720510172843933\n",
      "loss at step 88960: 2.725787305831909\n",
      "loss at step 88980: 2.7016234755516053\n",
      "loss at step 89000: 2.676201510429382\n",
      "loss at step 89020: 2.7051111817359925\n",
      "loss at step 89040: 2.6880819320678713\n",
      "loss at step 89060: 2.6538190960884096\n",
      "loss at step 89080: 2.671843433380127\n",
      "loss at step 89100: 2.699070167541504\n",
      "loss at step 89120: 2.8021532773971556\n",
      "loss at step 89140: 2.694414973258972\n",
      "loss at step 89160: 2.6525523185729982\n",
      "loss at step 89180: 2.6522494912147523\n",
      "loss at step 89200: 2.68445405960083\n",
      "validation loss: 2.7689112329483034\n",
      "validation accuracy: 15.10329716193656%\n",
      "loss at step 89220: 2.683059060573578\n",
      "loss at step 89240: 2.676081109046936\n",
      "loss at step 89260: 2.6193279504776\n",
      "loss at step 89280: 2.6131927967071533\n",
      "loss at step 89300: 2.697636616230011\n",
      "loss at step 89320: 2.611365592479706\n",
      "loss at step 89340: 2.6904755353927614\n",
      "loss at step 89360: 2.6817032933235168\n",
      "loss at step 89380: 2.691140151023865\n",
      "loss at step 89400: 2.696230113506317\n",
      "loss at step 89420: 2.6093517899513246\n",
      "loss at step 89440: 2.6424746990203856\n",
      "loss at step 89460: 2.6433592438697815\n",
      "loss at step 89480: 2.7386035203933714\n",
      "loss at step 89500: 2.7354217171669006\n",
      "loss at step 89520: 2.728895425796509\n",
      "loss at step 89540: 2.6899216532707215\n",
      "loss at step 89560: 2.715628743171692\n",
      "loss at step 89580: 2.694014573097229\n",
      "loss at step 89600: 2.663084363937378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 2.824626355965932\n",
      "validation accuracy: 14.132929883138564%\n",
      "loss at step 89620: 2.6821656465530395\n",
      "loss at step 89640: 2.5945727705955504\n",
      "loss at step 89660: 2.7084940314292907\n",
      "loss at step 89680: 2.6272295475006104\n",
      "loss at step 89700: 2.643423342704773\n",
      "loss at step 89720: 2.6210603594779966\n",
      "loss at step 89740: 2.6940165638923643\n",
      "loss at step 89760: 2.693687653541565\n",
      "loss at step 89780: 2.6580484390258787\n",
      "loss at step 89800: 2.6608741879463196\n",
      "loss at step 89820: 2.7335798859596254\n",
      "loss at step 89840: 2.679896867275238\n",
      "loss at step 89860: 2.6682975769042967\n",
      "loss at step 89880: 2.6780964970588683\n",
      "loss at step 89900: 2.6442212104797362\n",
      "loss at step 89920: 2.708136761188507\n",
      "loss at step 89940: 2.644916272163391\n",
      "loss at step 89960: 2.715713953971863\n",
      "loss at step 89980: 2.670178508758545\n",
      "loss at step 90000: 2.7517282485961916\n",
      "validation loss: 2.768266417980194\n",
      "validation accuracy: 15.348497495826377%\n",
      "loss at step 90020: 2.7632113337516784\n",
      "loss at step 90040: 2.650853192806244\n",
      "loss at step 90060: 2.6788950681686403\n",
      "loss at step 90080: 2.652616226673126\n",
      "loss at step 90100: 2.6709269642829896\n",
      "loss at step 90120: 2.7132577180862425\n",
      "loss at step 90140: 2.679379868507385\n",
      "loss at step 90160: 2.6611785769462584\n",
      "loss at step 90180: 2.6370688796043398\n",
      "loss at step 90200: 2.6953059673309325\n",
      "loss at step 90220: 2.708920955657959\n",
      "loss at step 90240: 2.73008873462677\n",
      "loss at step 90260: 2.7065693020820616\n",
      "loss at step 90280: 2.6562011361122133\n",
      "loss at step 90300: 2.66910115480423\n",
      "loss at step 90320: 2.714897930622101\n",
      "loss at step 90340: 2.644917571544647\n",
      "loss at step 90360: 2.697681558132172\n",
      "loss at step 90380: 2.7496547937393188\n",
      "loss at step 90400: 2.6982449889183044\n",
      "validation loss: 2.7859725443522136\n",
      "validation accuracy: 15.259808013355592%\n",
      "loss at step 90420: 2.6808284521102905\n",
      "loss at step 90440: 2.645047163963318\n",
      "loss at step 90460: 2.7501158595085142\n",
      "loss at step 90480: 2.6581297159194945\n",
      "loss at step 90500: 2.7143887877464294\n",
      "loss at step 90520: 2.6733007192611695\n",
      "loss at step 90540: 2.6598681092262266\n",
      "loss at step 90560: 2.691324830055237\n",
      "loss at step 90580: 2.6776180386543276\n",
      "loss at step 90600: 2.694522738456726\n",
      "loss at step 90620: 2.669385480880737\n",
      "loss at step 90640: 2.7174561858177184\n",
      "loss at step 90660: 2.667267954349518\n",
      "loss at step 90680: 2.667414104938507\n",
      "loss at step 90700: 2.6291149616241456\n",
      "loss at step 90720: 2.655657780170441\n",
      "loss at step 90740: 2.7373588562011717\n",
      "loss at step 90760: 2.752429735660553\n",
      "loss at step 90780: 2.6364370703697206\n",
      "loss at step 90800: 2.7140056490898132\n",
      "validation loss: 2.7620414741834005\n",
      "validation accuracy: 15.207637729549248%\n",
      "loss at step 90820: 2.78968346118927\n",
      "loss at step 90840: 2.66996511220932\n",
      "loss at step 90860: 2.6730819582939147\n",
      "loss at step 90880: 2.7258751153945924\n",
      "loss at step 90900: 2.7501370191574095\n",
      "loss at step 90920: 2.75172119140625\n",
      "loss at step 90940: 2.6703169465065004\n",
      "loss at step 90960: 2.8086050271987917\n",
      "loss at step 90980: 2.7835790395736693\n",
      "loss at step 91000: 2.729817545413971\n",
      "loss at step 91020: 2.7145818948745726\n",
      "loss at step 91040: 2.675426256656647\n",
      "loss at step 91060: 2.6447062849998475\n",
      "loss at step 91080: 2.6920339345932005\n",
      "loss at step 91100: 2.638213241100311\n",
      "loss at step 91120: 2.6678873658180238\n",
      "loss at step 91140: 2.638003635406494\n",
      "loss at step 91160: 2.6997320890426635\n",
      "loss at step 91180: 2.6558090448379517\n",
      "loss at step 91200: 2.729083526134491\n",
      "validation loss: 2.75977875550588\n",
      "validation accuracy: 15.437186978297163%\n",
      "loss at step 91220: 2.763697326183319\n",
      "loss at step 91240: 2.7338343024253846\n",
      "loss at step 91260: 2.7113373994827272\n",
      "loss at step 91280: 2.699117136001587\n",
      "loss at step 91300: 2.6844262957572935\n",
      "loss at step 91320: 2.743786025047302\n",
      "loss at step 91340: 2.699600076675415\n",
      "loss at step 91360: 2.758053719997406\n",
      "loss at step 91380: 2.7060842752456664\n",
      "loss at step 91400: 2.6525930762290955\n",
      "loss at step 91420: 2.697737991809845\n",
      "loss at step 91440: 2.61794695854187\n",
      "loss at step 91460: 2.6837686777114866\n",
      "loss at step 91480: 2.6758617997169494\n",
      "loss at step 91500: 2.6841899275779726\n",
      "loss at step 91520: 2.6555888056755066\n",
      "loss at step 91540: 2.677825927734375\n",
      "loss at step 91560: 2.6947795271873476\n",
      "loss at step 91580: 2.743371295928955\n",
      "loss at step 91600: 2.652321529388428\n",
      "validation loss: 2.8305581283569334\n",
      "validation accuracy: 12.171327212020033%\n",
      "loss at step 91620: 2.745837318897247\n",
      "loss at step 91640: 2.6841228365898133\n",
      "loss at step 91660: 2.7136224031448366\n",
      "loss at step 91680: 2.659667205810547\n",
      "loss at step 91700: 2.6883758068084718\n",
      "loss at step 91720: 2.711532402038574\n",
      "loss at step 91740: 2.539941966533661\n",
      "loss at step 91760: 2.652297627925873\n",
      "loss at step 91780: 2.6297863841056826\n",
      "loss at step 91800: 2.7025875806808473\n",
      "loss at step 91820: 2.668194758892059\n",
      "loss at step 91840: 2.678271770477295\n",
      "loss at step 91860: 2.642031228542328\n",
      "loss at step 91880: 2.680096983909607\n",
      "loss at step 91900: 2.711706614494324\n",
      "loss at step 91920: 2.6957991242408754\n",
      "loss at step 91940: 2.733711314201355\n",
      "loss at step 91960: 2.6443416237831117\n",
      "loss at step 91980: 2.7031941771507264\n",
      "loss at step 92000: 2.696942222118378\n",
      "validation loss: 2.7739208348592124\n",
      "validation accuracy: 15.228505843071785%\n",
      "loss at step 92020: 2.645548379421234\n",
      "loss at step 92040: 2.620034968852997\n",
      "loss at step 92060: 2.6164457201957703\n",
      "loss at step 92080: 2.7565338134765627\n",
      "loss at step 92100: 2.689190077781677\n",
      "loss at step 92120: 2.5925050020217895\n",
      "loss at step 92140: 2.7125784516334535\n",
      "loss at step 92160: 2.7018754482269287\n",
      "loss at step 92180: 2.7589695692062377\n",
      "loss at step 92200: 2.6598110675811766\n",
      "loss at step 92220: 2.719826066493988\n",
      "loss at step 92240: 2.698906588554382\n",
      "loss at step 92260: 2.600996899604797\n",
      "loss at step 92280: 2.613189232349396\n",
      "loss at step 92300: 2.6852391958236694\n",
      "loss at step 92320: 2.6667917132377625\n",
      "loss at step 92340: 2.619764339923859\n",
      "loss at step 92360: 2.7396354556083677\n",
      "loss at step 92380: 2.6717145323753355\n",
      "loss at step 92400: 2.6500123381614684\n",
      "validation loss: 2.762086114883423\n",
      "validation accuracy: 15.275459098497496%\n",
      "loss at step 92420: 2.586739993095398\n",
      "loss at step 92440: 2.681020975112915\n",
      "loss at step 92460: 2.72345632314682\n",
      "loss at step 92480: 2.6654413461685182\n",
      "loss at step 92500: 2.781168019771576\n",
      "loss at step 92520: 2.7100023150444033\n",
      "loss at step 92540: 2.773558163642883\n",
      "loss at step 92560: 2.7368683099746702\n",
      "loss at step 92580: 2.757526683807373\n",
      "loss at step 92600: 2.670797574520111\n",
      "loss at step 92620: 2.6717496037483217\n",
      "loss at step 92640: 2.6487815618515014\n",
      "loss at step 92660: 2.6203127026557924\n",
      "loss at step 92680: 2.6531144976615906\n",
      "loss at step 92700: 2.646813762187958\n",
      "loss at step 92720: 2.669726324081421\n",
      "loss at step 92740: 2.6152190566062927\n",
      "loss at step 92760: 2.6244094371795654\n",
      "loss at step 92780: 2.67650820016861\n",
      "loss at step 92800: 2.6700289726257322\n",
      "validation loss: 2.790208247502645\n",
      "validation accuracy: 14.4720367278798%\n",
      "loss at step 92820: 2.588563060760498\n",
      "loss at step 92840: 2.772453272342682\n",
      "loss at step 92860: 2.7126088619232176\n",
      "loss at step 92880: 2.6482964873313906\n",
      "loss at step 92900: 2.661823105812073\n",
      "loss at step 92920: 2.685024607181549\n",
      "loss at step 92940: 2.6821502447128296\n",
      "loss at step 92960: 2.646685481071472\n",
      "loss at step 92980: 2.585458505153656\n",
      "loss at step 93000: 2.6460660099983215\n",
      "loss at step 93020: 2.7097115874290467\n",
      "loss at step 93040: 2.643620264530182\n",
      "loss at step 93060: 2.713825213909149\n",
      "loss at step 93080: 2.759696531295776\n",
      "loss at step 93100: 2.7245471596717836\n",
      "loss at step 93120: 2.683055245876312\n",
      "loss at step 93140: 2.6010708451271056\n",
      "loss at step 93160: 2.6472303748130797\n",
      "loss at step 93180: 2.6323541045188903\n",
      "loss at step 93200: 2.648498702049255\n",
      "validation loss: 2.776182727018992\n",
      "validation accuracy: 14.800709515859767%\n",
      "loss at step 93220: 2.6904939293861387\n",
      "loss at step 93240: 2.83564749956131\n",
      "loss at step 93260: 2.6500469923019407\n",
      "loss at step 93280: 2.683154118061066\n",
      "loss at step 93300: 2.6968663215637205\n",
      "loss at step 93320: 2.7923908472061156\n",
      "loss at step 93340: 2.690462422370911\n",
      "loss at step 93360: 2.714737629890442\n",
      "loss at step 93380: 2.6423767805099487\n",
      "loss at step 93400: 2.6141180276870726\n",
      "loss at step 93420: 2.6984878540039063\n",
      "loss at step 93440: 2.733649754524231\n",
      "loss at step 93460: 2.6724464297294617\n",
      "loss at step 93480: 2.6132474660873415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at step 93500: 2.6895387172698975\n",
      "loss at step 93520: 2.6374205112457276\n",
      "loss at step 93540: 2.652469575405121\n",
      "loss at step 93560: 2.7325408935546873\n",
      "loss at step 93580: 2.6788623452186586\n",
      "loss at step 93600: 2.6965091586112977\n",
      "validation loss: 2.7779923772811888\n",
      "validation accuracy: 14.941569282136896%\n",
      "loss at step 93620: 2.595069396495819\n",
      "loss at step 93640: 2.724934732913971\n",
      "loss at step 93660: 2.7170013904571535\n",
      "loss at step 93680: 2.6191623210906982\n",
      "loss at step 93700: 2.6277590155601502\n",
      "loss at step 93720: 2.6751591086387636\n",
      "loss at step 93740: 2.6369277238845825\n",
      "loss at step 93760: 2.7097522616386414\n",
      "loss at step 93780: 2.6904122591018678\n",
      "loss at step 93800: 2.7778671622276305\n",
      "loss at step 93820: 2.6560313940048217\n",
      "loss at step 93840: 2.6721355319023132\n",
      "loss at step 93860: 2.6570173621177675\n",
      "loss at step 93880: 2.6860291719436646\n",
      "loss at step 93900: 2.612747514247894\n",
      "loss at step 93920: 2.752856731414795\n",
      "loss at step 93940: 2.71970739364624\n",
      "loss at step 93960: 2.632898783683777\n",
      "loss at step 93980: 2.688086986541748\n",
      "loss at step 94000: 2.598746144771576\n",
      "validation loss: 2.7634635305404665\n",
      "validation accuracy: 14.727671118530886%\n",
      "loss at step 94020: 2.672237241268158\n",
      "loss at step 94040: 2.6620834469795227\n",
      "loss at step 94060: 2.709052062034607\n",
      "loss at step 94080: 2.615519976615906\n",
      "loss at step 94100: 2.6036364316940306\n",
      "loss at step 94120: 2.6375877976417543\n",
      "loss at step 94140: 2.7133713960647583\n",
      "loss at step 94160: 2.6510425806045532\n",
      "loss at step 94180: 2.5995864152908323\n",
      "loss at step 94200: 2.690037262439728\n",
      "loss at step 94220: 2.7315134048461913\n",
      "loss at step 94240: 2.7272313952445986\n",
      "loss at step 94260: 2.63141006231308\n",
      "loss at step 94280: 2.7305504083633423\n",
      "loss at step 94300: 2.710041511058807\n",
      "loss at step 94320: 2.651016664505005\n",
      "loss at step 94340: 2.6463624358177187\n",
      "loss at step 94360: 2.723276710510254\n",
      "loss at step 94380: 2.6259284615516663\n",
      "loss at step 94400: 2.721181273460388\n",
      "validation loss: 2.7732595308621724\n",
      "validation accuracy: 15.343280467445744%\n",
      "loss at step 94420: 2.7104275584220887\n",
      "loss at step 94440: 2.6352040648460386\n",
      "loss at step 94460: 2.6586829304695128\n",
      "loss at step 94480: 2.671413505077362\n",
      "loss at step 94500: 2.729129230976105\n",
      "loss at step 94520: 2.7186324834823608\n",
      "loss at step 94540: 2.6994168281555178\n",
      "loss at step 94560: 2.5926182150840758\n",
      "loss at step 94580: 2.634863352775574\n",
      "loss at step 94600: 2.7134195446968077\n",
      "loss at step 94620: 2.612682843208313\n",
      "loss at step 94640: 2.688762664794922\n",
      "loss at step 94660: 2.571764349937439\n",
      "loss at step 94680: 2.6922237515449523\n",
      "loss at step 94700: 2.6463126301765443\n",
      "loss at step 94720: 2.6447050094604494\n",
      "loss at step 94740: 2.658163821697235\n",
      "loss at step 94760: 2.657167744636536\n",
      "loss at step 94780: 2.6321298837661744\n",
      "loss at step 94800: 2.6482959747314454\n",
      "validation loss: 2.766648856798808\n",
      "validation accuracy: 14.962437395659434%\n",
      "loss at step 94820: 2.747714936733246\n",
      "loss at step 94840: 2.7029891133308412\n",
      "loss at step 94860: 2.707930088043213\n",
      "loss at step 94880: 2.706616389751434\n",
      "loss at step 94900: 2.6923485517501833\n",
      "loss at step 94920: 2.709189236164093\n",
      "loss at step 94940: 2.7580592155456545\n",
      "loss at step 94960: 2.7493934512138365\n",
      "loss at step 94980: 2.6924209356307984\n",
      "loss at step 95000: 2.766639506816864\n",
      "loss at step 95020: 2.600072503089905\n",
      "loss at step 95040: 2.6219653367996214\n",
      "loss at step 95060: 2.6503711104393006\n",
      "loss at step 95080: 2.6595799803733824\n",
      "loss at step 95100: 2.712991678714752\n",
      "loss at step 95120: 2.7279255747795106\n",
      "loss at step 95140: 2.6817492961883547\n",
      "loss at step 95160: 2.642276906967163\n",
      "loss at step 95180: 2.7226122617721558\n",
      "loss at step 95200: 2.6141805052757263\n",
      "validation loss: 2.7614865215619404\n",
      "validation accuracy: 15.311978297161938%\n",
      "loss at step 95220: 2.717548978328705\n",
      "loss at step 95240: 2.663916277885437\n",
      "loss at step 95260: 2.735672962665558\n",
      "loss at step 95280: 2.6775463461875915\n",
      "loss at step 95300: 2.624402105808258\n",
      "loss at step 95320: 2.6465167880058287\n",
      "loss at step 95340: 2.67119642496109\n",
      "loss at step 95360: 2.7617804646492004\n",
      "loss at step 95380: 2.6212759733200075\n",
      "loss at step 95400: 2.682828366756439\n",
      "loss at step 95420: 2.623263108730316\n",
      "loss at step 95440: 2.667642283439636\n",
      "loss at step 95460: 2.748425233364105\n",
      "loss at step 95480: 2.643329882621765\n",
      "loss at step 95500: 2.6347946524620056\n",
      "loss at step 95520: 2.6735850095748903\n",
      "loss at step 95540: 2.6218220591545105\n",
      "loss at step 95560: 2.611327338218689\n",
      "loss at step 95580: 2.7149351954460146\n",
      "loss at step 95600: 2.762098824977875\n",
      "validation loss: 2.7948105732599893\n",
      "validation accuracy: 14.273789649415694%\n",
      "loss at step 95620: 2.597300577163696\n",
      "loss at step 95640: 2.675319695472717\n",
      "loss at step 95660: 2.686541187763214\n",
      "loss at step 95680: 2.618710291385651\n",
      "loss at step 95700: 2.6308863282203676\n",
      "loss at step 95720: 2.6997417569160462\n",
      "loss at step 95740: 2.7111154437065124\n",
      "loss at step 95760: 2.7125139832496643\n",
      "loss at step 95780: 2.6337907671928407\n",
      "loss at step 95800: 2.6222278594970705\n",
      "loss at step 95820: 2.687162530422211\n",
      "loss at step 95840: 2.640857195854187\n",
      "loss at step 95860: 2.6382923245429994\n",
      "loss at step 95880: 2.674266350269318\n",
      "loss at step 95900: 2.6749635100364686\n",
      "loss at step 95920: 2.6741763830184935\n",
      "loss at step 95940: 2.635551393032074\n",
      "loss at step 95960: 2.715293025970459\n",
      "loss at step 95980: 2.6383360028266907\n",
      "loss at step 96000: 2.6522537589073183\n",
      "validation loss: 2.7763492902119955\n",
      "validation accuracy: 15.405884808013356%\n",
      "loss at step 96020: 2.719174802303314\n",
      "loss at step 96040: 2.704107332229614\n",
      "loss at step 96060: 2.6832479357719423\n",
      "loss at step 96080: 2.686625039577484\n",
      "loss at step 96100: 2.6942036271095278\n",
      "loss at step 96120: 2.649581241607666\n",
      "loss at step 96140: 2.689294183254242\n",
      "loss at step 96160: 2.712748944759369\n",
      "loss at step 96180: 2.7217950344085695\n",
      "loss at step 96200: 2.7103360295295715\n",
      "loss at step 96220: 2.678792154788971\n",
      "loss at step 96240: 2.7452957391738892\n",
      "loss at step 96260: 2.7646144032478333\n",
      "loss at step 96280: 2.6816607475280763\n",
      "loss at step 96300: 2.7389320015907286\n",
      "loss at step 96320: 2.7076413154602053\n",
      "loss at step 96340: 2.6225548446178437\n",
      "loss at step 96360: 2.6798298239707945\n",
      "loss at step 96380: 2.635722553730011\n",
      "loss at step 96400: 2.7045070052146913\n",
      "validation loss: 2.7961822819709776\n",
      "validation accuracy: 14.826794657762937%\n",
      "loss at step 96420: 2.701896941661835\n",
      "loss at step 96440: 2.6539391040802003\n",
      "loss at step 96460: 2.634150516986847\n",
      "loss at step 96480: 2.713601303100586\n",
      "loss at step 96500: 2.7455734848976134\n",
      "loss at step 96520: 2.6999731421470643\n",
      "loss at step 96540: 2.641571605205536\n",
      "loss at step 96560: 2.7053120017051695\n",
      "loss at step 96580: 2.685110104084015\n",
      "loss at step 96600: 2.665075421333313\n",
      "loss at step 96620: 2.6370784640312195\n",
      "loss at step 96640: 2.6972571134567263\n",
      "loss at step 96660: 2.7587582230567933\n",
      "loss at step 96680: 2.669870471954346\n",
      "loss at step 96700: 2.682417392730713\n",
      "loss at step 96720: 2.6961846232414244\n",
      "loss at step 96740: 2.652226376533508\n",
      "loss at step 96760: 2.675035238265991\n",
      "loss at step 96780: 2.637882745265961\n",
      "loss at step 96800: 2.6999167680740355\n",
      "validation loss: 2.7910644642512\n",
      "validation accuracy: 14.832011686143574%\n",
      "loss at step 96820: 2.6847605228424074\n",
      "loss at step 96840: 2.6861604928970335\n",
      "loss at step 96860: 2.6441935658454896\n",
      "loss at step 96880: 2.706198275089264\n",
      "loss at step 96900: 2.675787961483002\n",
      "loss at step 96920: 2.6029757380485536\n",
      "loss at step 96940: 2.690618121623993\n",
      "loss at step 96960: 2.6396596312522886\n",
      "loss at step 96980: 2.701758074760437\n",
      "loss at step 97000: 2.6804988861083983\n",
      "loss at step 97020: 2.682591438293457\n",
      "loss at step 97040: 2.705983543395996\n",
      "loss at step 97060: 2.7318103075027467\n",
      "loss at step 97080: 2.729992938041687\n",
      "loss at step 97100: 2.6781687259674074\n",
      "loss at step 97120: 2.651958501338959\n",
      "loss at step 97140: 2.6570229053497316\n",
      "loss at step 97160: 2.6894412279129027\n",
      "loss at step 97180: 2.6710178375244142\n",
      "loss at step 97200: 2.5999895453453066\n",
      "validation loss: 2.7755364036560057\n",
      "validation accuracy: 15.296327212020033%\n",
      "loss at step 97220: 2.5879711866378785\n",
      "loss at step 97240: 2.6516276359558106\n",
      "loss at step 97260: 2.6337138175964356\n",
      "loss at step 97280: 2.6753045678138734\n",
      "loss at step 97300: 2.6896502017974853\n",
      "loss at step 97320: 2.628876781463623\n",
      "loss at step 97340: 2.715713393688202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at step 97360: 2.672602152824402\n",
      "loss at step 97380: 2.685239005088806\n",
      "loss at step 97400: 2.6621739149093626\n",
      "loss at step 97420: 2.7698076248168944\n",
      "loss at step 97440: 2.6420738458633424\n",
      "loss at step 97460: 2.778098261356354\n",
      "loss at step 97480: 2.700210988521576\n",
      "loss at step 97500: 2.6513081192970276\n",
      "loss at step 97520: 2.6943679213523866\n",
      "loss at step 97540: 2.74864581823349\n",
      "loss at step 97560: 2.7187570691108705\n",
      "loss at step 97580: 2.6432628870010375\n",
      "loss at step 97600: 2.7239083886146545\n",
      "validation loss: 2.7779286924997963\n",
      "validation accuracy: 15.625%\n",
      "loss at step 97620: 2.646199607849121\n",
      "loss at step 97640: 2.6900200247764587\n",
      "loss at step 97660: 2.6529792666435243\n",
      "loss at step 97680: 2.64248206615448\n",
      "loss at step 97700: 2.721072030067444\n",
      "loss at step 97720: 2.7067981243133543\n",
      "loss at step 97740: 2.7177141308784485\n",
      "loss at step 97760: 2.656858777999878\n",
      "loss at step 97780: 2.746215748786926\n",
      "loss at step 97800: 2.7380526185035707\n",
      "loss at step 97820: 2.702978730201721\n",
      "loss at step 97840: 2.6634265542030335\n",
      "loss at step 97860: 2.730253005027771\n",
      "loss at step 97880: 2.719661808013916\n",
      "loss at step 97900: 2.712123620510101\n",
      "loss at step 97920: 2.6380070447921753\n",
      "loss at step 97940: 2.705203890800476\n",
      "loss at step 97960: 2.726286435127258\n",
      "loss at step 97980: 2.629625952243805\n",
      "loss at step 98000: 2.7476718425750732\n",
      "validation loss: 2.7696566430727643\n",
      "validation accuracy: 15.911936560934892%\n",
      "loss at step 98020: 2.742675805091858\n",
      "loss at step 98040: 2.6768881916999816\n",
      "loss at step 98060: 2.6112427949905395\n",
      "loss at step 98080: 2.740255320072174\n",
      "loss at step 98100: 2.698252558708191\n",
      "loss at step 98120: 2.7382675290107725\n",
      "loss at step 98140: 2.65738924741745\n",
      "loss at step 98160: 2.6836068272590636\n",
      "loss at step 98180: 2.6882561445236206\n",
      "loss at step 98200: 2.668379771709442\n",
      "loss at step 98220: 2.6633023262023925\n",
      "loss at step 98240: 2.7412050485610964\n",
      "loss at step 98260: 2.627863359451294\n",
      "loss at step 98280: 2.710756766796112\n",
      "loss at step 98300: 2.696386229991913\n",
      "loss at step 98320: 2.7234267711639406\n",
      "loss at step 98340: 2.6608901381492616\n",
      "loss at step 98360: 2.621164917945862\n",
      "loss at step 98380: 2.723308193683624\n",
      "loss at step 98400: 2.545192015171051\n",
      "validation loss: 2.770996092160543\n",
      "validation accuracy: 15.37458263772955%\n",
      "loss at step 98420: 2.630410540103912\n",
      "loss at step 98440: 2.6288337826728823\n",
      "loss at step 98460: 2.6798784852027895\n",
      "loss at step 98480: 2.6888638377189635\n",
      "loss at step 98500: 2.7061784148216246\n",
      "loss at step 98520: 2.645326578617096\n",
      "loss at step 98540: 2.6865439653396606\n",
      "loss at step 98560: 2.720587956905365\n",
      "loss at step 98580: 2.6054574728012083\n",
      "loss at step 98600: 2.736260974407196\n",
      "loss at step 98620: 2.661404716968536\n",
      "loss at step 98640: 2.7523913621902465\n",
      "loss at step 98660: 2.6949607253074648\n",
      "loss at step 98680: 2.6682475209236145\n",
      "loss at step 98700: 2.658708691596985\n",
      "loss at step 98720: 2.6846991419792174\n",
      "loss at step 98740: 2.6036680340766907\n",
      "loss at step 98760: 2.7111170649528504\n",
      "loss at step 98780: 2.6341164231300356\n",
      "loss at step 98800: 2.7716779112815857\n",
      "validation loss: 2.786151282787323\n",
      "validation accuracy: 15.306761268781303%\n",
      "loss at step 98820: 2.665023684501648\n",
      "loss at step 98840: 2.735125207901001\n",
      "loss at step 98860: 2.673298740386963\n",
      "loss at step 98880: 2.6420318722724914\n",
      "loss at step 98900: 2.6853426814079286\n",
      "loss at step 98920: 2.665121042728424\n",
      "loss at step 98940: 2.638190245628357\n",
      "loss at step 98960: 2.728706419467926\n",
      "loss at step 98980: 2.5699674129486083\n",
      "loss at step 99000: 2.624898612499237\n",
      "loss at step 99020: 2.6863016843795777\n",
      "loss at step 99040: 2.6811652541160584\n",
      "loss at step 99060: 2.713277292251587\n",
      "loss at step 99080: 2.5706804156303407\n",
      "loss at step 99100: 2.679396653175354\n",
      "loss at step 99120: 2.656355893611908\n",
      "loss at step 99140: 2.6728063225746155\n",
      "loss at step 99160: 2.633147919178009\n",
      "loss at step 99180: 2.701870584487915\n",
      "loss at step 99200: 2.586871790885925\n",
      "validation loss: 2.7699821909268696\n",
      "validation accuracy: 15.311978297161938%\n",
      "loss at step 99220: 2.7022871136665345\n",
      "loss at step 99240: 2.6868707299232484\n",
      "loss at step 99260: 2.6583852052688597\n",
      "loss at step 99280: 2.623564577102661\n",
      "loss at step 99300: 2.721025228500366\n",
      "loss at step 99320: 2.637107765674591\n",
      "loss at step 99340: 2.7653432130813598\n",
      "loss at step 99360: 2.666272497177124\n",
      "loss at step 99380: 2.6609034180641173\n",
      "loss at step 99400: 2.6502958059310915\n",
      "loss at step 99420: 2.7838305711746214\n",
      "loss at step 99440: 2.667742156982422\n",
      "loss at step 99460: 2.6556904554367065\n",
      "loss at step 99480: 2.7034639596939085\n",
      "loss at step 99500: 2.6958281993865967\n",
      "loss at step 99520: 2.593091642856598\n",
      "loss at step 99540: 2.6648832321166993\n",
      "loss at step 99560: 2.657538902759552\n",
      "loss at step 99580: 2.737997043132782\n",
      "loss at step 99600: 2.5735008478164674\n",
      "validation loss: 2.7501408211390177\n",
      "validation accuracy: 15.557178631051752%\n",
      "loss at step 99620: 2.7467828750610352\n",
      "loss at step 99640: 2.6173636436462404\n",
      "loss at step 99660: 2.6536641478538514\n",
      "loss at step 99680: 2.64514399766922\n",
      "loss at step 99700: 2.6684079647064207\n",
      "loss at step 99720: 2.597629415988922\n",
      "loss at step 99740: 2.733866262435913\n",
      "loss at step 99760: 2.6476396560668944\n",
      "loss at step 99780: 2.6628007531166076\n",
      "loss at step 99800: 2.7154608964920044\n",
      "loss at step 99820: 2.686681807041168\n",
      "loss at step 99840: 2.6596773624420167\n",
      "loss at step 99860: 2.61461056470871\n",
      "loss at step 99880: 2.635505127906799\n",
      "loss at step 99900: 2.7478646278381347\n",
      "loss at step 99920: 2.7151662349700927\n",
      "loss at step 99940: 2.716217303276062\n",
      "loss at step 99960: 2.7815728187561035\n",
      "loss at step 99980: 2.674265444278717\n",
      "loss at step 100000: 2.600595235824585\n",
      "validation loss: 2.7526467514038084\n",
      "validation accuracy: 15.583263772954925%\n",
      "loss at step 100020: 2.6826744079589844\n",
      "loss at step 100040: 2.655386233329773\n",
      "loss at step 100060: 2.6518168807029725\n",
      "loss at step 100080: 2.6715973377227784\n",
      "loss at step 100100: 2.6568042635917664\n",
      "loss at step 100120: 2.6297035932540895\n",
      "loss at step 100140: 2.6804941058158875\n",
      "loss at step 100160: 2.6505754709243776\n",
      "loss at step 100180: 2.7927969813346865\n",
      "loss at step 100200: 2.7249706625938415\n",
      "loss at step 100220: 2.642904043197632\n",
      "loss at step 100240: 2.67677526473999\n",
      "loss at step 100260: 2.665601873397827\n",
      "loss at step 100280: 2.6549294114112856\n",
      "loss at step 100300: 2.662738490104675\n",
      "loss at step 100320: 2.6952699422836304\n",
      "loss at step 100340: 2.652869975566864\n",
      "loss at step 100360: 2.6797465801239015\n",
      "loss at step 100380: 2.6845712065696716\n",
      "loss at step 100400: 2.7023580193519594\n",
      "validation loss: 2.7563818232218424\n",
      "validation accuracy: 15.426752921535893%\n",
      "loss at step 100420: 2.6916805028915407\n",
      "loss at step 100440: 2.787429893016815\n",
      "loss at step 100460: 2.707179236412048\n",
      "loss at step 100480: 2.728468596935272\n",
      "loss at step 100500: 2.712824022769928\n",
      "loss at step 100520: 2.697883129119873\n",
      "loss at step 100540: 2.6283767580986024\n",
      "loss at step 100560: 2.6945321798324584\n",
      "loss at step 100580: 2.650334870815277\n",
      "loss at step 100600: 2.637824010848999\n",
      "loss at step 100620: 2.680971884727478\n",
      "loss at step 100640: 2.6593612670898437\n",
      "loss at step 100660: 2.715474283695221\n",
      "loss at step 100680: 2.668130624294281\n",
      "loss at step 100700: 2.606585645675659\n",
      "loss at step 100720: 2.649514937400818\n",
      "loss at step 100740: 2.7079681038856505\n",
      "loss at step 100760: 2.6856385231018067\n",
      "loss at step 100780: 2.6846742033958435\n",
      "loss at step 100800: 2.657011020183563\n",
      "validation loss: 2.7635630877812702\n",
      "validation accuracy: 15.416318864774626%\n",
      "loss at step 100820: 2.577933096885681\n",
      "loss at step 100840: 2.6242039680480955\n",
      "loss at step 100860: 2.694901168346405\n",
      "loss at step 100880: 2.6275450229644775\n",
      "loss at step 100900: 2.6278520822525024\n",
      "loss at step 100920: 2.6548794746398925\n",
      "loss at step 100940: 2.704505944252014\n",
      "loss at step 100960: 2.646700644493103\n",
      "loss at step 100980: 2.6639907717704774\n",
      "loss at step 101000: 2.706630301475525\n",
      "loss at step 101020: 2.72328143119812\n",
      "loss at step 101040: 2.626245069503784\n",
      "loss at step 101060: 2.6961872816085815\n",
      "loss at step 101080: 2.641522979736328\n",
      "loss at step 101100: 2.6288896560668946\n",
      "loss at step 101120: 2.6105339407920836\n",
      "loss at step 101140: 2.5470063209533693\n",
      "loss at step 101160: 2.763033378124237\n",
      "loss at step 101180: 2.6176449775695803\n",
      "loss at step 101200: 2.707752048969269\n",
      "validation loss: 2.7509003671010337\n",
      "validation accuracy: 15.129382303839733%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at step 101220: 2.6691025853157044\n",
      "loss at step 101240: 2.625395894050598\n",
      "loss at step 101260: 2.5753268122673036\n",
      "loss at step 101280: 2.6050233483314513\n",
      "loss at step 101300: 2.6523296356201174\n",
      "loss at step 101320: 2.6847665786743162\n",
      "loss at step 101340: 2.6948291182518007\n",
      "loss at step 101360: 2.6853113412857055\n",
      "loss at step 101380: 2.759034752845764\n",
      "loss at step 101400: 2.7054874420166017\n",
      "loss at step 101420: 2.7358849167823793\n",
      "loss at step 101440: 2.6349490761756895\n",
      "loss at step 101460: 2.7603220343589783\n",
      "loss at step 101480: 2.6653544425964357\n",
      "loss at step 101500: 2.7170209527015685\n",
      "loss at step 101520: 2.664554464817047\n",
      "loss at step 101540: 2.6926305890083313\n",
      "loss at step 101560: 2.637912356853485\n",
      "loss at step 101580: 2.612354552745819\n",
      "loss at step 101600: 2.7167787671089174\n",
      "validation loss: 2.753519817988078\n",
      "validation accuracy: 15.880634390651085%\n",
      "loss at step 101620: 2.6247223615646362\n",
      "loss at step 101640: 2.7726691365242004\n",
      "loss at step 101660: 2.6700878500938416\n",
      "loss at step 101680: 2.6334391236305237\n",
      "loss at step 101700: 2.6483423352241515\n",
      "loss at step 101720: 2.607190215587616\n",
      "loss at step 101740: 2.656725561618805\n",
      "loss at step 101760: 2.6807453274726867\n",
      "loss at step 101780: 2.6765047788619993\n",
      "loss at step 101800: 2.7493249654769896\n",
      "loss at step 101820: 2.6570781469345093\n",
      "loss at step 101840: 2.6814342737197876\n",
      "loss at step 101860: 2.6138041973114015\n",
      "loss at step 101880: 2.7469377875328065\n",
      "loss at step 101900: 2.6854766607284546\n",
      "loss at step 101920: 2.744040274620056\n",
      "loss at step 101940: 2.7044296503067016\n",
      "loss at step 101960: 2.667844533920288\n",
      "loss at step 101980: 2.6508268237113954\n",
      "loss at step 102000: 2.704443645477295\n",
      "validation loss: 2.7824529790878296\n",
      "validation accuracy: 15.332846410684475%\n",
      "loss at step 102020: 2.722701096534729\n",
      "loss at step 102040: 2.6932382464408873\n",
      "loss at step 102060: 2.608439600467682\n",
      "loss at step 102080: 2.6499721884727476\n",
      "loss at step 102100: 2.667458546161652\n",
      "loss at step 102120: 2.608884632587433\n",
      "loss at step 102140: 2.743909990787506\n",
      "loss at step 102160: 2.701737809181213\n",
      "loss at step 102180: 2.678101193904877\n",
      "loss at step 102200: 2.646030330657959\n",
      "loss at step 102220: 2.585927653312683\n",
      "loss at step 102240: 2.749586522579193\n",
      "loss at step 102260: 2.6682697415351866\n",
      "loss at step 102280: 2.706846535205841\n",
      "loss at step 102300: 2.6561012506484984\n",
      "loss at step 102320: 2.6317821383476256\n",
      "loss at step 102340: 2.622367560863495\n",
      "loss at step 102360: 2.630259120464325\n",
      "loss at step 102380: 2.665152645111084\n",
      "loss at step 102400: 2.7378724694252012\n",
      "validation loss: 2.764724214076996\n",
      "validation accuracy: 15.181552587646078%\n",
      "loss at step 102420: 2.7104717254638673\n",
      "loss at step 102440: 2.6862298250198364\n",
      "loss at step 102460: 2.5917885422706606\n",
      "loss at step 102480: 2.7151568651199343\n",
      "loss at step 102500: 2.605632185935974\n",
      "loss at step 102520: 2.6690051436424254\n",
      "loss at step 102540: 2.7093069553375244\n",
      "loss at step 102560: 2.6543346285820006\n",
      "loss at step 102580: 2.7361994862556456\n",
      "loss at step 102600: 2.6919180035591124\n",
      "loss at step 102620: 2.6798468947410585\n",
      "loss at step 102640: 2.5915268301963805\n",
      "loss at step 102660: 2.6788307070732116\n",
      "loss at step 102680: 2.7739124417304994\n",
      "loss at step 102700: 2.685481333732605\n",
      "loss at step 102720: 2.6208579778671264\n",
      "loss at step 102740: 2.67641624212265\n",
      "loss at step 102760: 2.7098536849021913\n",
      "loss at step 102780: 2.7595757722854612\n",
      "loss at step 102800: 2.79321768283844\n",
      "validation loss: 2.7527615706125896\n",
      "validation accuracy: 15.567612687813021%\n",
      "loss at step 102820: 2.7406548857688904\n",
      "loss at step 102840: 2.6582484364509584\n",
      "loss at step 102860: 2.6291716933250426\n",
      "loss at step 102880: 2.6620266318321226\n",
      "loss at step 102900: 2.645905649662018\n",
      "loss at step 102920: 2.6298094749450684\n",
      "loss at step 102940: 2.7090476751327515\n",
      "loss at step 102960: 2.5708307147026064\n",
      "loss at step 102980: 2.696746802330017\n",
      "loss at step 103000: 2.630674588680267\n",
      "loss at step 103020: 2.6988653182983398\n",
      "loss at step 103040: 2.6988643169403077\n",
      "loss at step 103060: 2.612182319164276\n",
      "loss at step 103080: 2.570484471321106\n",
      "loss at step 103100: 2.6243633151054384\n",
      "loss at step 103120: 2.6728023648262025\n",
      "loss at step 103140: 2.6945929169654845\n",
      "loss at step 103160: 2.70952285528183\n",
      "loss at step 103180: 2.5602059721946717\n",
      "loss at step 103200: 2.622843074798584\n",
      "validation loss: 2.739774176279704\n",
      "validation accuracy: 15.191986644407345%\n",
      "loss at step 103220: 2.6231765747070312\n",
      "loss at step 103240: 2.654982054233551\n",
      "loss at step 103260: 2.701494073867798\n",
      "loss at step 103280: 2.645461678504944\n",
      "loss at step 103300: 2.71703497171402\n",
      "loss at step 103320: 2.657624912261963\n",
      "loss at step 103340: 2.7019389152526854\n",
      "loss at step 103360: 2.67813355922699\n",
      "loss at step 103380: 2.635546028614044\n",
      "loss at step 103400: 2.667586612701416\n",
      "loss at step 103420: 2.6285449385643007\n",
      "loss at step 103440: 2.6363857626914977\n",
      "loss at step 103460: 2.6429529786109924\n",
      "loss at step 103480: 2.679202449321747\n",
      "loss at step 103500: 2.7063095808029174\n",
      "loss at step 103520: 2.6352641344070435\n",
      "loss at step 103540: 2.6972482562065125\n",
      "loss at step 103560: 2.6728808879852295\n",
      "loss at step 103580: 2.7313154339790344\n",
      "loss at step 103600: 2.6215066432952883\n",
      "validation loss: 2.7553455702463787\n",
      "validation accuracy: 15.588480801335558%\n",
      "loss at step 103620: 2.6560117840766906\n",
      "loss at step 103640: 2.6551684975624084\n",
      "loss at step 103660: 2.7588626623153685\n",
      "loss at step 103680: 2.731205105781555\n",
      "loss at step 103700: 2.709763991832733\n",
      "loss at step 103720: 2.6855955362319945\n",
      "loss at step 103740: 2.6796642184257506\n",
      "loss at step 103760: 2.6625519514083864\n",
      "loss at step 103780: 2.669406163692474\n",
      "loss at step 103800: 2.700350856781006\n",
      "loss at step 103820: 2.7002559065818788\n",
      "loss at step 103840: 2.6986851930618285\n",
      "loss at step 103860: 2.6934683561325072\n",
      "loss at step 103880: 2.6421788692474366\n",
      "loss at step 103900: 2.634745514392853\n",
      "loss at step 103920: 2.6541051983833315\n",
      "loss at step 103940: 2.6095044374465943\n",
      "loss at step 103960: 2.6616451382637023\n",
      "loss at step 103980: 2.7042649030685424\n",
      "loss at step 104000: 2.6381248354911806\n",
      "validation loss: 2.773179148038228\n",
      "validation accuracy: 15.098080133555927%\n",
      "loss at step 104020: 2.6271644711494444\n",
      "loss at step 104040: 2.6932302951812743\n",
      "loss at step 104060: 2.6478013038635253\n",
      "loss at step 104080: 2.6689850449562074\n",
      "loss at step 104100: 2.736022174358368\n",
      "loss at step 104120: 2.6728386640548707\n",
      "loss at step 104140: 2.6239349961280825\n",
      "loss at step 104160: 2.722017025947571\n",
      "loss at step 104180: 2.5961347460746764\n",
      "loss at step 104200: 2.6052512407302855\n",
      "loss at step 104220: 2.6476690649986265\n",
      "loss at step 104240: 2.6541274189949036\n",
      "loss at step 104260: 2.649751901626587\n",
      "loss at step 104280: 2.6615004539489746\n",
      "loss at step 104300: 2.713424026966095\n",
      "loss at step 104320: 2.701254117488861\n",
      "loss at step 104340: 2.671482002735138\n",
      "loss at step 104360: 2.741016960144043\n",
      "loss at step 104380: 2.6236891746520996\n",
      "loss at step 104400: 2.6522675514221192\n",
      "validation loss: 2.7561159912745157\n",
      "validation accuracy: 15.609348914858096%\n",
      "loss at step 104420: 2.640388345718384\n",
      "loss at step 104440: 2.7154637694358827\n",
      "loss at step 104460: 2.707200622558594\n",
      "loss at step 104480: 2.6515551686286924\n",
      "loss at step 104500: 2.6415765643119813\n",
      "loss at step 104520: 2.6722354650497437\n",
      "loss at step 104540: 2.761936032772064\n",
      "loss at step 104560: 2.6526378452777863\n",
      "loss at step 104580: 2.713090419769287\n",
      "loss at step 104600: 2.728797507286072\n",
      "loss at step 104620: 2.7203807830810547\n",
      "loss at step 104640: 2.6373189210891725\n",
      "loss at step 104660: 2.7291711449623106\n",
      "loss at step 104680: 2.7111533284187317\n",
      "loss at step 104700: 2.6968575000762938\n",
      "loss at step 104720: 2.658084046840668\n",
      "loss at step 104740: 2.7058194398880007\n",
      "loss at step 104760: 2.7263733386993407\n",
      "loss at step 104780: 2.683784711360931\n",
      "loss at step 104800: 2.705625092983246\n",
      "validation loss: 2.769200246334076\n",
      "validation accuracy: 14.905050083472455%\n",
      "loss at step 104820: 2.610459935665131\n",
      "loss at step 104840: 2.6181176900863647\n",
      "loss at step 104860: 2.6756479382514953\n",
      "loss at step 104880: 2.665285038948059\n",
      "loss at step 104900: 2.600920331478119\n",
      "loss at step 104920: 2.668275737762451\n",
      "loss at step 104940: 2.624976658821106\n",
      "loss at step 104960: 2.6214743733406065\n",
      "loss at step 104980: 2.6606247425079346\n",
      "loss at step 105000: 2.6394182682037353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at step 105020: 2.6956525564193727\n",
      "loss at step 105040: 2.7128366589546205\n",
      "loss at step 105060: 2.680810809135437\n",
      "loss at step 105080: 2.642867636680603\n",
      "loss at step 105100: 2.643260860443115\n",
      "loss at step 105120: 2.719448482990265\n",
      "loss at step 105140: 2.6530309557914733\n",
      "loss at step 105160: 2.657451891899109\n",
      "loss at step 105180: 2.720752990245819\n",
      "loss at step 105200: 2.6286710619926454\n",
      "validation loss: 2.7473398129145306\n",
      "validation accuracy: 15.588480801335558%\n",
      "loss at step 105220: 2.6763653635978697\n",
      "loss at step 105240: 2.54657701253891\n",
      "loss at step 105260: 2.668469715118408\n",
      "loss at step 105280: 2.633170962333679\n",
      "loss at step 105300: 2.679142141342163\n",
      "loss at step 105320: 2.6609315633773805\n",
      "loss at step 105340: 2.6678587079048155\n",
      "loss at step 105360: 2.617349588871002\n",
      "loss at step 105380: 2.6016167521476747\n",
      "loss at step 105400: 2.684893453121185\n",
      "loss at step 105420: 2.635358726978302\n",
      "loss at step 105440: 2.6918542861938475\n",
      "loss at step 105460: 2.651079475879669\n",
      "loss at step 105480: 2.690655601024628\n",
      "loss at step 105500: 2.787392783164978\n",
      "loss at step 105520: 2.670849871635437\n",
      "loss at step 105540: 2.657441532611847\n",
      "loss at step 105560: 2.6223097801208497\n",
      "loss at step 105580: 2.6967936992645263\n",
      "loss at step 105600: 2.670787847042084\n",
      "validation loss: 2.7477950779596965\n",
      "validation accuracy: 15.306761268781303%\n",
      "loss at step 105620: 2.655202496051788\n",
      "loss at step 105640: 2.7104085087776184\n",
      "loss at step 105660: 2.6705973982810973\n",
      "loss at step 105680: 2.6147151231765746\n",
      "loss at step 105700: 2.675024163722992\n",
      "loss at step 105720: 2.630488967895508\n",
      "loss at step 105740: 2.710871660709381\n",
      "loss at step 105760: 2.6557570457458497\n",
      "loss at step 105780: 2.679660165309906\n",
      "loss at step 105800: 2.6792764186859133\n",
      "loss at step 105820: 2.70730859041214\n",
      "loss at step 105840: 2.6085971117019655\n",
      "loss at step 105860: 2.7438302278518676\n",
      "loss at step 105880: 2.6512481331825257\n",
      "loss at step 105900: 2.706695783138275\n",
      "loss at step 105920: 2.663619327545166\n",
      "loss at step 105940: 2.6366219401359556\n",
      "loss at step 105960: 2.670826959609985\n",
      "loss at step 105980: 2.633934426307678\n",
      "loss at step 106000: 2.6704214453697204\n",
      "validation loss: 2.7632705704371134\n",
      "validation accuracy: 15.2911101836394%\n",
      "loss at step 106020: 2.6515743374824523\n",
      "loss at step 106040: 2.7659897923469545\n",
      "loss at step 106060: 2.6638889789581297\n",
      "loss at step 106080: 2.7166555523872375\n",
      "loss at step 106100: 2.708792674541473\n",
      "loss at step 106120: 2.65445294380188\n",
      "loss at step 106140: 2.5727794766426086\n",
      "loss at step 106160: 2.7044519662857054\n",
      "loss at step 106180: 2.653339719772339\n",
      "loss at step 106200: 2.6852416038513183\n",
      "loss at step 106220: 2.652614784240723\n",
      "loss at step 106240: 2.6843093276023864\n",
      "loss at step 106260: 2.615171802043915\n",
      "loss at step 106280: 2.6217689037323\n",
      "loss at step 106300: 2.748337745666504\n",
      "loss at step 106320: 2.576107394695282\n",
      "loss at step 106340: 2.7133228540420533\n",
      "loss at step 106360: 2.655081045627594\n",
      "loss at step 106380: 2.7463414311408996\n",
      "loss at step 106400: 2.707033932209015\n",
      "validation loss: 2.7760622215271\n",
      "validation accuracy: 15.202420701168615%\n",
      "loss at step 106420: 2.7019189476966856\n",
      "loss at step 106440: 2.634120774269104\n",
      "loss at step 106460: 2.6710164546966553\n",
      "loss at step 106480: 2.6273514866828918\n",
      "loss at step 106500: 2.6270315885543822\n",
      "loss at step 106520: 2.602487790584564\n",
      "loss at step 106540: 2.670349931716919\n",
      "loss at step 106560: 2.6773698329925537\n",
      "loss at step 106580: 2.6682071924209594\n",
      "loss at step 106600: 2.6951586604118347\n",
      "loss at step 106620: 2.641699993610382\n",
      "loss at step 106640: 2.70918071269989\n",
      "loss at step 106660: 2.67935471534729\n",
      "loss at step 106680: 2.6495909571647642\n",
      "loss at step 106700: 2.667149209976196\n",
      "loss at step 106720: 2.6749099850654603\n",
      "loss at step 106740: 2.5563026785850527\n",
      "loss at step 106760: 2.6284046292304994\n",
      "loss at step 106780: 2.6248412847518923\n",
      "loss at step 106800: 2.62146075963974\n",
      "validation loss: 2.9454303201039633\n",
      "validation accuracy: 12.63042570951586%\n",
      "loss at step 106820: 2.6668501019477846\n",
      "loss at step 106840: 2.6864981174468996\n",
      "loss at step 106860: 2.6174914240837097\n",
      "loss at step 106880: 2.637184178829193\n",
      "loss at step 106900: 2.632103741168976\n",
      "loss at step 106920: 2.8375986099243162\n",
      "loss at step 106940: 2.7603800773620604\n",
      "loss at step 106960: 2.7444095730781557\n",
      "loss at step 106980: 2.743270218372345\n",
      "loss at step 107000: 2.734322202205658\n",
      "loss at step 107020: 2.6589207291603087\n",
      "loss at step 107040: 2.6961731553077697\n",
      "loss at step 107060: 2.6701620697975157\n",
      "loss at step 107080: 2.7005544185638426\n",
      "loss at step 107100: 2.722348141670227\n",
      "loss at step 107120: 2.69272540807724\n",
      "loss at step 107140: 2.6910003900527952\n",
      "loss at step 107160: 2.6404969811439516\n",
      "loss at step 107180: 2.686880958080292\n",
      "loss at step 107200: 2.761266624927521\n",
      "validation loss: 2.7802494541803995\n",
      "validation accuracy: 15.682387312186979%\n",
      "loss at step 107220: 2.657201552391052\n",
      "loss at step 107240: 2.6246466755867006\n",
      "loss at step 107260: 2.6459136843681335\n",
      "loss at step 107280: 2.7896629929542542\n",
      "loss at step 107300: 2.759889376163483\n",
      "loss at step 107320: 2.664979064464569\n",
      "loss at step 107340: 2.7332790732383727\n",
      "loss at step 107360: 2.6355546712875366\n",
      "loss at step 107380: 2.691220498085022\n",
      "loss at step 107400: 2.7296237349510193\n",
      "loss at step 107420: 2.5994067788124084\n",
      "loss at step 107440: 2.5889975786209107\n",
      "loss at step 107460: 2.7511128187179565\n",
      "loss at step 107480: 2.7235347509384153\n",
      "loss at step 107500: 2.6583396434783935\n",
      "loss at step 107520: 2.6904736518859864\n",
      "loss at step 107540: 2.6106014966964723\n",
      "loss at step 107560: 2.653657019138336\n",
      "loss at step 107580: 2.666101861000061\n",
      "loss at step 107600: 2.7347539186477663\n",
      "validation loss: 2.759207401275635\n",
      "validation accuracy: 15.83889816360601%\n",
      "loss at step 107620: 2.7375691294670106\n",
      "loss at step 107640: 2.711056351661682\n",
      "loss at step 107660: 2.674638330936432\n",
      "loss at step 107680: 2.695183980464935\n",
      "loss at step 107700: 2.7102028131484985\n",
      "loss at step 107720: 2.7150630593299865\n",
      "loss at step 107740: 2.653797423839569\n",
      "loss at step 107760: 2.5823463082313536\n",
      "loss at step 107780: 2.682648980617523\n",
      "loss at step 107800: 2.6252145409584045\n",
      "loss at step 107820: 2.663159120082855\n",
      "loss at step 107840: 2.6388089060783386\n",
      "loss at step 107860: 2.6369350433349608\n",
      "loss at step 107880: 2.6432266235351562\n",
      "loss at step 107900: 2.6796104073524476\n",
      "loss at step 107920: 2.69287348985672\n",
      "loss at step 107940: 2.7475391745567324\n",
      "loss at step 107960: 2.6922503471374513\n",
      "loss at step 107980: 2.554889512062073\n",
      "loss at step 108000: 2.7684041619300843\n",
      "validation loss: 2.806374708811442\n",
      "validation accuracy: 14.957220367278797%\n",
      "loss at step 108020: 2.651887309551239\n",
      "loss at step 108040: 2.603096568584442\n",
      "loss at step 108060: 2.6080323696136474\n",
      "loss at step 108080: 2.69559987783432\n",
      "loss at step 108100: 2.658687174320221\n",
      "loss at step 108120: 2.636406409740448\n",
      "loss at step 108140: 2.6262053728103636\n",
      "loss at step 108160: 2.6564168453216555\n",
      "loss at step 108180: 2.6886374592781066\n",
      "loss at step 108200: 2.6189137935638427\n",
      "loss at step 108220: 2.7114996910095215\n",
      "loss at step 108240: 2.7319625496864317\n",
      "loss at step 108260: 2.671777379512787\n",
      "loss at step 108280: 2.6957886934280397\n",
      "loss at step 108300: 2.731896126270294\n",
      "loss at step 108320: 2.628623294830322\n",
      "loss at step 108340: 2.653664791584015\n",
      "loss at step 108360: 2.704528272151947\n",
      "loss at step 108380: 2.6195330023765564\n",
      "loss at step 108400: 2.660325860977173\n",
      "validation loss: 2.7437504307428995\n",
      "validation accuracy: 15.630217028380637%\n",
      "loss at step 108420: 2.659663200378418\n",
      "loss at step 108440: 2.577401542663574\n",
      "loss at step 108460: 2.630521607398987\n",
      "loss at step 108480: 2.641120672225952\n",
      "loss at step 108500: 2.7539959073066713\n",
      "loss at step 108520: 2.69485719203949\n",
      "loss at step 108540: 2.631569814682007\n",
      "loss at step 108560: 2.706747794151306\n",
      "loss at step 108580: 2.6551590085029604\n",
      "loss at step 108600: 2.6332105159759522\n",
      "loss at step 108620: 2.682763147354126\n",
      "loss at step 108640: 2.6234732627868653\n",
      "loss at step 108660: 2.6110783219337463\n",
      "loss at step 108680: 2.5821346640586853\n",
      "loss at step 108700: 2.7042914509773253\n",
      "loss at step 108720: 2.7143463134765624\n",
      "loss at step 108740: 2.650501048564911\n",
      "loss at step 108760: 2.640375006198883\n",
      "loss at step 108780: 2.666060173511505\n",
      "loss at step 108800: 2.6415119647979735\n",
      "validation loss: 2.7431066465377807\n",
      "validation accuracy: 15.750208681135225%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at step 108820: 2.6926472544670106\n",
      "loss at step 108840: 2.618320369720459\n",
      "loss at step 108860: 2.7179187536239624\n",
      "loss at step 108880: 2.7108777165412903\n",
      "loss at step 108900: 2.6983333587646485\n",
      "loss at step 108920: 2.546515429019928\n",
      "loss at step 108940: 2.7496787309646606\n",
      "loss at step 108960: 2.8116978645324706\n",
      "loss at step 108980: 2.688618516921997\n",
      "loss at step 109000: 2.690885639190674\n",
      "loss at step 109020: 2.682310688495636\n",
      "loss at step 109040: 2.6846598863601683\n",
      "loss at step 109060: 2.705125463008881\n",
      "loss at step 109080: 2.701354277133942\n",
      "loss at step 109100: 2.633997344970703\n",
      "loss at step 109120: 2.6749712347984316\n",
      "loss at step 109140: 2.682475709915161\n",
      "loss at step 109160: 2.64698805809021\n",
      "loss at step 109180: 2.570146417617798\n",
      "loss at step 109200: 2.680081069469452\n",
      "validation loss: 2.7673524030049643\n",
      "validation accuracy: 15.44762103505843%\n",
      "loss at step 109220: 2.677096796035767\n",
      "loss at step 109240: 2.654175853729248\n",
      "loss at step 109260: 2.6872895479202272\n",
      "loss at step 109280: 2.737926697731018\n",
      "loss at step 109300: 2.6796091198921204\n",
      "loss at step 109320: 2.5925723195075987\n",
      "loss at step 109340: 2.710832488536835\n",
      "loss at step 109360: 2.6398444056510924\n",
      "loss at step 109380: 2.670577549934387\n",
      "loss at step 109400: 2.669767689704895\n",
      "loss at step 109420: 2.6358094811439514\n",
      "loss at step 109440: 2.6190983057022095\n",
      "loss at step 109460: 2.673917758464813\n",
      "loss at step 109480: 2.6474688410758973\n",
      "loss at step 109500: 2.7034224152565\n",
      "loss at step 109520: 2.668919324874878\n",
      "loss at step 109540: 2.6014928340911867\n",
      "loss at step 109560: 2.7188586473464964\n",
      "loss at step 109580: 2.727127230167389\n",
      "loss at step 109600: 2.7286234498023987\n",
      "validation loss: 2.771759262084961\n",
      "validation accuracy: 15.395450751252088%\n",
      "loss at step 109620: 2.6939173340797424\n",
      "loss at step 109640: 2.6396401643753054\n",
      "loss at step 109660: 2.626423919200897\n",
      "loss at step 109680: 2.6383872509002684\n",
      "loss at step 109700: 2.6454718351364135\n",
      "loss at step 109720: 2.6660255551338197\n",
      "loss at step 109740: 2.606402778625488\n",
      "loss at step 109760: 2.6548301935195924\n",
      "loss at step 109780: 2.606165826320648\n",
      "loss at step 109800: 2.6797983646392822\n",
      "loss at step 109820: 2.6572458267211916\n",
      "loss at step 109840: 2.6136723875999452\n",
      "loss at step 109860: 2.692980718612671\n",
      "loss at step 109880: 2.662259602546692\n",
      "loss at step 109900: 2.6168892860412596\n",
      "loss at step 109920: 2.6822421312332154\n",
      "loss at step 109940: 2.632042169570923\n",
      "loss at step 109960: 2.7226287961006164\n",
      "loss at step 109980: 2.624893641471863\n",
      "loss at step 110000: 2.664483034610748\n",
      "validation loss: 2.773815249602\n",
      "validation accuracy: 14.743322203672788%\n",
      "loss at step 110020: 2.672345018386841\n",
      "loss at step 110040: 2.6584015011787416\n",
      "loss at step 110060: 2.6661653399467466\n",
      "loss at step 110080: 2.591676414012909\n",
      "loss at step 110100: 2.7240665674209597\n",
      "loss at step 110120: 2.7352286696434023\n",
      "loss at step 110140: 2.6658782005310058\n",
      "loss at step 110160: 2.638843500614166\n",
      "loss at step 110180: 2.712145483493805\n",
      "loss at step 110200: 2.710970199108124\n",
      "loss at step 110220: 2.672085165977478\n",
      "loss at step 110240: 2.7772161960601807\n",
      "loss at step 110260: 2.631845259666443\n",
      "loss at step 110280: 2.626410961151123\n",
      "loss at step 110300: 2.7425439953804016\n",
      "loss at step 110320: 2.705543637275696\n",
      "loss at step 110340: 2.6278533697128297\n",
      "loss at step 110360: 2.677833545207977\n",
      "loss at step 110380: 2.666364347934723\n",
      "loss at step 110400: 2.6704317927360535\n",
      "validation loss: 2.760144597689311\n",
      "validation accuracy: 15.343280467445744%\n",
      "loss at step 110420: 2.675364685058594\n",
      "loss at step 110440: 2.699652945995331\n",
      "loss at step 110460: 2.71198046207428\n",
      "loss at step 110480: 2.6237518429756164\n",
      "loss at step 110500: 2.7233163356781005\n",
      "loss at step 110520: 2.642923080921173\n",
      "loss at step 110540: 2.633720600605011\n",
      "loss at step 110560: 2.6862411379814146\n",
      "loss at step 110580: 2.7970432877540587\n",
      "loss at step 110600: 2.64635146856308\n",
      "loss at step 110620: 2.573899245262146\n",
      "loss at step 110640: 2.6269429445266725\n",
      "loss at step 110660: 2.5940857768058776\n",
      "loss at step 110680: 2.6011006474494933\n",
      "loss at step 110700: 2.655099594593048\n",
      "loss at step 110720: 2.6498553276062013\n",
      "loss at step 110740: 2.6479578375816346\n",
      "loss at step 110760: 2.676377034187317\n",
      "loss at step 110780: 2.6553086400032044\n",
      "loss at step 110800: 2.652062010765076\n",
      "validation loss: 2.740393317540487\n",
      "validation accuracy: 15.562395659432388%\n",
      "loss at step 110820: 2.686787474155426\n",
      "loss at step 110840: 2.7246071457862855\n",
      "loss at step 110860: 2.666693556308746\n",
      "loss at step 110880: 2.7143070340156554\n",
      "loss at step 110900: 2.6992167115211485\n",
      "loss at step 110920: 2.6638414740562437\n",
      "loss at step 110940: 2.678603231906891\n",
      "loss at step 110960: 2.6711949706077576\n",
      "loss at step 110980: 2.6866257309913637\n",
      "loss at step 111000: 2.711828374862671\n",
      "loss at step 111020: 2.659982883930206\n",
      "loss at step 111040: 2.6504861235618593\n",
      "loss at step 111060: 2.7037291288375855\n",
      "loss at step 111080: 2.627199912071228\n",
      "loss at step 111100: 2.708516252040863\n",
      "loss at step 111120: 2.6758008360862733\n",
      "loss at step 111140: 2.6663192749023437\n",
      "loss at step 111160: 2.631950581073761\n",
      "loss at step 111180: 2.633815276622772\n",
      "loss at step 111200: 2.657751214504242\n",
      "validation loss: 2.735712946256002\n",
      "validation accuracy: 15.844115191986644%\n",
      "loss at step 111220: 2.6716755628585815\n",
      "loss at step 111240: 2.6817078590393066\n",
      "loss at step 111260: 2.6991062641143797\n",
      "loss at step 111280: 2.5879637718200685\n",
      "loss at step 111300: 2.671312153339386\n",
      "loss at step 111320: 2.567802846431732\n",
      "loss at step 111340: 2.7525033712387086\n",
      "loss at step 111360: 2.619838333129883\n",
      "loss at step 111380: 2.5873507857322693\n",
      "loss at step 111400: 2.684738051891327\n",
      "loss at step 111420: 2.699460971355438\n",
      "loss at step 111440: 2.605328631401062\n",
      "loss at step 111460: 2.692893397808075\n",
      "loss at step 111480: 2.634794569015503\n",
      "loss at step 111500: 2.660349130630493\n",
      "loss at step 111520: 2.7189711928367615\n",
      "loss at step 111540: 2.6198821663856506\n",
      "loss at step 111560: 2.666379415988922\n",
      "loss at step 111580: 2.667672610282898\n",
      "loss at step 111600: 2.609592854976654\n",
      "validation loss: 2.7554453857739767\n",
      "validation accuracy: 15.207637729549248%\n",
      "loss at step 111620: 2.6910163164138794\n",
      "loss at step 111640: 2.6792595982551575\n",
      "loss at step 111660: 2.696852278709412\n",
      "loss at step 111680: 2.650823438167572\n",
      "loss at step 111700: 2.611808168888092\n",
      "loss at step 111720: 2.7677947282791138\n",
      "loss at step 111740: 2.6350563287734987\n",
      "loss at step 111760: 2.6195359766483306\n",
      "loss at step 111780: 2.621983861923218\n",
      "loss at step 111800: 2.706203031539917\n",
      "loss at step 111820: 2.666851544380188\n",
      "loss at step 111840: 2.701217544078827\n",
      "loss at step 111860: 2.579925262928009\n",
      "loss at step 111880: 2.6469449639320373\n",
      "loss at step 111900: 2.6632233023643495\n",
      "loss at step 111920: 2.6923884630203245\n",
      "loss at step 111940: 2.5910594701766967\n",
      "loss at step 111960: 2.6630796909332277\n",
      "loss at step 111980: 2.6189232230186463\n",
      "loss at step 112000: 2.6197224259376526\n",
      "validation loss: 2.7320793072382608\n",
      "validation accuracy: 15.531093489148581%\n",
      "loss at step 112020: 2.6696285963058473\n",
      "loss at step 112040: 2.651199495792389\n",
      "loss at step 112060: 2.6607192516326905\n",
      "loss at step 112080: 2.6622283339500425\n",
      "loss at step 112100: 2.6350422620773317\n",
      "loss at step 112120: 2.6480942368507385\n",
      "loss at step 112140: 2.6656742215156557\n",
      "loss at step 112160: 2.598944091796875\n",
      "loss at step 112180: 2.679743432998657\n",
      "loss at step 112200: 2.6295116186141967\n",
      "loss at step 112220: 2.6564904689788817\n",
      "loss at step 112240: 2.7129674792289733\n",
      "loss at step 112260: 2.6670783281326296\n",
      "loss at step 112280: 2.702352249622345\n",
      "loss at step 112300: 2.73108913898468\n",
      "loss at step 112320: 2.6275140166282656\n",
      "loss at step 112340: 2.7339787125587462\n",
      "loss at step 112360: 2.643223524093628\n",
      "loss at step 112380: 2.66345329284668\n",
      "loss at step 112400: 2.6432830572128294\n",
      "validation loss: 2.7544608688354493\n",
      "validation accuracy: 15.265025041736227%\n",
      "loss at step 112420: 2.665639781951904\n",
      "loss at step 112440: 2.5954679012298585\n",
      "loss at step 112460: 2.5959484934806825\n",
      "loss at step 112480: 2.7148345470428468\n",
      "loss at step 112500: 2.614334464073181\n",
      "loss at step 112520: 2.594731032848358\n",
      "loss at step 112540: 2.6160125613212584\n",
      "loss at step 112560: 2.5762187480926513\n",
      "loss at step 112580: 2.7403972268104555\n",
      "loss at step 112600: 2.644652229547501\n",
      "loss at step 112620: 2.729637634754181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at step 112640: 2.6645275473594667\n",
      "loss at step 112660: 2.691979467868805\n",
      "loss at step 112680: 2.6795437693595887\n",
      "loss at step 112700: 2.669618558883667\n",
      "loss at step 112720: 2.6545138120651246\n",
      "loss at step 112740: 2.7186007261276246\n",
      "loss at step 112760: 2.6112676441669462\n",
      "loss at step 112780: 2.667857086658478\n",
      "loss at step 112800: 2.727476692199707\n",
      "validation loss: 2.7420052909851074\n",
      "validation accuracy: 15.661519198664442%\n",
      "loss at step 112820: 2.6203613758087156\n",
      "loss at step 112840: 2.6709481835365296\n",
      "loss at step 112860: 2.647661340236664\n",
      "loss at step 112880: 2.662482261657715\n",
      "loss at step 112900: 2.720310890674591\n",
      "loss at step 112920: 2.618266224861145\n",
      "loss at step 112940: 2.6424999117851256\n",
      "loss at step 112960: 2.6215763092041016\n",
      "loss at step 112980: 2.8250272393226625\n",
      "loss at step 113000: 2.6372488379478454\n",
      "loss at step 113020: 2.621204102039337\n",
      "loss at step 113040: 2.7044130206108092\n",
      "loss at step 113060: 2.6996842861175536\n",
      "loss at step 113080: 2.566091442108154\n",
      "loss at step 113100: 2.6672661304473877\n",
      "loss at step 113120: 2.698426377773285\n",
      "loss at step 113140: 2.672096538543701\n",
      "loss at step 113160: 2.661935603618622\n",
      "loss at step 113180: 2.742637884616852\n",
      "loss at step 113200: 2.7372363567352296\n",
      "validation loss: 2.7418725450833636\n",
      "validation accuracy: 14.972871452420703%\n",
      "loss at step 113220: 2.6217739820480346\n",
      "loss at step 113240: 2.620009124279022\n",
      "loss at step 113260: 2.6033002018928526\n",
      "loss at step 113280: 2.6532396554946898\n",
      "loss at step 113300: 2.6450652360916136\n",
      "loss at step 113320: 2.6681654930114744\n",
      "loss at step 113340: 2.6331148505210877\n",
      "loss at step 113360: 2.577837920188904\n",
      "loss at step 113380: 2.712501859664917\n",
      "loss at step 113400: 2.6195798993110655\n",
      "loss at step 113420: 2.6241735577583314\n",
      "loss at step 113440: 2.64932781457901\n",
      "loss at step 113460: 2.6707902431488035\n",
      "loss at step 113480: 2.6837320804595945\n",
      "loss at step 113500: 2.69013055562973\n",
      "loss at step 113520: 2.716566789150238\n",
      "loss at step 113540: 2.6526362180709837\n",
      "loss at step 113560: 2.723394477367401\n",
      "loss at step 113580: 2.688178610801697\n",
      "loss at step 113600: 2.639285981655121\n",
      "validation loss: 2.7817967700958253\n",
      "validation accuracy: 14.826794657762937%\n",
      "loss at step 113620: 2.654319643974304\n",
      "loss at step 113640: 2.7005247592926027\n",
      "loss at step 113660: 2.674560070037842\n",
      "loss at step 113680: 2.7380911111831665\n",
      "loss at step 113700: 2.617365562915802\n",
      "loss at step 113720: 2.7527238845825197\n",
      "loss at step 113740: 2.691689932346344\n",
      "loss at step 113760: 2.668383705615997\n",
      "loss at step 113780: 2.593385910987854\n",
      "loss at step 113800: 2.6731703877449036\n",
      "loss at step 113820: 2.6639504551887514\n",
      "loss at step 113840: 2.6305676698684692\n",
      "loss at step 113860: 2.6188992142677305\n",
      "loss at step 113880: 2.6170156717300417\n",
      "loss at step 113900: 2.593336892127991\n",
      "loss at step 113920: 2.604787755012512\n",
      "loss at step 113940: 2.579099726676941\n",
      "loss at step 113960: 2.6924888372421263\n",
      "loss at step 113980: 2.6819095730781557\n",
      "loss at step 114000: 2.6090678811073302\n",
      "validation loss: 2.743230708440145\n",
      "validation accuracy: 15.311978297161938%\n",
      "loss at step 114020: 2.584210157394409\n",
      "loss at step 114040: 2.6569505453109743\n",
      "loss at step 114060: 2.609489691257477\n",
      "loss at step 114080: 2.580091643333435\n",
      "loss at step 114100: 2.6223111510276795\n",
      "loss at step 114120: 2.6555199027061462\n",
      "loss at step 114140: 2.6625250935554505\n",
      "loss at step 114160: 2.6694857120513915\n",
      "loss at step 114180: 2.690258288383484\n",
      "loss at step 114200: 2.666555345058441\n",
      "loss at step 114220: 2.642761528491974\n",
      "loss at step 114240: 2.6693969488143923\n",
      "loss at step 114260: 2.7722384214401243\n",
      "loss at step 114280: 2.6606614112854006\n",
      "loss at step 114300: 2.61492338180542\n",
      "loss at step 114320: 2.6630869030952455\n",
      "loss at step 114340: 2.6251434564590452\n",
      "loss at step 114360: 2.682390105724335\n",
      "loss at step 114380: 2.6418937027454374\n",
      "loss at step 114400: 2.68942848443985\n",
      "validation loss: 2.774165515899658\n",
      "validation accuracy: 15.405884808013356%\n",
      "loss at step 114420: 2.6816463828086854\n",
      "loss at step 114440: 2.593015730381012\n",
      "loss at step 114460: 2.658875620365143\n",
      "loss at step 114480: 2.6363128423690796\n",
      "loss at step 114500: 2.6627552270889283\n",
      "loss at step 114520: 2.682500660419464\n",
      "loss at step 114540: 2.6636425375938417\n",
      "loss at step 114560: 2.6954835772514345\n",
      "loss at step 114580: 2.6998192071914673\n",
      "loss at step 114600: 2.68667711019516\n",
      "loss at step 114620: 2.740578305721283\n",
      "loss at step 114640: 2.692721664905548\n",
      "loss at step 114660: 2.6892016887664796\n",
      "loss at step 114680: 2.6205215573310854\n",
      "loss at step 114700: 2.6542237758636475\n",
      "loss at step 114720: 2.6922855019569396\n",
      "loss at step 114740: 2.6551589608192443\n",
      "loss at step 114760: 2.6397316336631773\n",
      "loss at step 114780: 2.6783610224723815\n",
      "loss at step 114800: 2.5850670337677\n",
      "validation loss: 2.7485284773508707\n",
      "validation accuracy: 15.625%\n",
      "loss at step 114820: 2.658985638618469\n",
      "loss at step 114840: 2.6288474202156067\n",
      "loss at step 114860: 2.733864736557007\n",
      "loss at step 114880: 2.6606712222099302\n",
      "loss at step 114900: 2.5431678891181946\n",
      "loss at step 114920: 2.5703211784362794\n",
      "loss at step 114940: 2.652439999580383\n",
      "loss at step 114960: 2.6293610453605654\n",
      "loss at step 114980: 2.6491547346115114\n",
      "loss at step 115000: 2.606255757808685\n",
      "loss at step 115020: 2.7107663631439207\n",
      "loss at step 115040: 2.678882563114166\n",
      "loss at step 115060: 2.6554696440696715\n",
      "loss at step 115080: 2.723539042472839\n",
      "loss at step 115100: 2.7925287008285524\n",
      "loss at step 115120: 2.6948362708091738\n",
      "loss at step 115140: 2.671460735797882\n",
      "loss at step 115160: 2.7816986560821535\n",
      "loss at step 115180: 2.7171651244163515\n",
      "loss at step 115200: 2.675211179256439\n",
      "validation loss: 2.8361707894007364\n",
      "validation accuracy: 12.066986644407345%\n",
      "loss at step 115220: 2.6031739115715027\n",
      "loss at step 115240: 2.741770553588867\n",
      "loss at step 115260: 2.6400839567184446\n",
      "loss at step 115280: 2.6752645134925843\n",
      "loss at step 115300: 2.6473333835601807\n",
      "loss at step 115320: 2.6343435525894163\n",
      "loss at step 115340: 2.6120226979255676\n",
      "loss at step 115360: 2.651104998588562\n",
      "loss at step 115380: 2.6518704533576964\n",
      "loss at step 115400: 2.614919865131378\n",
      "loss at step 115420: 2.6276848196983336\n",
      "loss at step 115440: 2.6163671255111693\n",
      "loss at step 115460: 2.6202197790145876\n",
      "loss at step 115480: 2.68732225894928\n",
      "loss at step 115500: 2.6345543384552004\n",
      "loss at step 115520: 2.703185510635376\n",
      "loss at step 115540: 2.560671496391296\n",
      "loss at step 115560: 2.5837082862854004\n",
      "loss at step 115580: 2.688635230064392\n",
      "loss at step 115600: 2.673959231376648\n",
      "validation loss: 2.762273901303609\n",
      "validation accuracy: 15.322412353923207%\n",
      "loss at step 115620: 2.7383206725120544\n",
      "loss at step 115640: 2.581473684310913\n",
      "loss at step 115660: 2.7135337471961973\n",
      "loss at step 115680: 2.696727991104126\n",
      "loss at step 115700: 2.7095010042190553\n",
      "loss at step 115720: 2.698409855365753\n",
      "loss at step 115740: 2.60298570394516\n",
      "loss at step 115760: 2.7137503027915955\n",
      "loss at step 115780: 2.540311634540558\n",
      "loss at step 115800: 2.600844621658325\n",
      "loss at step 115820: 2.7067067503929136\n",
      "loss at step 115840: 2.7145349025726317\n",
      "loss at step 115860: 2.6678624629974363\n",
      "loss at step 115880: 2.7448517084121704\n",
      "loss at step 115900: 2.670678448677063\n",
      "loss at step 115920: 2.639699935913086\n",
      "loss at step 115940: 2.7391098618507383\n",
      "loss at step 115960: 2.67943788766861\n",
      "loss at step 115980: 2.6802568197250367\n",
      "loss at step 116000: 2.640869903564453\n",
      "validation loss: 2.7394878657658897\n",
      "validation accuracy: 15.583263772954925%\n",
      "loss at step 116020: 2.7150208950042725\n",
      "loss at step 116040: 2.6019859433174135\n",
      "loss at step 116060: 2.578294062614441\n",
      "loss at step 116080: 2.660831320285797\n",
      "loss at step 116100: 2.6533764839172362\n",
      "loss at step 116120: 2.657234179973602\n",
      "loss at step 116140: 2.7134418964385985\n",
      "loss at step 116160: 2.6103070735931397\n",
      "loss at step 116180: 2.731029415130615\n",
      "loss at step 116200: 2.660201895236969\n",
      "loss at step 116220: 2.6499160051345827\n",
      "loss at step 116240: 2.6249825596809386\n",
      "loss at step 116260: 2.5748295307159426\n",
      "loss at step 116280: 2.7384148478507995\n",
      "loss at step 116300: 2.6322736620903013\n",
      "loss at step 116320: 2.7553481817245484\n",
      "loss at step 116340: 2.617366373538971\n",
      "loss at step 116360: 2.651208448410034\n",
      "loss at step 116380: 2.6865480661392214\n",
      "loss at step 116400: 2.6166675686836243\n",
      "validation loss: 2.772202245394389\n",
      "validation accuracy: 15.218071786310517%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at step 116420: 2.756659984588623\n",
      "loss at step 116440: 2.714279556274414\n",
      "loss at step 116460: 2.7071182131767273\n",
      "loss at step 116480: 2.664304792881012\n",
      "loss at step 116500: 2.614995503425598\n",
      "loss at step 116520: 2.551398479938507\n",
      "loss at step 116540: 2.6691449165344237\n",
      "loss at step 116560: 2.6349333763122558\n",
      "loss at step 116580: 2.664230000972748\n",
      "loss at step 116600: 2.6055002450942992\n",
      "loss at step 116620: 2.640992558002472\n",
      "loss at step 116640: 2.664368748664856\n",
      "loss at step 116660: 2.6880857586860656\n",
      "loss at step 116680: 2.6151037931442263\n",
      "loss at step 116700: 2.625714099407196\n",
      "loss at step 116720: 2.6638700008392333\n",
      "loss at step 116740: 2.630499076843262\n",
      "loss at step 116760: 2.6483338952064512\n",
      "loss at step 116780: 2.6879353761672973\n",
      "loss at step 116800: 2.651603376865387\n",
      "validation loss: 2.7430141480763752\n",
      "validation accuracy: 15.625%\n",
      "loss at step 116820: 2.6489794492721557\n",
      "loss at step 116840: 2.6270684123039247\n",
      "loss at step 116860: 2.6639470219612122\n",
      "loss at step 116880: 2.6270710110664366\n",
      "loss at step 116900: 2.6012281715869903\n",
      "loss at step 116920: 2.608307647705078\n",
      "loss at step 116940: 2.6849510073661804\n",
      "loss at step 116960: 2.6803884983062742\n",
      "loss at step 116980: 2.6487793922424316\n",
      "loss at step 117000: 2.6676187992095945\n",
      "loss at step 117020: 2.633983337879181\n",
      "loss at step 117040: 2.592663073539734\n",
      "loss at step 117060: 2.664504587650299\n",
      "loss at step 117080: 2.6308611631393433\n",
      "loss at step 117100: 2.6090575456619263\n",
      "loss at step 117120: 2.7599725008010862\n",
      "loss at step 117140: 2.8371317863464354\n",
      "loss at step 117160: 2.7061075568199158\n",
      "loss at step 117180: 2.6129863381385805\n",
      "loss at step 117200: 2.552820634841919\n",
      "validation loss: 2.7420346323649087\n",
      "validation accuracy: 15.536310517529214%\n",
      "loss at step 117220: 2.6476777315139772\n",
      "loss at step 117240: 2.6883064985275267\n",
      "loss at step 117260: 2.5979892730712892\n",
      "loss at step 117280: 2.754410684108734\n",
      "loss at step 117300: 2.5549354434013365\n",
      "loss at step 117320: 2.607880473136902\n",
      "loss at step 117340: 2.6332153916358947\n",
      "loss at step 117360: 2.7188580989837647\n",
      "loss at step 117380: 2.6424198746681213\n",
      "loss at step 117400: 2.6636984944343567\n",
      "loss at step 117420: 2.672225308418274\n",
      "loss at step 117440: 2.653440201282501\n",
      "loss at step 117460: 2.698504364490509\n",
      "loss at step 117480: 2.636490285396576\n",
      "loss at step 117500: 2.5545689940452574\n",
      "loss at step 117520: 2.6223510622978212\n",
      "loss at step 117540: 2.6582774758338927\n",
      "loss at step 117560: 2.7226155161857606\n",
      "loss at step 117580: 2.650568687915802\n",
      "loss at step 117600: 2.6422114253044127\n",
      "validation loss: 2.728602261543274\n",
      "validation accuracy: 15.437186978297163%\n",
      "loss at step 117620: 2.7461088418960573\n",
      "loss at step 117640: 2.617629551887512\n",
      "loss at step 117660: 2.6061218976974487\n",
      "loss at step 117680: 2.6790438413619997\n",
      "loss at step 117700: 2.7122939944267275\n",
      "loss at step 117720: 2.6828832745552065\n",
      "loss at step 117740: 2.6065828800201416\n",
      "loss at step 117760: 2.57870352268219\n",
      "loss at step 117780: 2.6739068031311035\n",
      "loss at step 117800: 2.733334851264954\n",
      "loss at step 117820: 2.6912567019462585\n",
      "loss at step 117840: 2.717132806777954\n",
      "loss at step 117860: 2.606122672557831\n",
      "loss at step 117880: 2.561265993118286\n",
      "loss at step 117900: 2.5671248197555543\n",
      "loss at step 117920: 2.61585955619812\n",
      "loss at step 117940: 2.585129642486572\n",
      "loss at step 117960: 2.6720556259155273\n",
      "loss at step 117980: 2.6643249869346617\n",
      "loss at step 118000: 2.692563271522522\n",
      "validation loss: 2.736093592643738\n",
      "validation accuracy: 15.473706176961603%\n",
      "loss at step 118020: 2.6031796097755433\n",
      "loss at step 118040: 2.6592525124549864\n",
      "loss at step 118060: 2.6274273872375487\n",
      "loss at step 118080: 2.6473018288612367\n",
      "loss at step 118100: 2.605075681209564\n",
      "loss at step 118120: 2.6226898431777954\n",
      "loss at step 118140: 2.5691592693328857\n",
      "loss at step 118160: 2.617835533618927\n",
      "loss at step 118180: 2.6276549458503724\n",
      "loss at step 118200: 2.658403789997101\n",
      "loss at step 118220: 2.6802062034606933\n",
      "loss at step 118240: 2.6849475502967834\n",
      "loss at step 118260: 2.6525462865829468\n",
      "loss at step 118280: 2.650964987277985\n",
      "loss at step 118300: 2.67400803565979\n",
      "loss at step 118320: 2.660798990726471\n",
      "loss at step 118340: 2.6363110303878785\n",
      "loss at step 118360: 2.6717451095581053\n",
      "loss at step 118380: 2.6973870515823366\n",
      "loss at step 118400: 2.6028122901916504\n",
      "validation loss: 2.7308783030509947\n",
      "validation accuracy: 15.734557595993323%\n",
      "loss at step 118420: 2.6905313372612\n",
      "loss at step 118440: 2.6548619627952577\n",
      "loss at step 118460: 2.6515881299972532\n",
      "loss at step 118480: 2.6343144178390503\n",
      "loss at step 118500: 2.7073691129684447\n",
      "loss at step 118520: 2.667672336101532\n",
      "loss at step 118540: 2.590532088279724\n",
      "loss at step 118560: 2.6568851113319396\n",
      "loss at step 118580: 2.712847089767456\n",
      "loss at step 118600: 2.6239441514015196\n",
      "loss at step 118620: 2.6807093381881715\n",
      "loss at step 118640: 2.715741217136383\n",
      "loss at step 118660: 2.6572983980178835\n",
      "loss at step 118680: 2.594503569602966\n",
      "loss at step 118700: 2.6980417728424073\n",
      "loss at step 118720: 2.6480940222740172\n",
      "loss at step 118740: 2.606640136241913\n",
      "loss at step 118760: 2.6128661036491394\n",
      "loss at step 118780: 2.6275569677352903\n",
      "loss at step 118800: 2.659798192977905\n",
      "validation loss: 2.7598148504892985\n",
      "validation accuracy: 15.431969949916526%\n",
      "loss at step 118820: 2.665801203250885\n",
      "loss at step 118840: 2.727900981903076\n",
      "loss at step 118860: 2.6230487704277037\n",
      "loss at step 118880: 2.711660897731781\n",
      "loss at step 118900: 2.5408450245857237\n",
      "loss at step 118920: 2.6305849671363832\n",
      "loss at step 118940: 2.7288461327552795\n",
      "loss at step 118960: 2.646173346042633\n",
      "loss at step 118980: 2.719397282600403\n",
      "loss at step 119000: 2.66830974817276\n",
      "loss at step 119020: 2.6597489953041076\n",
      "loss at step 119040: 2.616960144042969\n",
      "loss at step 119060: 2.560904932022095\n",
      "loss at step 119080: 2.602207636833191\n",
      "loss at step 119100: 2.695496451854706\n",
      "loss at step 119120: 2.661841702461243\n",
      "loss at step 119140: 2.522986900806427\n",
      "loss at step 119160: 2.700638997554779\n",
      "loss at step 119180: 2.699118423461914\n",
      "loss at step 119200: 2.7444537758827208\n",
      "validation loss: 2.7436905829111735\n",
      "validation accuracy: 15.687604340567612%\n",
      "loss at step 119220: 2.6456054210662843\n",
      "loss at step 119240: 2.7102683186531067\n",
      "loss at step 119260: 2.6682191848754884\n",
      "loss at step 119280: 2.705876874923706\n",
      "loss at step 119300: 2.6716114401817324\n",
      "loss at step 119320: 2.6578162550926208\n",
      "loss at step 119340: 2.7062911987304688\n",
      "loss at step 119360: 2.7324100852012636\n",
      "loss at step 119380: 2.6595534086227417\n",
      "loss at step 119400: 2.6675187706947328\n",
      "loss at step 119420: 2.59477813243866\n",
      "loss at step 119440: 2.6463313817977907\n",
      "loss at step 119460: 2.630061674118042\n",
      "loss at step 119480: 2.6326216101646422\n",
      "loss at step 119500: 2.5955482602119444\n",
      "loss at step 119520: 2.6115570187568666\n",
      "loss at step 119540: 2.658398461341858\n",
      "loss at step 119560: 2.5907677888870237\n",
      "loss at step 119580: 2.629043734073639\n",
      "loss at step 119600: 2.659856939315796\n",
      "validation loss: 2.742409904797872\n",
      "validation accuracy: 15.656302170283807%\n",
      "loss at step 119620: 2.680415141582489\n",
      "loss at step 119640: 2.627048110961914\n",
      "loss at step 119660: 2.697162961959839\n"
     ]
    }
   ],
   "source": [
    "trainer = WavenetTrainer(model=model,\n",
    "                         dataset=data,\n",
    "                         lr=0.001,\n",
    "                         snapshot_path='snapshots',\n",
    "                         snapshot_name='chaconne_model',\n",
    "                         snapshot_interval=1000,\n",
    "                         logger=logger,\n",
    "                         dtype=dtype,\n",
    "                         ltype=ltype)\n",
    "\n",
    "print('start training...')\n",
    "trainer.train(batch_size=8,\n",
    "              epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating\n",
    "This model has the Fast Wavenet Generation Algorithm (https://arxiv.org/abs/1611.09482) implemented. This might run faster on the cpu. You can give some starting data (of at least the length of receptive field) or let the model generate from zero. In my experience, a temperature between 0.5 and 1.0 yields the best results, but this may depend on the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_data = data[250000][0] # use start data from the data set\n",
    "start_data = torch.max(start_data, 0)[1] # convert one hot vectors to integers\n",
    "\n",
    "def prog_callback(step, total_steps):\n",
    "    print(str(100 * step // total_steps) + \"% generated\")\n",
    "\n",
    "generated = model.generate_fast(num_samples=160000,\n",
    "                                 first_samples=start_data,\n",
    "                                 progress_callback=prog_callback,\n",
    "                                 progress_interval=1000,\n",
    "                                 temperature=1.0,\n",
    "                                 regularize=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "\n",
    "ipd.Audio(generated, rate=16000)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
